{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input (tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation function\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#def mσ(x):\n",
    "    #return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "    \n",
    "def mσ(x):\n",
    "    return tf.math.divide(1, 1 + tf.math.exp(tf.math.negative(x)))\n",
    "\n",
    "get_custom_objects().update({'custom_activation': Activation(mσ)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mdσ(x):\n",
    "    #return np.tanh(x)\n",
    "    \n",
    "    \n",
    "#def md2σ(x):\n",
    "    #return np.divide(1., np.square(np.cosh(x)))\n",
    "\n",
    "def mdσ(x):\n",
    "    return mσ(x) * (1 - mσ(x))\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return mσ(x) * (1 - mσ(x)) * (1 - 2*mσ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]]\n",
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "[[4.53958077e-05]\n",
      " [2.06115369e-09]\n",
      " [9.34807787e-14]]\n",
      "[[-4.53916860e-05]\n",
      " [-2.06115368e-09]\n",
      " [-9.34807787e-14]]\n"
     ]
    }
   ],
   "source": [
    "x = [[10.], [20.], [30.]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(tf.keras.activations.sigmoid(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be001141",
   "metadata": {},
   "source": [
    "Define Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=3,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activationfunction = 'sigmoid',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation = activationfunction,\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation = activationfunction)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = []\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        self.tmp_layer_output.append(Z)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88587d9",
   "metadata": {},
   "source": [
    "Compute gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803620",
   "metadata": {},
   "source": [
    "Compute gradient for layer l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_layer(W,b,a,δ):\n",
    "    z1 = tf.transpose(a @ W)  \n",
    "    b = tf.reshape(b, z1.shape)\n",
    "    z2 = z1 + b\n",
    "    z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "    return W @ z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb197a1",
   "metadata": {},
   "source": [
    "Compute gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(N):\n",
    "    δ = get_gradient_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ = get_gradient_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "    return δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba635ba",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "798f579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_last_layer(W,b,a,δ):\n",
    "    z1 = np.transpose(a @ W)  \n",
    "    b = np.reshape(b, np.shape(z1))\n",
    "    z2 = z1 + b\n",
    "    z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "    ϑ = np.diag(tf.reshape(md2σ(z2), [-1]))\n",
    "    \n",
    "    return W @ z3, W @ ϑ @ np.transpose(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8133f",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30fc6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_hidden_layer(W,b,a,δ,ϑ):\n",
    "    z1 = np.transpose(a @ W)  \n",
    "    b = np.reshape(b, np.shape(z1))\n",
    "    z2 = z1 + b\n",
    "    z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "    t2 = δ * md2σ(z2)\n",
    "    H1 = W @ np.diag(tf.reshape(t2, [-1])) @ np.transpose(W)\n",
    "\n",
    "    dσt = np.diag(tf.reshape(mdσ(z2), [-1]))\n",
    "    H2 = W @ dσt @ ϑ @ dσt @ np.transpose(W)\n",
    "    \n",
    "    return W @ z3, H1+H2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18966",
   "metadata": {},
   "source": [
    "Compute Hessian and gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(N):\n",
    "    δ,ϑ = get_gradient_hessian_last_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ,ϑ = get_gradient_hessian_hidden_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "            \n",
    "    return δ,ϑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5129b90",
   "metadata": {},
   "source": [
    "Why do we get a 2D vector when we insert a 2D vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11139e62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.00342231]\n",
      " [-0.00041724]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor([3.2260446e-05 1.1036929e-04 1.1036928e-04 4.9554634e-05], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((1,2))\n",
    "\n",
    "out = NeuralN(x)\n",
    "#print(out)\n",
    "\n",
    "δ1 = get_gradient(NeuralN)\n",
    "δ2,ϑ = get_hessian(NeuralN)\n",
    "\n",
    "print(δ1- δ2)\n",
    "print(δ1)\n",
    "print(tf.reshape(ϑ, [-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04ba692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'pinn_45/dense_265/kernel:0' shape=(2, 20) dtype=float32, numpy=\n",
       "array([[ 0.0880483 , -0.17588723,  0.08583418,  0.4105976 ,  0.6004367 ,\n",
       "        -0.65359247,  0.5774773 ,  0.02379402, -0.4286404 , -0.36014584,\n",
       "         0.594869  ,  0.4404291 ,  0.12029324,  0.3650555 ,  0.27668867,\n",
       "        -0.09195187, -0.2137065 ,  0.12755962, -0.39795718,  0.13168746],\n",
       "       [-0.13520354,  0.16878895, -0.32880524, -0.44594526,  0.22692293,\n",
       "         0.05171677, -0.11949193,  0.22293565, -0.5445636 , -0.09364519,\n",
       "        -0.33409217,  0.11928429, -0.44044486, -0.26548696,  0.27016002,\n",
       "        -0.6687721 , -0.6298158 , -0.48683828,  0.1881969 , -0.01200932]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralN.hidden[0].weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510d2f",
   "metadata": {},
   "source": [
    "-> We need to choose appropriate dtypes so that no operation overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fvals1(N, x):\n",
    "\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(x)\n",
    "        y = N(x)\n",
    "\n",
    "    dy_dx = g.gradient(y, x)\n",
    "    dy_dx = np.transpose(dy_dx.numpy())\n",
    "\n",
    "    return y, dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b017a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.098707e-10\n",
      "2.098707e-10\n",
      "0.0\n",
      "2.910383e-10\n",
      "2.910383e-10\n",
      "0.0\n",
      "1.3015629e-10\n",
      "1.3015629e-10\n",
      "0.0\n",
      "2.1187908e-10\n",
      "2.1187908e-10\n",
      "0.0\n",
      "1.4551915e-10\n",
      "1.4551915e-10\n",
      "0.0\n",
      "1.3015629e-10\n",
      "1.3015629e-10\n",
      "0.0\n",
      "1.6463612e-10\n",
      "1.6463612e-10\n",
      "0.0\n",
      "4.6928517e-10\n",
      "4.6928517e-10\n",
      "0.0\n",
      "1.0493535e-10\n",
      "1.0493535e-10\n",
      "0.0\n",
      "1.3015629e-10\n",
      "1.3015629e-10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out = NeuralN(x)\n",
    "    δ1 = get_gradient(NeuralN)\n",
    "    δ2,ϑ = get_hessian(NeuralN)\n",
    "    δ_ad = _fvals1(NeuralN, x)[1]\n",
    "    print(np.linalg.norm(δ1-δ2))\n",
    "    print(np.linalg.norm(δ1-δ_ad))\n",
    "    print(np.linalg.norm(δ2-δ_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e968d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fvals2(N, x):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as h:\n",
    "        h.watch(x)\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(x)\n",
    "            y = N(x)\n",
    "\n",
    "        dy_dx = g.gradient(y, x)\n",
    "    \n",
    "    d2y_d2x = h.jacobian(dy_dx, x)\n",
    "\n",
    "    return y, dy_dx, d2y_d2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "109ef374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00030535393\n",
      "0.00047995147\n",
      "0.00023550929\n",
      "0.00081346196\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x000001D4C17CD700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8.421156e-05\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x000001D4C1406DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.00035943557\n",
      "0.0001582433\n",
      "0.000398746\n",
      "0.00050151214\n",
      "0.000113904665\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out = NeuralN(x)\n",
    "    δ,ϑ = get_hessian(NeuralN)\n",
    "    ϑ_ad = _fvals2(NeuralN, x)[2]\n",
    "    print(np.linalg.norm(ϑ-ϑ_ad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffbf82",
   "metadata": {},
   "source": [
    "Maybe gradient tape thinks that the neural network is not differentiable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4fec",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e683",
   "metadata": {},
   "source": [
    "Here we only approximate the \"half\" gradient so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a110916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=2,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='sigmoid',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = ResNetActivation\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation = RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        self.tmp_layer_output = [input_tensor]\n",
    "        \n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        \n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            self.tmp_layer_output.append(N)\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        \n",
    "        Phi = self.wb(N, training=training)\n",
    "\n",
    "#         As = self.A(input_tensor, training=training)\n",
    "#         sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "#         Phi += .5 * sAs\n",
    "#         Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957d02",
   "metadata": {},
   "source": [
    "Gradient of model, which approximates solution of pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ResNet(R):\n",
    "    δ = get_gradient_layer(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "    δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "    for k in range(R.ResNetLayers-2, 0, -1):\n",
    "        δ = δ + R.ResNetStepsize * get_gradient_layer(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "          \n",
    "    \n",
    "    δ = get_gradient_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ)\n",
    "    \n",
    "    #return δ + np.transpose(R.A(R.tmp_layer_output[0]).numpy()) + R.c.get_weights()[0]\n",
    "    return δ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd12371",
   "metadata": {},
   "source": [
    "Something is wrong with the 'whole' gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e05e73c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.0401888 ]\n",
      " [-0.01666789]], shape=(2, 1), dtype=float32)\n",
      "[[ 0.0401888 ]\n",
      " [-0.01666789]]\n",
      "1.8626451e-09\n"
     ]
    }
   ],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.constant([[1., 10.]])\n",
    "\n",
    "out = Resnet(x)\n",
    "\n",
    "δ = get_gradient_ResNet(Resnet)\n",
    "\n",
    "print(δ)\n",
    "\n",
    "δ_ad = _fvals1(Resnet, x)\n",
    "\n",
    "print(δ_ad[1])\n",
    "\n",
    "print(np.linalg.norm(δ - δ_ad[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e35744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "3.1828893e-08\n",
      "1.4901161e-08\n",
      "1.3038516e-08\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out = Resnet(x)\n",
    "    δ1 = get_gradient_ResNet(Resnet)\n",
    "    δ_ad = _fvals1(Resnet, x)[1]\n",
    "    print(np.linalg.norm(δ1-δ_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0d14a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_layer_ResNet(W,b,a,δ):\n",
    "    z1 = np.transpose(a @ W)  \n",
    "    b = np.reshape(b, np.shape(z1))\n",
    "    z2 = z1 + b\n",
    "    z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "    z4 = md2σ(z2) * δ\n",
    "    ϑ = np.diag(tf.reshape(z4, [-1]))\n",
    "    \n",
    "    return W @ z3, W @ ϑ @ np.transpose(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "600c9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian_ResNet(R):\n",
    "    δ,ϑ = get_gradient_hessian_layer_ResNet(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "    δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "    for k in range(R.ResNetLayers-2, 0, -1):\n",
    "        δ_new, ϑ_new_1 = get_gradient_hessian_layer_ResNet(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "        z1 = np.transpose(R.tmp_layer_output[k] @ R.ResNet[k].weights[0])  \n",
    "        b = np.reshape(R.ResNet[k].weights()[1], np.shape(z1))\n",
    "        z2 = z1 + b\n",
    "        t1 = ϑ + R.ResNetStepsize * R.ResNet[k].weights[0] @ np.diag(tf.reshape(mdσ(z2), [-1])) @ ϑ\n",
    "        ϑ_new_2 = np.transpose(t1) + R.ResNetStepsize * R.ResNet[k].weights[0] @ np.diag(tf.reshape(mdσ(z2), [-1])) @ np.transpose(t1)\n",
    "        ϑ = ϑ_new_1 + ϑ_new_2\n",
    "        δ = δ + R.ResNetStepsize * δ_new\n",
    "    \n",
    "      \n",
    "    δ, ϑ = get_gradient_hessian_hidden_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ, ϑ)\n",
    "    \n",
    "    #return δ + np.transpose(R.A(R.tmp_layer_output[0]).numpy()) + R.c.get_weights()[0], ϑ + np.transpose(R.A.get_weights()[0])\n",
    "    return δ, ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51c88154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004866444\n",
      "0.017996006\n",
      "0.021355437\n",
      "0.007731237\n",
      "0.00909908\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out = Resnet(x)\n",
    "    δ,ϑ = get_hessian_ResNet(Resnet)\n",
    "    ϑ_ad = _fvals2(Resnet, x)[2]\n",
    "    print(np.linalg.norm(ϑ-ϑ_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4dc1cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'pinn__res_net_10/dense_94/kernel:0' shape=(2, 16) dtype=float32, numpy=\n",
       "array([[ 0.21207619,  0.38117278, -0.11182824,  0.27755016, -0.11570671,\n",
       "         0.46281052,  0.17287827,  0.14012116,  0.47641778,  0.16146809,\n",
       "        -0.3751303 , -0.16747999, -0.10676545,  0.32442945,  0.09818757,\n",
       "        -0.03592718],\n",
       "       [-0.5770765 ,  0.09112537,  0.43927562,  0.5082332 , -0.53277296,\n",
       "         0.06791842, -0.42723218,  0.5460204 ,  0.3342405 ,  0.5088353 ,\n",
       "        -0.27283287, -0.5655137 , -0.41135675,  0.34313715,  0.35957617,\n",
       "         0.1782304 ]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Resnet.ResNet[0].weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb30a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
