{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input (tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6ebc501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b05776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation function\n",
    "# from keras.layers import Activation\n",
    "# from keras import backend as K\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#def mσ(x):\n",
    "    #return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "    \n",
    "def mσ(x):\n",
    "    return tf.math.divide(1., 1. + tf.math.exp(tf.math.negative(x)))\n",
    "\n",
    "# get_custom_objects().update({'custom_activation': Activation(mσ)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mdσ(x):\n",
    "    #return np.tanh(x)\n",
    "    \n",
    "    \n",
    "#def md2σ(x):\n",
    "    #return np.divide(1., np.square(np.cosh(x)))\n",
    "\n",
    "def mdσ(x):\n",
    "    return mσ(x) * (1. - mσ(x))\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return mσ(x) * (1. - mσ(x)) * (1. - 2.*mσ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f387ce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((10,4))\n",
    "W = tf.random.uniform((4,1))\n",
    "b = tf.random.uniform((1,1))\n",
    "z = x @ W + b\n",
    "print(z.shape)\n",
    "print(mdσ(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.5416677e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-4.541255e-05]\n",
      " [-0.000000e+00]\n",
      " [-0.000000e+00]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[10.], [20.], [30.]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(tf.keras.activations.sigmoid(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be001141",
   "metadata": {},
   "source": [
    "Define Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=3,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activationfunction = 'sigmoid',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation = activationfunction,\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation = activationfunction)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = [X]\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ = get_gradient_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "        return output, δ\n",
    "    \n",
    "\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        #x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        δ,ϑ = get_gradient_hessian_last_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ,ϑ = get_gradient_hessian_hidden_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ,  ϑ)\n",
    "\n",
    "        return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88587d9",
   "metadata": {},
   "source": [
    "Compute gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803620",
   "metadata": {},
   "source": [
    "Compute gradient for layer l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W + b)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z1) * δ\n",
    "    return W @ (mdσ(tf.transpose(a @ W + b)) * δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb197a1",
   "metadata": {},
   "source": [
    "Compute gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient(N, x):\n",
    "#     output = N(x)\n",
    "#     δ = get_gradient_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ = get_gradient_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "#     return output, δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba635ba",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "798f579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_last_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     ϑ = tf.linalg.diag(tf.reshape(md2σ(z2), [-1]))''\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     return W @ (mdσ(z) * δ), W @ (md2σ(z) * tf.transpose(W))\n",
    "    \n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ (tf.tensordot(tf.reshape(md2σ(z), [-1]), tf.transpose(W), axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8133f",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "30fc6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_hidden_layer(W,b,a,δ,ϑ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     t2 = δ * md2σ(z2)\n",
    "#     H1 = W @ tf.linalg.diag(tf.reshape(t2, [-1])) @ tf.transpose(W)\n",
    "#     dσt = tf.linalg.diag(tf.reshape(mdσ(z2), [-1]))\n",
    "#     H2 = W @ dσt @ ϑ @ dσt @ tf.transpose(W)\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     dσt = mdσ(z) * tf.transpose(W)\n",
    "#     return W @ (mdσ(z) * δ), W @ ((δ * md2σ(z)) * tf.transpose(W)) + tf.transpose(dσt) @ ϑ @ dσt \n",
    "\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    WT = tf.transpose(W)\n",
    "    dσt = tf.reshape([tf.reshape(i, (i.shape[0], 1)) * WT for i in tf.unstack(mdσ(z), axis = 1)], (z.shape[1],WT.shape[0],WT.shape[1]))\n",
    "    dσtT = tf.reshape([tf.transpose(i) for i in tf.unstack(dσt, axis = 0)], (dσt.shape[0],dσt.shape[2],dσt.shape[1]))\n",
    "    t1 = δ * md2σ(z)\n",
    "    t2 = tf.reshape([tf.reshape(i, (i.shape[0], 1)) * WT for i in tf.unstack(t1, axis = 1)], (t1.shape[1],WT.shape[0],WT.shape[1]))\n",
    "    A = W @ t2\n",
    "    B = dσtT @ ϑ @ dσt \n",
    "    return W @ (mdσ(z) * δ), A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18966",
   "metadata": {},
   "source": [
    "Compute Hessian and gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian(N, x):\n",
    "#     #x = tf.reshape(x, (1,2))\n",
    "#     output = N(x)\n",
    "#     δ,ϑ = get_gradient_hessian_last_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ,ϑ = get_gradient_hessian_hidden_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "      \n",
    "    \n",
    "#     return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9a866fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 4), dtype=float32, numpy=\n",
       "array([[ 1.8318875e-02,  1.9722445e-02, -1.2099607e-01, -5.3044055e-02],\n",
       "       [ 3.1326380e-01,  1.7344353e+00,  2.5658166e-01, -1.0187318e+00],\n",
       "       [ 2.0118143e-01, -1.5569185e-01, -2.5193062e-01, -6.8841517e-01],\n",
       "       [-5.1701792e-02,  8.9940103e-03,  5.3145830e-03, -4.3171961e-02],\n",
       "       [-2.2841246e-01,  4.6632549e-01,  5.1627588e-01,  1.8602259e-01],\n",
       "       [ 1.3212215e+00,  1.9395611e+00, -6.9541252e-01,  4.3820586e-02],\n",
       "       [-1.2883713e+00, -2.9338384e+00, -1.1029279e+00,  1.5042586e+00],\n",
       "       [-1.0875108e+00, -1.9109375e+00,  8.5978377e-01, -2.0167856e+00],\n",
       "       [-1.6927073e+00,  2.4919282e-01, -1.1295455e+00,  1.0537713e+00],\n",
       "       [-1.4724010e-02, -1.0798075e-03, -5.6281447e-02,  4.5124345e-02],\n",
       "       [ 7.5288743e-01,  1.0207346e-01,  2.5509557e-01, -3.1016055e-01],\n",
       "       [ 1.0784518e-01, -7.3585458e-02, -4.5844603e-02, -1.2507530e-01],\n",
       "       [-1.4665782e-01,  3.4777382e-01,  1.9384292e-01,  5.7739604e-01],\n",
       "       [-4.4973627e-01, -7.5121212e-01,  5.0816679e-01, -6.3811105e-01],\n",
       "       [-3.8430873e-01,  2.0589462e-01, -8.3159037e-02, -5.2028582e-02],\n",
       "       [-4.0433413e-01, -2.2967610e-01, -2.0816388e+00,  4.8592314e-01],\n",
       "       [-3.0922467e-01, -8.9876252e-01,  1.8503577e+00, -7.0936286e-01],\n",
       "       [-6.3892865e-01,  3.0612797e-01,  9.6128443e-03, -7.9006828e-02],\n",
       "       [ 1.4133646e-01, -8.4253266e-02, -6.9242932e-02, -2.0501609e-01],\n",
       "       [-8.6253934e-02, -4.3013383e-02, -7.9782913e-03,  4.0593743e-02]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal((20,))\n",
    "A = tf.random.normal((20,4))\n",
    "# print(x)\n",
    "# print(A)\n",
    "# print(tf.math.multiply(A,x))\n",
    "# print(tf.math.multiply(x,A))\n",
    "# print(A)\n",
    "# M = tf.tensordot(x, A, axes=0)\n",
    "# print(M)\n",
    "# B = tf.random.normal((3,3))\n",
    "# print(B @ M)\n",
    "# print(M@M)\n",
    "#tf.reshape(x, [-1])\n",
    "#tf.tensordot(tf.transpose(x), A, axes=1)\n",
    "#[x[:,i] * A for i in range(x.shape[0])]\n",
    "# for i in tf.unstack(x, axis = 1):\n",
    "#     print(i*A)\n",
    "# B = tf.random.normal((3,20,20))\n",
    "# C = tf.random.normal((3,20,20))\n",
    "# B@C\n",
    "# print(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20)))\n",
    "# print(tf.transpose(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20))))\n",
    "#print(A)\n",
    "\n",
    "#print(tf.reshape([tf.transpose(i) for i in tf.unstack(A, axis = 0)], (3,4,4)))\n",
    "\n",
    "tf.reshape(x, (x.shape[0], 1))*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "01a34d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "tf.Tensor([-1.327027e-05], shape=(1,), dtype=float32)\n",
      "tf.Tensor(-1.3270246e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((1,2))\n",
    "\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "\n",
    "print(yxx_ad)\n",
    "print(ϑ[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5129b90",
   "metadata": {},
   "source": [
    "Why do we get a 2D vector when we insert a 2D vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "11139e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n",
      "(20, 20)\n",
      "(20, 10)\n",
      "(20, 20)\n",
      "(20, 10)\n",
      "(20, 2)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.00021457  0.00020477  0.00019922  0.00030564  0.00019726  0.0002298\n",
      "   0.0002109   0.00026123  0.00019085  0.00028901]\n",
      " [-0.00036115 -0.0004447  -0.00035387 -0.00041372 -0.00032382 -0.0004113\n",
      "  -0.00037382 -0.00039567 -0.00032715 -0.00038928]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((10,2))\n",
    "\n",
    "#out = NeuralN(x)\n",
    "#print(out)\n",
    "\n",
    "out1, δ1 = NeuralN.get_gradient(x)\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "\n",
    "print(δ1- δ2)\n",
    "print(δ1)\n",
    "#print(tf.reshape(ϑ, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510d2f",
   "metadata": {},
   "source": [
    "-> We need to choose appropriate dtypes so that no operation overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals1(N, x):\n",
    "\n",
    "#     with tf.GradientTape() as g:\n",
    "#         g.watch(x)\n",
    "#         y = N(x)\n",
    "\n",
    "#     dy_dx = g.gradient(y, x)\n",
    "#     dy_dx = np.transpose(dy_dx.numpy())\n",
    "\n",
    "#     return y, dy_dx\n",
    "\n",
    "def _fvals1(N, t, x):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "                \n",
    "    yt = tape.gradient(y, t)\n",
    "    yx = tape.gradient(y, x)\n",
    "\n",
    "    return y, yt, yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b017a518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "3.4924597e-10\n",
      "2.3283064e-10\n",
      "3.4924597e-10\n",
      "2.3283064e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "2.910383e-10\n",
      "1.1641532e-10\n",
      "2.910383e-10\n",
      "1.1641532e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "2.4738256e-10\n",
      "0.0\n",
      "2.4738256e-10\n",
      "0.0\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.4551915e-11\n",
      "5.820766e-11\n",
      "1.4551915e-11\n",
      "5.820766e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.1641532e-10\n",
      "3.4924597e-10\n",
      "1.1641532e-10\n",
      "3.4924597e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "3.4924597e-10\n",
      "1.3096724e-10\n",
      "3.4924597e-10\n",
      "1.3096724e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "5.820766e-11\n",
      "1.1641532e-10\n",
      "5.820766e-11\n",
      "1.1641532e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n",
      "1.1641532e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    x = tf.random.uniform((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out1, δ1 = NeuralN.get_gradient(x)\n",
    "    out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yt_ad, yx_ad = _fvals1(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ1[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ1[0]-yt_ad))\n",
    "    print(np.linalg.norm(δ2[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ2[0]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e968d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals2(N, x):\n",
    "\n",
    "#     with tf.GradientTape(persistent=True) as h:\n",
    "#         h.watch(x)\n",
    "#         with tf.GradientTape() as g:\n",
    "#             g.watch(x)\n",
    "#             y = N(x)\n",
    "\n",
    "#         dy_dx = g.gradient(y, x)\n",
    "    \n",
    "#     d2y_d2x = h.jacobian(dy_dx, x)\n",
    "\n",
    "#     return y, dy_dx, d2y_d2x\n",
    "\n",
    "def _fvals2_ad(N, t, x):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "        yx = tape.gradient(y, x)\n",
    "    \n",
    "    yt = tape.gradient(y, t)\n",
    "    yxx = tape.gradient(yx, x)\n",
    "\n",
    "    return y, yt, yx, yxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "109ef374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "5.820766e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "5.820766e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.6370905e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "4.3655746e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "7.275958e-12\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "4.3655746e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "1.4551915e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "7.275958e-12\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "2.910383e-11\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 20)\n",
      "(20, 1)\n",
      "(20, 2)\n",
      "2.5465852e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out,δ,ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(ϑ[0, 1, 1]-yxx_ad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffbf82",
   "metadata": {},
   "source": [
    "Maybe gradient tape thinks that the neural network is not differentiable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4fec",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e683",
   "metadata": {},
   "source": [
    "Here we only approximate the \"half\" gradient so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8a110916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=2,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='sigmoid',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = ResNetActivation\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation = RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        self.tmp_layer_output = [input_tensor]\n",
    "        \n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        \n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            self.tmp_layer_output.append(N)\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        \n",
    "        Phi = self.wb(N, training=training)\n",
    "\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        Phi += .5 * sAs\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "        \n",
    "        print(δ.shape)\n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ = δ + self.ResNetStepsize * get_gradient_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "\n",
    "        δ = get_gradient_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0]\n",
    "        \n",
    "        \n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        δ,ϑ,z = get_gradient_hessian_layer_ResNet(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "            t = ϑ + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "            ϑ_new_2 = tf.transpose(t) + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "            ϑ = ϑ_new_1 + ϑ_new_2\n",
    "            δ = δ + self.ResNetStepsize * δ_new\n",
    "\n",
    "\n",
    "        δ, ϑ = get_gradient_hessian_hidden_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ, ϑ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957d02",
   "metadata": {},
   "source": [
    "Gradient of model, which approximates solution of pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "06b6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_ResNet(R,x):\n",
    "#     output = R(x)\n",
    "#     δ = get_gradient_layer(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ = δ + R.ResNetStepsize * get_gradient_layer(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "          \n",
    "    \n",
    "#     δ = get_gradient_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd12371",
   "metadata": {},
   "source": [
    "Something is wrong with the 'whole' gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e05e73c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n",
      "tf.Tensor(\n",
      "[[-0.44613445]\n",
      " [ 8.789103  ]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor([-0.4461347], shape=(1,), dtype=float32)\n",
      "tf.Tensor([8.789103], shape=(1,), dtype=float32)\n",
      "2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.constant([[1., 10.]])\n",
    "\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "\n",
    "print(δ)\n",
    "\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "\n",
    "print(yx_ad)\n",
    "print(yt_ad)\n",
    "\n",
    "print(np.linalg.norm(δ[0] - yx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0e35744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n",
      "0.0\n",
      "1.1920929e-07\n",
      "(16, 1)\n",
      "0.0\n",
      "0.0\n",
      "(16, 1)\n",
      "0.0\n",
      "5.9604645e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "2.3841858e-07\n",
      "(16, 1)\n",
      "2.9802322e-08\n",
      "0.0\n",
      "(16, 1)\n",
      "7.450581e-09\n",
      "0.0\n",
      "(16, 1)\n",
      "2.2351742e-08\n",
      "5.9604645e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "1.4901161e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "2.2351742e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "1.1920929e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ = Resnet.get_gradient(x)\n",
    "    out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ[0] - yx_ad))\n",
    "    print(np.linalg.norm(δ[1] - yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "934c44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 19)\n",
      "3.8656955e-07\n",
      "6.891787e-08\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((19,2))\n",
    "Resnet = PINN_ResNet()\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "print(np.linalg.norm(δ[0,:]-yx_ad))\n",
    "print(np.linalg.norm(δ[1,:]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0d14a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_layer_ResNet(W,b,a,δ):\n",
    "#     z1 = np.transpose(a @ W)  \n",
    "#     b = np.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "#     z4 = md2σ(z2) * δ\n",
    "#     ϑ = np.diag(tf.reshape(z4, [-1]))\n",
    "    \n",
    "#     return W @ z3, W @ ϑ @ np.transpose(W)\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ ((md2σ(z) * δ) * tf.transpose(W)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "600c9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian_ResNet(R,x):\n",
    "#     x = tf.reshape(x, (1,2))\n",
    "#     output = R(x)\n",
    "#     δ,ϑ,z = get_gradient_hessian_layer_ResNet(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "#         t = ϑ + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "#         ϑ_new_2 = tf.transpose(t) + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "#         ϑ = ϑ_new_1 + ϑ_new_2\n",
    "#         δ = δ + R.ResNetStepsize * δ_new\n",
    "    \n",
    "      \n",
    "#     δ, ϑ = get_gradient_hessian_hidden_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ, ϑ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51c88154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.4901161e-08\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ, ϑ = Resnet.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(ϑ[1,1]-yxx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7dd7defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x000001DDC75B6CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5.9604645e-08\n",
      "1.6323405e-07\n",
      "1.4901161e-07\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((10,2))\n",
    "Resnet = PINN_ResNet()\n",
    "result = tf.vectorized_map(Resnet.get_gradient_and_hessian, x)\n",
    "out, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "# print(result[2][:,0,0])\n",
    "# print(yxx_ad)\n",
    "print(np.linalg.norm(result[2][:,1,1] - yxx_ad))\n",
    "# print(tf.reshape(result[1][:,0], [-1]))\n",
    "# print(tf.concat(yt_ad, 0))\n",
    "print(np.linalg.norm(tf.reshape(result[1][:,1], [-1]) - tf.concat(yt_ad, 0)))\n",
    "print(np.linalg.norm(tf.reshape(result[1][:,0], [-1]) - tf.concat(yx_ad, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27de24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039fc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
