{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mσ(x):\n",
    "    return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "        \n",
    "        \n",
    "def mdσ(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return np.divide(1., np.square(np.cosh(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.]\n",
      " [20.]\n",
      " [30.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[8.24461446e-09]\n",
      " [1.69934170e-17]\n",
      " [3.50260431e-26]]\n"
     ]
    }
   ],
   "source": [
    "x = [[10], [20], [30]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=4,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation= mσ,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation),\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation= 'relu')\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = []\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        self.tmp_layer_output.append(Z)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientLayer(W,b,a,δ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    return W.dot(np.diag(mdσ(z) * δ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradient(N):\n",
    "    δ = getGradientLayer(N.out.get_weights()[0], N.out.get_weights()[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ = getGradientLayer(N.hidden[k].get_weights()[0], N.hidden[k].get_weights()[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "    return δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace56d1a",
   "metadata": {},
   "source": [
    "Hessian and gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3d65eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientHessianLayer(W,b,a,δ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    ϑ = np.reshape(np.diag(md2σ(z)), (W.shape[1], W.shape[1]))\n",
    "    return W.dot(np.diag(mdσ(z) * δ)), W @ ϑ @ np.transpose(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4f97f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientHessianLayer_hidden(W,b,a,δ,ϑ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    \n",
    "    t2  = δ * md2σ(z)\n",
    "    H1 = W @ np.reshape(np.diag(t2), (W.shape[1], W.shape[1])) @ np.transpose(W)\n",
    "\n",
    "    dσt = mdσ(z)\n",
    "    t3 = np.diag(dσt) @ ϑ @ np.diag(dσt)\n",
    "    H2 = W @ np.reshape(t3, (W.shape[1], W.shape[1])) @ np.transpose(W)\n",
    "    return W.dot(np.diag(mdσ(z) * δ)), H1 + H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHessian(N):\n",
    "    δ,ϑ = getGradientHessianLayer(N.out.get_weights()[0], N.out.get_weights()[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ,ϑ = getGradientHessianLayer(N.hidden[k].get_weights()[0], N.hidden[k].get_weights()[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "            \n",
    "    return δ,ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3dad02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "δ,ϑ = getGradientHessianLayer(N.out.get_weights()[0], N.out.get_weights()[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "print(ϑ.shape)\n",
    "print(N.out.get_weights()[0].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "11139e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.12862899])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = PINN()\n",
    "# x = tf.constant([[150.], [150.]])\n",
    "x = tf.constant([[150.]])\n",
    "out = N(x)\n",
    "print(out)\n",
    "#print(N.tmp_layer_output[2])\n",
    "#print(N.hidden[0].get_weights()[0] * x + N.hidden[0].get_weights()[1])\n",
    "#print(mσ(N.hidden[0].get_weights()[0]))\n",
    "#print(np.shape(N.out.get_weights()[0]))\n",
    "#print(np.shape(N.hidden[-1].get_weights()[0]))\n",
    "#print(N.hidden[1].get_weights()[0])\n",
    "getGradient(N)\n",
    "#print(len(N.hidden))\n",
    "#print(N.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fvals1(N, x):\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x)\n",
    "        funcvalue = N(x)\n",
    "\n",
    "    return tape.gradient(funcvalue, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e968d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(_fvals1(N, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cab643",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47837a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=1,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='softplus',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = mσ\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation=RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        Phi = self.wb(N, training=training)\n",
    "        #print(input_tensor)\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        #print(As)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        #print(sAs)\n",
    "        Phi += .5 * sAs\n",
    "        #print(Phi.shape)\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "        #return self.out(Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35088ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis = 1 not in [-1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7dd793c01968>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mResnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPINN_ResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[0mexpanded_num_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mexpanded_num_dims\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mexpanded_num_dims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m       raise ValueError(\"axis = %d not in [%d, %d)\" %\n\u001b[0m\u001b[0;32m   1422\u001b[0m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axis = 1 not in [-1, 1)"
     ]
    }
   ],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.stack([1., 1.], axis=1)\n",
    "out = Resnet(x)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
