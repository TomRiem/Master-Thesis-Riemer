{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mσ(x):\n",
    "    return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "        \n",
    "        \n",
    "def mdσ(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return np.divide(1., np.square(np.cosh(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.]\n",
      " [20.]\n",
      " [30.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[8.24461446e-09]\n",
      " [1.69934170e-17]\n",
      " [3.50260431e-26]]\n"
     ]
    }
   ],
   "source": [
    "x = [[10], [20], [30]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be001141",
   "metadata": {},
   "source": [
    "Define Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=4,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation= mσ,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation),\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation= 'relu')\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = []\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        self.tmp_layer_output.append(Z)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88587d9",
   "metadata": {},
   "source": [
    "Compute gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803620",
   "metadata": {},
   "source": [
    "Compute gradient for layer l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientLayer(W,b,a,δ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    return W.dot(np.diag(mdσ(z) * δ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb197a1",
   "metadata": {},
   "source": [
    "Compute gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradient(N):\n",
    "    δ = getGradientLayer(N.out.get_weights()[0], N.out.get_weights()[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ = getGradientLayer(N.hidden[k].get_weights()[0], N.hidden[k].get_weights()[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "    return δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba635ba",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "798f579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientHessianLayer(W,b,a,δ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    ϑ = np.reshape(np.diag(md2σ(z)), (W.shape[1], W.shape[1]))\n",
    "    return W.dot(np.diag(mdσ(z) * δ)), W @ ϑ @ np.transpose(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8133f",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "30fc6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGradientHessianLayer_hidden(W,b,a,δ,ϑ):\n",
    "    z = np.transpose(W).dot(np.transpose(a))  \n",
    "    # kann schöner gemacht werden. Eigentlich reicht einmal transponieren.\n",
    "    b = np.reshape(b, np.shape(z))\n",
    "    z = z + b\n",
    "    δh = np.reshape(δ, np.shape(md2σ(z)))\n",
    "    t2 = δh * md2σ(z)\n",
    "    t2 = [item for sublist in t2 for item in sublist]\n",
    "    H1 = W @ np.diag(t2) @ np.transpose(W)\n",
    "\n",
    "    dσt = mdσ(z)\n",
    "    dσt = [item for sublist in dσt for item in sublist]\n",
    "    t3 = np.diag(dσt) @ ϑ @ np.diag(dσt)\n",
    "    H2 = W @ np.reshape(t3, (W.shape[1], W.shape[1])) @ np.transpose(W)\n",
    "    return W.dot(np.diag(mdσ(z) * δ)), H1+H2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18966",
   "metadata": {},
   "source": [
    "Compute Hessian and gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHessian(N):\n",
    "    δ,ϑ = getGradientHessianLayer(N.out.get_weights()[0], N.out.get_weights()[1], N.tmp_layer_output[-1], np.identity(N.output_dim))\n",
    "\n",
    "    for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "        δ,ϑ = getGradientHessianLayer_hidden(N.hidden[k].get_weights()[0], N.hidden[k].get_weights()[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "            \n",
    "    return δ,ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "11139e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-352-03cc8c401697>:10: RuntimeWarning: overflow encountered in cosh\n",
      "  return np.divide(1., np.square(np.cosh(x)))\n",
      "<ipython-input-352-03cc8c401697>:10: RuntimeWarning: overflow encountered in square\n",
      "  return np.divide(1., np.square(np.cosh(x)))\n"
     ]
    }
   ],
   "source": [
    "N = PINN()\n",
    "# x = tf.constant([[150.], [150.]])\n",
    "x = tf.constant([[150.]])\n",
    "out = N(x)\n",
    "# print(out)\n",
    "#print(N.tmp_layer_output[2])\n",
    "#print(N.hidden[0].get_weights()[0] * x + N.hidden[0].get_weights()[1])\n",
    "#print(mσ(N.hidden[0].get_weights()[0]))\n",
    "#print(np.shape(N.out.get_weights()[0]))\n",
    "#print(np.shape(N.hidden[-1].get_weights()[0]))\n",
    "#print(N.hidden[1].get_weights()[0])\n",
    "δ1 = getGradient(N)\n",
    "δ2,ϑ = getHessian(N)\n",
    "#print(len(N.hidden))\n",
    "#print(N.num_hidden_layers)\n",
    "print(δ1 - δ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fvals1(N, x):\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x)\n",
    "        funcvalue = N(x)\n",
    "\n",
    "    return tape.gradient(funcvalue, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_fvals1(N, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4fec",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a110916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=1,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='softplus',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = mσ\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation=RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        Phi = self.wb(N, training=training)\n",
    "        #print(input_tensor)\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        #print(As)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        #print(sAs)\n",
    "        Phi += .5 * sAs\n",
    "        #print(Phi.shape)\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "        #return self.out(Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.stack([1., 1.], axis=1)\n",
    "out = Resnet(x)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
