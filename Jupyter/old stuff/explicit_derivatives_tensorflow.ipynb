{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input (tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebc501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation function\n",
    "# from keras.layers import Activation\n",
    "# from keras import backend as K\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#def mσ(x):\n",
    "    #return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "    \n",
    "def mσ(x):\n",
    "    return tf.math.divide(1., 1. + tf.math.exp(tf.math.negative(x)))\n",
    "\n",
    "# get_custom_objects().update({'custom_activation': Activation(mσ)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mdσ(x):\n",
    "    #return np.tanh(x)\n",
    "    \n",
    "    \n",
    "#def md2σ(x):\n",
    "    #return np.divide(1., np.square(np.cosh(x)))\n",
    "\n",
    "def mdσ(x):\n",
    "    return mσ(x) * (1. - mσ(x))\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return mσ(x) * (1. - mσ(x)) * (1. - 2.*mσ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f387ce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((10,4))\n",
    "W = tf.random.uniform((4,1))\n",
    "b = tf.random.uniform((1,1))\n",
    "z = x @ W + b\n",
    "print(z.shape)\n",
    "print(mdσ(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.5416677e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-4.541255e-05]\n",
      " [-0.000000e+00]\n",
      " [-0.000000e+00]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[10.], [20.], [30.]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(tf.keras.activations.sigmoid(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be001141",
   "metadata": {},
   "source": [
    "Define Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=3,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activationfunction = 'sigmoid',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation = activationfunction,\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation = activationfunction)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = [X]\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ = get_gradient_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "        return output, δ\n",
    "    \n",
    "\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        #x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        δ,ϑ,z = get_gradient_hessian_last_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ,ϑ = get_gradient_hessian_hidden_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ,  ϑ)\n",
    "\n",
    "        return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88587d9",
   "metadata": {},
   "source": [
    "Compute gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803620",
   "metadata": {},
   "source": [
    "Compute gradient for layer l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W + b)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z1) * δ\n",
    "    return W @ (mdσ(tf.transpose(a @ W + b)) * δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb197a1",
   "metadata": {},
   "source": [
    "Compute gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient(N, x):\n",
    "#     output = N(x)\n",
    "#     δ = get_gradient_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ = get_gradient_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "#     return output, δ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b289aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_hadamard(v,A):\n",
    "    return [tf.reshape(i, (i.shape[0], 1)) * A for i in tf.unstack(v, axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba635ba",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798f579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_last_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     ϑ = tf.linalg.diag(tf.reshape(md2σ(z2), [-1]))''\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     return W @ (mdσ(z) * δ), W @ (md2σ(z) * tf.transpose(W))\n",
    "    \n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ tensor_hadamard(δ * md2σ(z),tf.transpose(W)), z\n",
    "#[tf.reshape(i, (i.shape[0], 1)) * tf.transpose(W) for i in tf.unstack(δ * md2σ(z), axis = 1)], z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8133f",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fc6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_hidden_layer(W,b,a,δ,ϑ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     t2 = δ * md2σ(z2)\n",
    "#     H1 = W @ tf.linalg.diag(tf.reshape(t2, [-1])) @ tf.transpose(W)\n",
    "#     dσt = tf.linalg.diag(tf.reshape(mdσ(z2), [-1]))\n",
    "#     H2 = W @ dσt @ ϑ @ dσt @ tf.transpose(W)\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     dσt = mdσ(z) * tf.transpose(W)\n",
    "#     return W @ (mdσ(z) * δ), W @ ((δ * md2σ(z)) * tf.transpose(W)) + tf.transpose(dσt) @ ϑ @ dσt \n",
    "\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    WT = tf.transpose(W)\n",
    "    mdσz = mdσ(z)\n",
    "    dσt = tensor_hadamard(mdσz, WT)\n",
    "    dσtT = tf.transpose(dσt, perm=[0, 2, 1])\n",
    "    t2 = tensor_hadamard(δ * md2σ(z), WT)\n",
    "    return W @ (mdσz * δ), W @ t2 + dσtT @ ϑ @ dσt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18966",
   "metadata": {},
   "source": [
    "Compute Hessian and gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian(N, x):\n",
    "#     #x = tf.reshape(x, (1,2))\n",
    "#     output = N(x)\n",
    "#     δ,ϑ = get_gradient_hessian_last_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ,ϑ = get_gradient_hessian_hidden_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "      \n",
    "    \n",
    "#     return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a866fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.random.normal((20,))\n",
    "# A = tf.random.normal((20,4))\n",
    "# print(x)\n",
    "# print(A)\n",
    "# print(tf.math.multiply(A,x))\n",
    "# print(tf.math.multiply(x,A))\n",
    "# print(A)\n",
    "# M = tf.tensordot(x, A, axes=0)\n",
    "# print(M)\n",
    "# B = tf.random.normal((3,3))\n",
    "# print(B @ M)\n",
    "# print(M@M)\n",
    "#tf.reshape(x, [-1])\n",
    "#tf.tensordot(tf.transpose(x), A, axes=1)\n",
    "#[x[:,i] * A for i in range(x.shape[0])]\n",
    "# for i in tf.unstack(x, axis = 1):\n",
    "#     print(i*A)\n",
    "# B = tf.random.normal((3,20,20))\n",
    "# C = tf.random.normal((3,20,20))\n",
    "# B@C\n",
    "# print(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20)))\n",
    "# print(tf.transpose(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20))))\n",
    "#print(A)\n",
    "\n",
    "#print(tf.reshape([tf.transpose(i) for i in tf.unstack(A, axis = 0)], (3,4,4)))\n",
    "\n",
    "#tf.reshape(x, (x.shape[0], 1))*A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5129b90",
   "metadata": {},
   "source": [
    "Why do we get a 2D vector when we insert a 2D vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11139e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.1629434e-03 -1.1721451e-03 -1.1019403e-03 -1.2002669e-03\n",
      "  -1.0989591e-03 -1.1910177e-03 -1.1870349e-03 -1.1253051e-03\n",
      "  -1.0930389e-03 -1.1791835e-03]\n",
      " [ 2.1174898e-04  2.2755483e-04  2.2179197e-04  2.3000706e-04\n",
      "   2.1788178e-04  2.3825876e-04  1.8178142e-04  8.4744795e-05\n",
      "   1.8635154e-04  2.2060158e-04]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((10,2))\n",
    "\n",
    "#out = NeuralN(x)\n",
    "#print(out)\n",
    "\n",
    "out1, δ1 = NeuralN.get_gradient(x)\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "\n",
    "print(δ1- δ2)\n",
    "print(δ1)\n",
    "#print(tf.reshape(ϑ, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510d2f",
   "metadata": {},
   "source": [
    "-> We need to choose appropriate dtypes so that no operation overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals1(N, x):\n",
    "\n",
    "#     with tf.GradientTape() as g:\n",
    "#         g.watch(x)\n",
    "#         y = N(x)\n",
    "\n",
    "#     dy_dx = g.gradient(y, x)\n",
    "#     dy_dx = np.transpose(dy_dx.numpy())\n",
    "\n",
    "#     return y, dy_dx\n",
    "\n",
    "def _fvals1(N, t, x):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "                \n",
    "    yt = tape.gradient(y, t)\n",
    "    yx = tape.gradient(y, x)\n",
    "\n",
    "    return y, yt, yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b017a518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.656613e-10\n",
      "4.656613e-10\n",
      "4.656613e-10\n",
      "4.656613e-10\n",
      "1.1641532e-10\n",
      "2.3283064e-10\n",
      "1.1641532e-10\n",
      "2.3283064e-10\n",
      "4.656613e-10\n",
      "1.1641532e-10\n",
      "4.656613e-10\n",
      "1.1641532e-10\n",
      "0.0\n",
      "5.820766e-11\n",
      "0.0\n",
      "5.820766e-11\n",
      "5.2386895e-10\n",
      "3.4924597e-10\n",
      "5.2386895e-10\n",
      "3.4924597e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "5.820766e-11\n",
      "1.1641532e-10\n",
      "5.820766e-11\n",
      "1.1641532e-10\n",
      "5.820766e-11\n",
      "0.0\n",
      "5.820766e-11\n",
      "0.0\n",
      "5.820766e-11\n",
      "0.0\n",
      "5.820766e-11\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    x = tf.random.uniform((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out1, δ1 = NeuralN.get_gradient(x)\n",
    "    out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yt_ad, yx_ad = _fvals1(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ1[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ1[0]-yt_ad))\n",
    "    print(np.linalg.norm(δ2[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ2[0]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e968d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals2(N, x):\n",
    "\n",
    "#     with tf.GradientTape(persistent=True) as h:\n",
    "#         h.watch(x)\n",
    "#         with tf.GradientTape() as g:\n",
    "#             g.watch(x)\n",
    "#             y = N(x)\n",
    "\n",
    "#         dy_dx = g.gradient(y, x)\n",
    "    \n",
    "#     d2y_d2x = h.jacobian(dy_dx, x)\n",
    "\n",
    "#     return y, dy_dx, d2y_d2x\n",
    "\n",
    "def _fvals2_ad(N, t, x):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "        yx = tape.gradient(y, x)\n",
    "    \n",
    "    yt = tape.gradient(y, t)\n",
    "    yxx = tape.gradient(yx, x)\n",
    "\n",
    "    return y, yt, yx, yxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109ef374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "5.820766e-11\n",
      "1.4551915e-11\n",
      "1.4551915e-11\n",
      "7.275958e-12\n",
      "7.275958e-12\n",
      "1.4551915e-11\n",
      "3.637979e-12\n",
      "7.2759576e-11\n",
      "0.0\n",
      "2.910383e-11\n",
      "0.0\n",
      "1.546141e-11\n",
      "1.4551915e-11\n",
      "5.0931703e-11\n",
      "2.910383e-11\n",
      "7.275958e-12\n",
      "3.6379788e-11\n",
      "2.910383e-11\n",
      "1.4551915e-11\n",
      "3.6379788e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    x = tf.random.normal((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out,δ,ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(ϑ[0, 1, 1]-yxx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539ee268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((100,2))\n",
    "\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "\n",
    "# print(yxx_ad)\n",
    "# print(ϑ[:, 1, 1])\n",
    "# print(δ2[0,:])\n",
    "# print(δ2[1,:])\n",
    "#print(tf.linalg.norm(yxx_ad - ϑ[:, 1, 1]))\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffbf82",
   "metadata": {},
   "source": [
    "Maybe gradient tape thinks that the neural network is not differentiable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4fec",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e683",
   "metadata": {},
   "source": [
    "Here we only approximate the \"half\" gradient so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a110916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=2,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='sigmoid',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = ResNetActivation\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation = RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        self.tmp_layer_output = [input_tensor]\n",
    "        \n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        \n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            self.tmp_layer_output.append(N)\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        \n",
    "        Phi = self.wb(N, training=training)\n",
    "\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        Phi += .5 * sAs\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "        \n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ = δ + self.ResNetStepsize * get_gradient_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "\n",
    "        δ = get_gradient_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0]\n",
    "        \n",
    "        \n",
    "#     def get_gradient_and_hessian(self, x):\n",
    "#         x = tf.reshape(x, (1,2))\n",
    "#         output = self.call(x)\n",
    "#         δ,ϑ,z = get_gradient_hessian_layer_ResNet(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "#         δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "#         for k in range(self.ResNetLayers-2, 0, -1):\n",
    "#             δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "#             t = ϑ + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "#             ϑ_new_2 = tf.transpose(t) + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "#             ϑ = ϑ_new_1 + ϑ_new_2\n",
    "#             δ = δ + self.ResNetStepsize * δ_new\n",
    "\n",
    "\n",
    "#         δ, ϑ = get_gradient_hessian_hidden_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ, ϑ)\n",
    "\n",
    "#         M = self.A.weights[0]\n",
    "\n",
    "#         return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))\n",
    "\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        output = self.call(x)\n",
    "        δ,ϑ,z = get_gradient_hessian_last_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ_new, ϑ_new_1, z = get_gradient_hessian_last_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "            mz = mdσ(z)\n",
    "            #t = ϑ + self.ResNetStepsize * self.ResNet[k].weights[0] @ tf.reshape([tf.reshape(i, (i.shape[0], 1)) * ϑ for i in tf.unstack(mz, axis = 1)], (mz.shape[1],ϑ.shape[0],ϑ.shape[1]))\n",
    "            t = ϑ + self.ResNetStepsize * self.ResNet[k].weights[0] @ tensor_hadamard(mz, ϑ)\n",
    "            #tT = tf.reshape([tf.transpose(i) for i in tf.unstack(t, axis = 0)], (t.shape[0],t.shape[2],t.shape[1]))\n",
    "            tT = tf.transpose(t, perm=[0, 2, 1])\n",
    "            #ϑ_new_2 = tT + self.ResNetStepsize * self.ResNet[k].weights[0] @ tf.reshape([tf.reshape(i, (i.shape[0], 1)) * tT for i in tf.unstack(mz, axis = 1)], (mz.shape[1],tT.shape[0],tT.shape[1]))\n",
    "            ϑ_new_2 = tT + self.ResNetStepsize * self.ResNet[k].weights[0] @ tensor_hadamard(mz, tT)\n",
    "            ϑ = ϑ_new_1 + ϑ_new_2\n",
    "            δ = δ + self.ResNetStepsize * δ_new\n",
    "\n",
    "\n",
    "        δ, ϑ = get_gradient_hessian_hidden_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ, ϑ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957d02",
   "metadata": {},
   "source": [
    "Gradient of model, which approximates solution of pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06b6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_ResNet(R,x):\n",
    "#     output = R(x)\n",
    "#     δ = get_gradient_layer(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ = δ + R.ResNetStepsize * get_gradient_layer(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "          \n",
    "    \n",
    "#     δ = get_gradient_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd12371",
   "metadata": {},
   "source": [
    "Something is wrong with the 'whole' gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e05e73c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6.383861  ]\n",
      " [-0.06816453]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor([6.383861], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-0.0681645], shape=(1,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.constant([[1., 10.]])\n",
    "\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "\n",
    "print(δ)\n",
    "\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "\n",
    "print(yx_ad)\n",
    "print(yt_ad)\n",
    "\n",
    "print(np.linalg.norm(δ[0] - yx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e35744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.1920929e-07\n",
      "0.0\n",
      "2.2351742e-08\n",
      "2.9802322e-08\n",
      "1.1920929e-07\n",
      "1.1920929e-07\n",
      "1.1920929e-07\n",
      "0.0\n",
      "1.1920929e-07\n",
      "4.4703484e-08\n",
      "0.0\n",
      "0.0\n",
      "5.9604645e-08\n",
      "1.1920929e-07\n",
      "0.0\n",
      "0.0\n",
      "5.9604645e-08\n",
      "5.9604645e-08\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ = Resnet.get_gradient(x)\n",
    "    out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ[0] - yx_ad))\n",
    "    print(np.linalg.norm(δ[1] - yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "934c44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1610136e-07\n",
      "4.3546078e-07\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((19,2))\n",
    "Resnet = PINN_ResNet()\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "print(np.linalg.norm(δ[0,:]-yx_ad))\n",
    "print(np.linalg.norm(δ[1,:]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0d14a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_layer_ResNet(W,b,a,δ):\n",
    "#     z1 = np.transpose(a @ W)  \n",
    "#     b = np.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "#     z4 = md2σ(z2) * δ\n",
    "#     ϑ = np.diag(tf.reshape(z4, [-1]))\n",
    "    \n",
    "#     return W @ z3, W @ ϑ @ np.transpose(W)\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ ((md2σ(z) * δ) * tf.transpose(W)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "600c9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian_ResNet(R,x):\n",
    "#     x = tf.reshape(x, (1,2))\n",
    "#     output = R(x)\n",
    "#     δ,ϑ,z = get_gradient_hessian_layer_ResNet(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "#         t = ϑ + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "#         ϑ_new_2 = tf.transpose(t) + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "#         ϑ = ϑ_new_1 + ϑ_new_2\n",
    "#         δ = δ + R.ResNetStepsize * δ_new\n",
    "    \n",
    "      \n",
    "#     δ, ϑ = get_gradient_hessian_hidden_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ, ϑ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51c88154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4901161e-08, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ, ϑ = Resnet.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "    #print(np.linalg.norm(ϑ[1,1]-yxx_ad))\n",
    "    print(tf.linalg.norm(ϑ[:,1,1] - yxx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd7defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.573173e-08, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((100,2))\n",
    "Resnet = PINN_ResNet()\n",
    "out2, δ2, ϑ = Resnet.get_gradient_and_hessian(x)\n",
    "out, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "# print(result[2][:,0,0])\n",
    "# print(yxx_ad)\n",
    "#print(np.linalg.norm(result[2][:,1,1] - yxx_ad))\n",
    "# print(tf.reshape(result[1][:,0], [-1]))\n",
    "# print(tf.concat(yt_ad, 0))\n",
    "# print(np.linalg.norm(tf.reshape(result[1][:,1], [-1]) - tf.concat(yt_ad, 0)))\n",
    "# print(np.linalg.norm(tf.reshape(result[1][:,0], [-1]) - tf.concat(yx_ad, 0)))\n",
    "print(tf.linalg.norm(yxx_ad - ϑ[:, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27de24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039fc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
