{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7417d936",
   "metadata": {},
   "source": [
    "# First and second derivative of FNN with respect to input (tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90e9a5",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a899b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebc501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12c0bb",
   "metadata": {},
   "source": [
    "Define activation function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05776f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation function\n",
    "# from keras.layers import Activation\n",
    "# from keras import backend as K\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#def mσ(x):\n",
    "    #return np.abs(x) + np.log(1. + np.exp(-2. * np.abs(x)))\n",
    "    \n",
    "def mσ(x):\n",
    "    return tf.math.divide(1., 1. + tf.math.exp(tf.math.negative(x)))\n",
    "\n",
    "# get_custom_objects().update({'custom_activation': Activation(mσ)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mdσ(x):\n",
    "    #return np.tanh(x)\n",
    "    \n",
    "    \n",
    "#def md2σ(x):\n",
    "    #return np.divide(1., np.square(np.cosh(x)))\n",
    "\n",
    "def mdσ(x):\n",
    "    return mσ(x) * (1. - mσ(x))\n",
    "    \n",
    "    \n",
    "def md2σ(x):\n",
    "    return mσ(x) * (1. - mσ(x)) * (1. - 2.*mσ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f387ce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((10,4))\n",
    "W = tf.random.uniform((4,1))\n",
    "b = tf.random.uniform((1,1))\n",
    "z = x @ W + b\n",
    "print(z.shape)\n",
    "print(mdσ(z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfa6983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.9999546]\n",
      " [1.       ]\n",
      " [1.       ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.5416677e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-4.541255e-05]\n",
      " [-0.000000e+00]\n",
      " [-0.000000e+00]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[10.], [20.], [30.]]\n",
    "\n",
    "print(mσ(x))\n",
    "print(tf.keras.activations.sigmoid(x))\n",
    "print(mdσ(x))\n",
    "print(md2σ(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5b1e1",
   "metadata": {},
   "source": [
    "Does not exactly match the results/values in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be001141",
   "metadata": {},
   "source": [
    "Define Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=3,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activationfunction = 'sigmoid',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation = activationfunction,\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation = activationfunction)\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = [X]\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ = get_gradient_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "        return output, δ\n",
    "    \n",
    "\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        #x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        δ,ϑ = get_gradient_hessian_last_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            δ,ϑ = get_gradient_hessian_hidden_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], δ,  ϑ)\n",
    "\n",
    "        return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88587d9",
   "metadata": {},
   "source": [
    "Compute gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78803620",
   "metadata": {},
   "source": [
    "Compute gradient for layer l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd5f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W + b)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z1) * δ\n",
    "    return W @ (mdσ(tf.transpose(a @ W + b)) * δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb197a1",
   "metadata": {},
   "source": [
    "Compute gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b270783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient(N, x):\n",
    "#     output = N(x)\n",
    "#     δ = get_gradient_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ = get_gradient_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ)\n",
    "            \n",
    "#     return output, δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba635ba",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798f579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_last_layer(W,b,a,δ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, z1.shape)\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     ϑ = tf.linalg.diag(tf.reshape(md2σ(z2), [-1]))''\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     return W @ (mdσ(z) * δ), W @ (md2σ(z) * tf.transpose(W))\n",
    "    \n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ (tf.tensordot(tf.reshape(md2σ(z), [-1]), tf.transpose(W), axes=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8133f",
   "metadata": {},
   "source": [
    "Compute gradient and Hessian of hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fc6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_hidden_layer(W,b,a,δ,ϑ):\n",
    "#     z1 = tf.transpose(a @ W)  \n",
    "#     b = tf.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = mdσ(z2) * δ\n",
    "#     t2 = δ * md2σ(z2)\n",
    "#     H1 = W @ tf.linalg.diag(tf.reshape(t2, [-1])) @ tf.transpose(W)\n",
    "#     dσt = tf.linalg.diag(tf.reshape(mdσ(z2), [-1]))\n",
    "#     H2 = W @ dσt @ ϑ @ dσt @ tf.transpose(W)\n",
    "\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     dσt = mdσ(z) * tf.transpose(W)\n",
    "#     return W @ (mdσ(z) * δ), W @ ((δ * md2σ(z)) * tf.transpose(W)) + tf.transpose(dσt) @ ϑ @ dσt \n",
    "\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    WT = tf.transpose(W)\n",
    "    dσt = tf.reshape([tf.reshape(i, (i.shape[0], 1)) * WT for i in tf.unstack(mdσ(z), axis = 1)], (z.shape[1],WT.shape[0],WT.shape[1]))\n",
    "    dσtT = tf.reshape([tf.transpose(i) for i in tf.unstack(dσt, axis = 0)], (dσt.shape[0],dσt.shape[2],dσt.shape[1]))\n",
    "    t1 = δ * md2σ(z)\n",
    "    t2 = tf.reshape([tf.reshape(i, (i.shape[0], 1)) * WT for i in tf.unstack(t1, axis = 1)], (t1.shape[1],WT.shape[0],WT.shape[1]))\n",
    "    return W @ (mdσ(z) * δ), W @ t2 + dσtT @ ϑ @ dσt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18966",
   "metadata": {},
   "source": [
    "Compute Hessian and gradient of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ad2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian(N, x):\n",
    "#     #x = tf.reshape(x, (1,2))\n",
    "#     output = N(x)\n",
    "#     δ,ϑ = get_gradient_hessian_last_layer(N.out.weights[0], N.out.weights[1], N.tmp_layer_output[-1], 1.)\n",
    "\n",
    "#     for k in range(N.num_hidden_layers-1, -1, -1):\n",
    "#         δ,ϑ = get_gradient_hessian_hidden_layer(N.hidden[k].weights[0], N.hidden[k].weights[1], N.tmp_layer_output[k], δ,  ϑ)\n",
    "      \n",
    "    \n",
    "#     return output, δ, ϑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a866fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 4), dtype=float32, numpy=\n",
       "array([[-1.90928900e+00,  8.90799522e-01, -1.25961691e-01,\n",
       "         6.86312377e-01],\n",
       "       [ 1.88682809e-01, -3.77509475e-01,  5.28218210e-01,\n",
       "        -9.82651301e-03],\n",
       "       [-8.37308228e-01, -3.58557105e-02,  6.73596203e-01,\n",
       "         1.06903851e+00],\n",
       "       [-1.30740955e-01,  9.54806507e-02, -9.97465923e-02,\n",
       "         5.66175149e-04],\n",
       "       [-4.90318447e-01, -5.17469227e-01,  6.80833220e-01,\n",
       "         3.40452582e-01],\n",
       "       [-7.39007413e-01,  2.34413087e-01,  4.27332550e-01,\n",
       "         6.19290471e-01],\n",
       "       [ 1.18752807e-01,  9.86086249e-01, -2.53201455e-01,\n",
       "        -1.09082066e-01],\n",
       "       [ 3.74923944e-01,  2.29168802e-01,  1.29654646e-01,\n",
       "         4.31816161e-01],\n",
       "       [-8.08732808e-01, -8.69572163e-02,  7.19311595e-01,\n",
       "        -1.01024225e-01],\n",
       "       [ 3.41906041e-01,  7.57398754e-02,  1.07020639e-01,\n",
       "         7.41557702e-02],\n",
       "       [ 1.05005622e+00, -5.32367349e-01, -1.11165583e+00,\n",
       "         7.79828906e-01],\n",
       "       [-9.37798500e-01,  8.21404338e-01,  1.98096597e+00,\n",
       "         3.39971572e-01],\n",
       "       [ 1.12021196e+00,  1.14905083e+00, -8.40815902e-01,\n",
       "         9.29868698e-01],\n",
       "       [ 4.09726024e-04, -3.32705915e-01, -1.34221211e-01,\n",
       "         2.70208776e-01],\n",
       "       [-4.59149271e-01, -2.33779922e-01,  2.10580921e+00,\n",
       "        -3.11926991e-01],\n",
       "       [-1.66871071e+00,  6.54619575e-01, -1.43545651e+00,\n",
       "        -2.74446559e+00],\n",
       "       [ 8.08188617e-02, -4.06958133e-01,  6.88210189e-01,\n",
       "         4.32227701e-01],\n",
       "       [ 1.42561361e-01, -1.50598705e-01,  4.30668801e-01,\n",
       "         4.01604176e-01],\n",
       "       [-2.92512514e-02,  1.67451501e-02, -4.05696742e-02,\n",
       "        -1.33758027e-03],\n",
       "       [ 1.93723384e-02,  3.98476534e-02, -1.05053507e-01,\n",
       "        -8.69600698e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal((20,))\n",
    "A = tf.random.normal((20,4))\n",
    "# print(x)\n",
    "# print(A)\n",
    "# print(tf.math.multiply(A,x))\n",
    "# print(tf.math.multiply(x,A))\n",
    "# print(A)\n",
    "# M = tf.tensordot(x, A, axes=0)\n",
    "# print(M)\n",
    "# B = tf.random.normal((3,3))\n",
    "# print(B @ M)\n",
    "# print(M@M)\n",
    "#tf.reshape(x, [-1])\n",
    "#tf.tensordot(tf.transpose(x), A, axes=1)\n",
    "#[x[:,i] * A for i in range(x.shape[0])]\n",
    "# for i in tf.unstack(x, axis = 1):\n",
    "#     print(i*A)\n",
    "# B = tf.random.normal((3,20,20))\n",
    "# C = tf.random.normal((3,20,20))\n",
    "# B@C\n",
    "# print(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20)))\n",
    "# print(tf.transpose(tf.reshape([i * A for i in tf.unstack(x, axis = 1)], (3,20,20))))\n",
    "#print(A)\n",
    "\n",
    "#print(tf.reshape([tf.transpose(i) for i in tf.unstack(A, axis = 0)], (3,4,4)))\n",
    "\n",
    "tf.reshape(x, (x.shape[0], 1))*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01a34d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1.3892365e-05 -3.8011753e-05 -3.7716218e-05 -2.2034983e-05\n",
      "  5.7161840e-05  4.3126987e-05 -3.5517085e-05 -6.4187770e-05\n",
      "  4.9054324e-05  1.1364011e-05], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 1.3892395e-05 -3.8011753e-05 -3.7716232e-05 -2.2034987e-05\n",
      "  5.7161837e-05  4.3126965e-05 -3.5517071e-05 -6.4187763e-05\n",
      "  4.9054313e-05  1.1363991e-05], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.00083098 -0.00098266 -0.00088578 -0.00091257 -0.00090229 -0.00090599\n",
      " -0.00098236 -0.00091943 -0.00086649 -0.00096009], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.00047242 -0.00039382 -0.00045981 -0.00039188 -0.00035929 -0.00039552\n",
      " -0.00039621 -0.00041609 -0.00039977 -0.00040504], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((10,2))\n",
    "\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "\n",
    "print(yxx_ad)\n",
    "print(ϑ[:, 1, 1])\n",
    "print(δ2[0,:])\n",
    "print(δ2[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5129b90",
   "metadata": {},
   "source": [
    "Why do we get a 2D vector when we insert a 2D vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11139e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.7030044e-04 2.0076986e-06 4.6281051e-04 2.0727841e-04 4.8363314e-04\n",
      "  4.4529710e-04 4.7050114e-04 3.3838977e-04 5.0887576e-04 4.3889112e-04]\n",
      " [2.4580392e-03 1.1594698e-03 2.4671643e-03 1.8820576e-03 2.5030558e-03\n",
      "  2.3964171e-03 2.5095581e-03 2.1675020e-03 2.4734978e-03 2.4517295e-03]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NeuralN = PINN()\n",
    "\n",
    "x = tf.random.normal((10,2))\n",
    "\n",
    "#out = NeuralN(x)\n",
    "#print(out)\n",
    "\n",
    "out1, δ1 = NeuralN.get_gradient(x)\n",
    "out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "\n",
    "print(δ1- δ2)\n",
    "print(δ1)\n",
    "#print(tf.reshape(ϑ, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510d2f",
   "metadata": {},
   "source": [
    "-> We need to choose appropriate dtypes so that no operation overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c00e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals1(N, x):\n",
    "\n",
    "#     with tf.GradientTape() as g:\n",
    "#         g.watch(x)\n",
    "#         y = N(x)\n",
    "\n",
    "#     dy_dx = g.gradient(y, x)\n",
    "#     dy_dx = np.transpose(dy_dx.numpy())\n",
    "\n",
    "#     return y, dy_dx\n",
    "\n",
    "def _fvals1(N, t, x):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "                \n",
    "    yt = tape.gradient(y, t)\n",
    "    yx = tape.gradient(y, x)\n",
    "\n",
    "    return y, yt, yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b017a518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.1641532e-10\n",
      "0.0\n",
      "1.1641532e-10\n",
      "0.0\n",
      "1.4551915e-10\n",
      "0.0\n",
      "1.4551915e-10\n",
      "5.820766e-11\n",
      "2.3283064e-10\n",
      "5.820766e-11\n",
      "2.3283064e-10\n",
      "8.731149e-11\n",
      "0.0\n",
      "8.731149e-11\n",
      "0.0\n",
      "1.1641532e-10\n",
      "2.3283064e-10\n",
      "1.1641532e-10\n",
      "2.3283064e-10\n",
      "0.0\n",
      "7.2759576e-11\n",
      "0.0\n",
      "7.2759576e-11\n",
      "1.1641532e-10\n",
      "1.4551915e-11\n",
      "1.1641532e-10\n",
      "1.4551915e-11\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "2.3283064e-10\n",
      "1.7462298e-10\n",
      "2.3283064e-10\n",
      "1.7462298e-10\n",
      "2.3283064e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    x = tf.random.uniform((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out1, δ1 = NeuralN.get_gradient(x)\n",
    "    out2, δ2, ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yt_ad, yx_ad = _fvals1(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ1[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ1[0]-yt_ad))\n",
    "    print(np.linalg.norm(δ2[1]-yx_ad))\n",
    "    print(np.linalg.norm(δ2[0]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e968d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _fvals2(N, x):\n",
    "\n",
    "#     with tf.GradientTape(persistent=True) as h:\n",
    "#         h.watch(x)\n",
    "#         with tf.GradientTape() as g:\n",
    "#             g.watch(x)\n",
    "#             y = N(x)\n",
    "\n",
    "#         dy_dx = g.gradient(y, x)\n",
    "    \n",
    "#     d2y_d2x = h.jacobian(dy_dx, x)\n",
    "\n",
    "#     return y, dy_dx, d2y_d2x\n",
    "\n",
    "def _fvals2_ad(N, t, x):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        tape.watch(x)\n",
    "        tx = tf.stack([t, x], axis=1)\n",
    "        y = N(tx)\n",
    "        yx = tape.gradient(y, x)\n",
    "    \n",
    "    yt = tape.gradient(y, t)\n",
    "    yxx = tape.gradient(yx, x)\n",
    "\n",
    "    return y, yt, yx, yxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109ef374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "8.731149e-11\n",
      "1.0186341e-10\n",
      "1.4551915e-11\n",
      "0.0\n",
      "1.1368684e-11\n",
      "5.820766e-11\n",
      "1.4551915e-11\n",
      "3.274181e-11\n",
      "6.548362e-11\n",
      "3.0013325e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    NeuralN = PINN()\n",
    "    out,δ,ϑ = NeuralN.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(NeuralN, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(ϑ[0, 1, 1]-yxx_ad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffbf82",
   "metadata": {},
   "source": [
    "Maybe gradient tape thinks that the neural network is not differentiable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4fec",
   "metadata": {},
   "source": [
    "# Explicit derivatives of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e683",
   "metadata": {},
   "source": [
    "Here we only approximate the \"half\" gradient so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8a110916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ResNetLayers=2,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='sigmoid',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = ResNetActivation\n",
    "        \n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation = RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = 2\n",
    "        self.output_dim = 1\n",
    "\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        self.tmp_layer_output = [input_tensor]\n",
    "        \n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        \n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            self.tmp_layer_output.append(N)\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        \n",
    "        Phi = self.wb(N, training=training)\n",
    "\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        Phi += .5 * sAs\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "    \n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        δ = get_gradient_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "        \n",
    "        print(δ.shape)\n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ = δ + self.ResNetStepsize * get_gradient_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "\n",
    "\n",
    "        δ = get_gradient_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0]\n",
    "        \n",
    "        \n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        δ,ϑ,z = get_gradient_hessian_layer_ResNet(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "        δ = self.wb.weights[0] + self.ResNetStepsize * δ\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], δ)\n",
    "            t = ϑ + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "            ϑ_new_2 = tf.transpose(t) + self.ResNetStepsize * self.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "            ϑ = ϑ_new_1 + ϑ_new_2\n",
    "            δ = δ + self.ResNetStepsize * δ_new\n",
    "\n",
    "\n",
    "        δ, ϑ = get_gradient_hessian_hidden_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], δ, ϑ)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957d02",
   "metadata": {},
   "source": [
    "Gradient of model, which approximates solution of pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "06b6137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_ResNet(R,x):\n",
    "#     output = R(x)\n",
    "#     δ = get_gradient_layer(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ = δ + R.ResNetStepsize * get_gradient_layer(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "          \n",
    "    \n",
    "#     δ = get_gradient_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd12371",
   "metadata": {},
   "source": [
    "Something is wrong with the 'whole' gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e05e73c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n",
      "tf.Tensor(\n",
      "[[-0.44613445]\n",
      " [ 8.789103  ]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor([-0.4461347], shape=(1,), dtype=float32)\n",
      "tf.Tensor([8.789103], shape=(1,), dtype=float32)\n",
      "2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "Resnet = PINN_ResNet()\n",
    "\n",
    "x = tf.constant([[1., 10.]])\n",
    "\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "\n",
    "print(δ)\n",
    "\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "\n",
    "print(yx_ad)\n",
    "print(yt_ad)\n",
    "\n",
    "print(np.linalg.norm(δ[0] - yx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0e35744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n",
      "0.0\n",
      "1.1920929e-07\n",
      "(16, 1)\n",
      "0.0\n",
      "0.0\n",
      "(16, 1)\n",
      "0.0\n",
      "5.9604645e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "2.3841858e-07\n",
      "(16, 1)\n",
      "2.9802322e-08\n",
      "0.0\n",
      "(16, 1)\n",
      "7.450581e-09\n",
      "0.0\n",
      "(16, 1)\n",
      "2.2351742e-08\n",
      "5.9604645e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "1.4901161e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "2.2351742e-08\n",
      "(16, 1)\n",
      "0.0\n",
      "1.1920929e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ = Resnet.get_gradient(x)\n",
    "    out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(δ[0] - yx_ad))\n",
    "    print(np.linalg.norm(δ[1] - yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "934c44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 19)\n",
      "3.8656955e-07\n",
      "6.891787e-08\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((19,2))\n",
    "Resnet = PINN_ResNet()\n",
    "out, δ = Resnet.get_gradient(x)\n",
    "out_ad, yx_ad, yt_ad = _fvals1(Resnet, x[:,0], x[:,1])\n",
    "print(np.linalg.norm(δ[0,:]-yx_ad))\n",
    "print(np.linalg.norm(δ[1,:]-yt_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0d14a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_hessian_layer_ResNet(W,b,a,δ):\n",
    "#     z1 = np.transpose(a @ W)  \n",
    "#     b = np.reshape(b, np.shape(z1))\n",
    "#     z2 = z1 + b\n",
    "#     z3 = np.diag(tf.reshape(mdσ(z2), [-1])) @ δ\n",
    "    \n",
    "#     z4 = md2σ(z2) * δ\n",
    "#     ϑ = np.diag(tf.reshape(z4, [-1]))\n",
    "    \n",
    "#     return W @ z3, W @ ϑ @ np.transpose(W)\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * δ), W @ ((md2σ(z) * δ) * tf.transpose(W)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "600c9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hessian_ResNet(R,x):\n",
    "#     x = tf.reshape(x, (1,2))\n",
    "#     output = R(x)\n",
    "#     δ,ϑ,z = get_gradient_hessian_layer_ResNet(R.ResNet[-1].weights[0], R.ResNet[-1].weights[1], R.tmp_layer_output[-1], R.wb.weights[0])\n",
    "\n",
    "#     δ = R.wb.weights[0] + R.ResNetStepsize * δ\n",
    " \n",
    "#     for k in range(R.ResNetLayers-2, 0, -1):\n",
    "#         δ_new, ϑ_new_1, z = get_gradient_hessian_layer_ResNet(R.ResNet[k].weights[0], R.ResNet[k].weights[1], R.tmp_layer_output[k], δ)\n",
    "#         t = ϑ + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * ϑ)\n",
    "#         ϑ_new_2 = tf.transpose(t) + R.ResNetStepsize * R.ResNet[k].weights[0] @ ( mdσ(z) * tf.transpose(t))\n",
    "#         ϑ = ϑ_new_1 + ϑ_new_2\n",
    "#         δ = δ + R.ResNetStepsize * δ_new\n",
    "    \n",
    "      \n",
    "#     δ, ϑ = get_gradient_hessian_hidden_layer(R.ResNet[0].weights[0], R.ResNet[0].weights[1], R.tmp_layer_output[0], δ, ϑ)\n",
    "    \n",
    "#     M = R.A.weights[0]\n",
    "    \n",
    "#     return output, δ + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + R.c.weights[0], ϑ + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51c88154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.4901161e-08\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "for i in range(10):\n",
    "    x = tf.random.normal((1,2))\n",
    "    Resnet = PINN_ResNet()\n",
    "    out, δ, ϑ = Resnet.get_gradient_and_hessian(x)\n",
    "    out_ad, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "    print(np.linalg.norm(ϑ[1,1]-yxx_ad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7dd7defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x000001DDC75B6CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5.9604645e-08\n",
      "1.6323405e-07\n",
      "1.4901161e-07\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((10,2))\n",
    "Resnet = PINN_ResNet()\n",
    "result = tf.vectorized_map(Resnet.get_gradient_and_hessian, x)\n",
    "out, yx_ad, yt_ad, yxx_ad = _fvals2_ad(Resnet, x[:,0], x[:,1])\n",
    "# print(result[2][:,0,0])\n",
    "# print(yxx_ad)\n",
    "print(np.linalg.norm(result[2][:,1,1] - yxx_ad))\n",
    "# print(tf.reshape(result[1][:,0], [-1]))\n",
    "# print(tf.concat(yt_ad, 0))\n",
    "print(np.linalg.norm(tf.reshape(result[1][:,1], [-1]) - tf.concat(yt_ad, 0)))\n",
    "print(np.linalg.norm(tf.reshape(result[1][:,0], [-1]) - tf.concat(yx_ad, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27de24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039fc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
