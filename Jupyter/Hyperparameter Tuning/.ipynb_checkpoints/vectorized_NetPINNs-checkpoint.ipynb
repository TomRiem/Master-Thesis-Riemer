{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def mσ(x):\n",
    "    return tf.math.divide(1, 1 + tf.math.exp(tf.math.negative(x)))\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def mdσ(x):\n",
    "    return mσ(x) * (1 - mσ(x))\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def md2σ(x):\n",
    "    return mσ(x) * (1 - mσ(x)) * (1 - 2*mσ(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_hadamard(v,A):\n",
    "    return [tf.reshape(i, (i.shape[0], 1)) * A for i in tf.unstack(v, axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def get_gradient_layer(W,b,a,grad):\n",
    "    return W @ (mdσ(tf.transpose(a @ W + b)) * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def get_gradient_hessian_last_layer(W,b,a,grad):\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     return W @ (mdσ(z) * grad), W @ (md2σ(z) * tf.transpose(W))\n",
    "    z = tf.transpose(a @ W + b)\n",
    "    return W @ (mdσ(z) * grad), W @ tensor_hadamard(grad * md2σ(z),tf.transpose(W)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def get_gradient_hessian_hidden_layer(W,b,a,grad,hess):\n",
    "#     z = tf.transpose(a @ W + b)\n",
    "#     dσt = mdσ(z) * tf.transpose(W)\n",
    "#     return W @ (mdσ(z) * grad), W @ ((grad * md2σ(z)) * tf.transpose(W)) + tf.transpose(dσt) @ hess @ dσt \n",
    "    z = tf.transpose(a @ W + b)\n",
    "    WT = tf.transpose(W)\n",
    "    mdσz = mdσ(z)\n",
    "    dσt = tensor_hadamard(mdσz, WT)\n",
    "    dσtT = tf.transpose(dσt, perm=[0, 2, 1])\n",
    "    t2 = tensor_hadamard(grad * md2σ(z), WT)\n",
    "    return W @ (mdσz * grad), W @ t2 + dσtT @ hess @ dσt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=2,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation='sigmoid',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = lb.shape[0]\n",
    "        self.output_dim = output_dim\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation),\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = []\n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        self.tmp_layer_output.append(Z)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            self.tmp_layer_output.append(Z)\n",
    "            \n",
    "        return self.out(Z)\n",
    "    \n",
    "\n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        grad = get_gradient_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            grad = get_gradient_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], grad)\n",
    "\n",
    "        return output, grad\n",
    "    \n",
    "\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        # x = tf.reshape(x, (1,2))\n",
    "        output = self.call(x)\n",
    "        grad,hess,z = get_gradient_hessian_last_layer(self.out.weights[0], self.out.weights[1], self.tmp_layer_output[-1], np.identity(self.output_dim))\n",
    "\n",
    "        for k in range(self.num_hidden_layers-1, -1, -1):\n",
    "            grad,hess = get_gradient_hessian_hidden_layer(self.hidden[k].weights[0], self.hidden[k].weights[1], self.tmp_layer_output[k], grad,  hess)\n",
    "\n",
    "        return output, grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN_ResNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "                 ResNetLayers=2,\n",
    "                 ResNetNeurons=16,\n",
    "                 ResNetStepsize=1.0,\n",
    "                 ResNetActivation='softplus',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super(PINN_ResNet, self).__init__(**kwargs)\n",
    "        \n",
    "        #RNact = tf.keras.activations.get(ResNetActivation)\n",
    "        #RNact = my_act\n",
    "        RNact = tf.keras.activations.get('sigmoid')\n",
    "\n",
    "        \n",
    "        self.ResNetLayers = ResNetLayers\n",
    "        self.ResNetStepsize = ResNetStepsize\n",
    "\n",
    "        self.ResNet = [tf.keras.layers.Dense(ResNetNeurons,\n",
    "                                        activation=RNact) for _ in range(self.ResNetLayers)]\n",
    "        self.wb = tf.keras.layers.Dense(1)\n",
    "        self.A = tf.keras.layers.Dense(2, use_bias=False)\n",
    "        self.c = tf.keras.layers.Dense(1, use_bias=False)\n",
    "        \n",
    "        #self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = lb.shape[0]\n",
    "        self.output_dim = 1\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        self.tmp_layer_output = [input_tensor]\n",
    "        N = self.ResNet[0](input_tensor, training=training)\n",
    "        for i in range(1, self.ResNetLayers):\n",
    "            self.tmp_layer_output.append(N)\n",
    "            N = N + self.ResNetStepsize * self.ResNet[i](N, training=training)\n",
    "        Phi = self.wb(N, training=training)\n",
    "        #print(input_tensor)\n",
    "        As = self.A(input_tensor, training=training)\n",
    "        #print(As)\n",
    "        sAs = tf.keras.layers.Dot(axes=(1))([input_tensor, As])\n",
    "        #print(sAs)\n",
    "        Phi += .5 * sAs\n",
    "        #print(Phi.shape)\n",
    "        Phi += self.c(input_tensor, training=training)\n",
    "            \n",
    "        return Phi\n",
    "        #return self.out(Phi)\n",
    "        \n",
    "    \n",
    "    #@tf.function\n",
    "    def get_gradient(self, x):\n",
    "        output = self.call(x)\n",
    "        grad = get_gradient_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "        grad = self.wb.weights[0] + self.ResNetStepsize * grad\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            grad = grad + self.ResNetStepsize * get_gradient_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], grad)\n",
    "\n",
    "\n",
    "        grad = get_gradient_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], grad)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, grad + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0]\n",
    "     \n",
    "    \n",
    "    #@tf.function\n",
    "    def get_gradient_and_hessian(self, x):\n",
    "        output = self.call(x)\n",
    "        grad,hess,z = get_gradient_hessian_last_layer(self.ResNet[-1].weights[0], self.ResNet[-1].weights[1], self.tmp_layer_output[-1], self.wb.weights[0])\n",
    "\n",
    "        grad = self.wb.weights[0] + self.ResNetStepsize * grad\n",
    "\n",
    "        for k in range(self.ResNetLayers-2, 0, -1):\n",
    "            grad_new, hess_new_1, z = get_gradient_hessian_last_layer(self.ResNet[k].weights[0], self.ResNet[k].weights[1], self.tmp_layer_output[k], grad)\n",
    "            mz = mdσ(z)\n",
    "            t = hess + self.ResNetStepsize * self.ResNet[k].weights[0] @ tensor_hadamard(mz, hess)\n",
    "            tT = tf.transpose(t, perm=[0, 2, 1])\n",
    "            hess_new_2 = tT + self.ResNetStepsize * self.ResNet[k].weights[0] @ tensor_hadamard(mz, tT)\n",
    "            hess = hess_new_1 + hess_new_2\n",
    "            grad = grad + self.ResNetStepsize * grad_new\n",
    "\n",
    "\n",
    "        grad, hess = get_gradient_hessian_hidden_layer(self.ResNet[0].weights[0], self.ResNet[0].weights[1], self.tmp_layer_output[0], grad, hess)\n",
    "\n",
    "        M = self.A.weights[0]\n",
    "\n",
    "        return output, grad + 0.5*tf.transpose(x @ (M + tf.transpose(M))) + self.c.weights[0], hess + 0.5*(M + tf.transpose(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = 'float64'\n",
    "\n",
    "# Lower bounds\n",
    "lb = tf.constant([0, 0], dtype=DTYPE)\n",
    "\n",
    "# Upper bounds\n",
    "ub = tf.constant([10, 1], dtype=DTYPE)\n",
    "\n",
    "pinn = PINN_ResNet(lb=lb, ub=ub)\n",
    "pinn(tf.constant([[1., 1.],[1., 0.], [0., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp=pinn.ResNet[0]\n",
    "tmp.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we rely on the package [NetworkX](https://networkx.org/).\n",
    "An alternative approach would be to employ [igraph](https://igraph.org/), which seems to be more lightweight.\n",
    "\n",
    "Next, we set up the class for a graph PINN which takes either an adjacency matrix $A$ or a `MultiDiGraph` as input `inp` to define the network as well as the Dirichlet data as a list of pairs $(i, u_i)_{i=1}^{n_D}$ corresponding to $u(v_i) = u_i$.\n",
    "*Notes*:\n",
    "- we define our graph as a `MultiDiGraph` since graphs imported from [OpenStreetMap](https://www.openstreetmap.de/) obey this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class GraphPINN():\n",
    "    \n",
    "    def __init__(self, inp, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub):\n",
    "        \n",
    "        if isinstance(inp, nx.classes.multidigraph.MultiGraph):\n",
    "            self.G = inp\n",
    "            self.A = nx.adjacency_matrix(G).toarray()\n",
    "            \n",
    "        elif tf.is_tensor(A) and len(A.shape)==2:\n",
    "            # Store adjacency matrix\n",
    "            self.A = A\n",
    "            # Define networkx multigraph\n",
    "            self.G = nx.MultiDiGraph(A.numpy())\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Check input type of GraphPINN.')\n",
    "        \n",
    "        self.dirichletNodes = dirichletNodes\n",
    "        self.dirichletAlpha = dirichletAlpha\n",
    "        self.dirichletBeta = dirichletBeta\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        # Get number of vertices\n",
    "        self.n_v = self.A.shape[0]\n",
    "        \n",
    "        # Determine lists of edges and lengths\n",
    "        self._determineEdgeList()\n",
    "        \n",
    "        # Determine list of vertices and incoming as well as outgoing edges\n",
    "        self._determineVertexList()\n",
    "        \n",
    "        # Setup list of neural networks\n",
    "        self._setupNNs()\n",
    "        \n",
    "        # Determine graph layout if necessary\n",
    "        self.pos = nx.kamada_kawai_layout(self.G)\n",
    "        \n",
    "    def _determineEdgeList(self):\n",
    "        \"\"\"Determine edge matrix and weight vector.\n",
    "        This could also be accomplished by a loop over `G.edges`:\n",
    "            for e in G.edges:\n",
    "                print(e)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.E = []\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(self.n_v):\n",
    "            for j in range(i + 1, self.n_v):\n",
    "                aij = self.A[i, j]\n",
    "                if aij > 0:\n",
    "                    #print('Connectivity between i: {} and j: {}'.format(i,j))\n",
    "                    self.E.append([i, j])\n",
    "                    self.W.append(aij.numpy())\n",
    "\n",
    "        # Get number of edges\n",
    "        self.ne = len(self.E)\n",
    "        \n",
    "    def _determineVertexList(self):\n",
    "        \n",
    "        self.Vin = [[] for _ in range(self.n_v)]\n",
    "        self.Vout = [[] for _ in range(self.n_v)]\n",
    "        \n",
    "        for i, e in enumerate(self.E):\n",
    "            # Unpack edge\n",
    "            vin, vout = e\n",
    "            self.Vin[vout].append(i)\n",
    "            self.Vout[vin].append(i)            \n",
    "            \n",
    "        self.innerVertices = np.setdiff1d(np.arange(self.n_v), self.dirichletNodes)\n",
    "            \n",
    "    def _setupNNs(self):\n",
    "        \n",
    "        self.NNs = []\n",
    "        for i, e in enumerate(self.E):\n",
    "            self.NNs.append(PINN_ResNet(lb=self.lb, ub=self.ub))\n",
    "    \n",
    "        print('Initialized {:d} neural nets.'.format(len(self.NNs)))\n",
    "    \n",
    "    def plotGraph(self, **kwargs):\n",
    "\n",
    "        nx.draw(self.G, pos=self.pos, with_labels=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a network which is determined by its\n",
    "adjacency matrix $A \\in \\mathbb{R}_+^{n_v \\times n_v}$ with $n_v$ the number of vertices.\n",
    "Note that this matrix is not symmetric, as it belongs to a *directed* graph.\n",
    "Here, an entry $a_{i,j} > 0$ indicates that there is an edge starting in vertex $i$ and ending in vertex $j$ with length $a_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTYPE = 'float32'\n",
    "DTYPE = 'float64'\n",
    "adj = 5\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "time_dependent = False\n",
    "\n",
    "# Specify adjacency matrix\n",
    "if adj == 1:\n",
    "    A = tf.constant([[0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 1.\n",
    "    xmin = 0.\n",
    "    xmax = 0.1\n",
    "    \n",
    "    u0 = lambda x : 0\n",
    "    dirichletNodes = np.array([0, 7])\n",
    "    #dirichletVals = np.array([1.0, 0.5])\n",
    "    dirichletAlpha = np.array([0.8, 0.0])\n",
    "    dirichletBeta = np.array([0.0, 0.5])\n",
    "                              \n",
    "    \n",
    "    eps = 0.01\n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "\n",
    "    \n",
    "elif adj == 2:\n",
    "    A = tf.constant([[0, 1],\n",
    "                 [0, 0]], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    eps = .01\n",
    "\n",
    "    dirichletNodes = np.array([0, 1])\n",
    "    dirichletAlpha = np.array([0.7, 0.0])\n",
    "    dirichletBeta = np.array([0.0, 0.8])\n",
    "    \n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return 0.\n",
    "        #return .5 +(.5*-tf.sin(x*np.pi))\n",
    "    \n",
    "elif adj == 3:\n",
    "    A = tf.constant([[0, 1, 0],\n",
    "                     [0, 0, 1],\n",
    "                     [0, 0, 0]], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 0.5\n",
    "    \n",
    "    eps = .01\n",
    "\n",
    "    dirichletNodes = np.array([0, 2])\n",
    "    \n",
    "    dirichletAlpha = np.array([0.7, 0.0])\n",
    "    dirichletBeta = np.array([0.0, 0.8])\n",
    "    #time_dependent = True\n",
    "    #dirichletAlpha = np.array([lambda x: 0.7*x/10, lambda x: 0.0*x])\n",
    "    #dirichletBeta = np.array([lambda x: 0.0, lambda x: 0.8*x/10])\n",
    "    \n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return 0.\n",
    "    \n",
    "elif adj == 4:\n",
    "    A = tf.constant([[0, 1, 0, 0],\n",
    "                     [0, 0, 1, 1],\n",
    "                     [0, 0, 0, 0],\n",
    "                     [0, 0, 0, 0]], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    eps = .01\n",
    "\n",
    "    dirichletNodes = np.array([0, 2, 3])\n",
    "    dirichletAlpha = np.array([0.7, 0.0, 0.])\n",
    "    dirichletBeta = np.array([0.0, 0.0, 0.0])\n",
    "    \n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return 0.\n",
    "    \n",
    "elif adj == 0:\n",
    "    A = tf.constant([[0, 0, 1,0],\n",
    "                 [0, 0, 1, 0],\n",
    "                 [0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0]], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 4.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    u0 = lambda x : 0\n",
    "    dirichletNodes = np.array([0, 1, 3])\n",
    "    #dirichletVals = np.array([1.0, 0.5])\n",
    "    dirichletAlpha = np.array([0.8, 0.8, 0.])\n",
    "    dirichletBeta = np.array([0.0, 0.0, 0.])\n",
    "                              \n",
    "    \n",
    "    eps = 1e-6\n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "elif adj == 5:\n",
    "    A = tf.constant([[0, 0, 1, 0, 0, 0],\n",
    "                     [0, 0, 1, 0, 0, 0],\n",
    "                     [0, 0, 0, 1, 0, 0],\n",
    "                     [0, 0, 0, 0, 1, 1],\n",
    "                     [0, 0, 0, 0, 0, 0],\n",
    "                     [0, 0, 0, 0, 0, 0]], dtype=tf.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    dirichletNodes = np.array([0, 1, 4, 5])\n",
    "    #dirichletVals = np.array([1.0, 0.5])\n",
    "    dirichletAlpha = np.array([0.9, 0.3, 0., 0.])\n",
    "    dirichletBeta = np.array([0.0, 0.0, 0.8, 0.1])\n",
    "                              \n",
    "    eps = 1e-2\n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return 0.\n",
    "\n",
    "\n",
    "# Lower bounds\n",
    "lb = tf.constant([tmin, xmin], dtype=DTYPE)\n",
    "\n",
    "# Upper bounds\n",
    "ub = tf.constant([tmax, xmax], dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "gPINN = GraphPINN(A, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub)\n",
    "gPINN.plotGraph()\n",
    "print('Vin:', gPINN.Vin)\n",
    "print('Vout:', gPINN.Vout)\n",
    "gPINN.E\n",
    "gPINN.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw collocation points uniformly or take them equidistantly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'deterministic'\n",
    "mode = 'uniform'\n",
    "\n",
    "N_0 = 1000\n",
    "N_b = 1000\n",
    "N_r = 4000\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "def drawCollocationPoints(N_0, N_b, N_r):\n",
    "    X_r = tf.random.uniform((N_r,2), lb, ub, dtype=DTYPE)\n",
    "    \n",
    "    # Draw uniform sample points for initial boundary data\n",
    "    t_0 = tf.ones((N_0,1), dtype=DTYPE)*lb[0]\n",
    "    x_0 = tf.random.uniform((N_0,1), lb[1], ub[1], dtype=DTYPE)\n",
    "    X_0 = tf.concat([t_0, x_0], axis=1)\n",
    "    \n",
    "    # Boundary data\n",
    "    t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
    "    x_l = tf.ones((N_b,1), dtype=DTYPE) * lb[1]\n",
    "    X_l = tf.concat([t_b, x_l], axis=1)\n",
    "    \n",
    "    x_u = tf.ones((N_b,1), dtype=DTYPE) * ub[1]\n",
    "    X_u = tf.concat([t_b, x_u], axis=1)\n",
    "    \n",
    "    X_b = tf.concat([X_l, X_u], axis=0)\n",
    "\n",
    "    # Draw uniformly sampled collocation points\n",
    "    t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
    "    x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
    "    X_r = tf.concat([t_r, x_r], axis=1)\n",
    "    # X_data = tf.concat([X_0, X_b, X_r], axis=0)\n",
    "    return X_r, X_0, X_l, X_u\n",
    "\n",
    "if mode == 'deterministic':\n",
    "\n",
    "    # Uniform distributed collocation points\n",
    "    t_r = tf.linspace(lb[0], ub[0], N_0+1)\n",
    "    x_r = tf.linspace(lb[1], ub[1], N_b+1)\n",
    "    tt, xx = tf.meshgrid(t_r,x_r)\n",
    "    X_r = tf.concat([tf.reshape(tt,(-1,1)), tf.reshape(xx,(-1,1))], axis=1)\n",
    "\n",
    "elif mode == 'uniform':\n",
    "    \n",
    "    X_r, X_0, X_l, X_u = drawCollocationPoints(N_0, N_b, N_r)\n",
    "    #X_data = tf.concat([X_0, X_b, X_r], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw collocation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_r[:,0].numpy(), X_r[:,1].numpy(),color='red',alpha=0.3)\n",
    "ax.scatter(X_0[:,0].numpy(), X_0[:,1].numpy(), color='blue', alpha=0.5)\n",
    "ax.scatter(X_u[:,0].numpy(), X_u[:,1].numpy(), color='cyan', alpha=0.5)\n",
    "ax.scatter(X_l[:,0].numpy(), X_l[:,1].numpy(), color='cyan', alpha=0.5)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up class for time-dependent equations\n",
    "\n",
    "We have to handle different kinds of boundary and vertex conditions:\n",
    "- all inner vertices: Kirchhoff-Neumann conditon\n",
    "- initial time conditions on all edges\n",
    "- Dirichlet conditions on selected vertices as long as it is an inflowing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the functions (_fvals1, _fvals2) that explicitly calculate the derivatives for us. For this we use 'tf.vectorized_map', which decomposes a tensor along the axis 0, applies the given functions (get_gradient, get_gradient_and_hessian) to the individual rows and returns a nested tensor with the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "class graphPINNSolver(object):\n",
    "    def __init__(self, graphPINN, Xr, X0, Xl, Xu):\n",
    "        \n",
    "        self.graphPINN = graphPINN\n",
    "        self.ne = self.graphPINN.ne\n",
    "        \n",
    "        self._updateData(Xr, X0, Xl, Xu)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist = []\n",
    "        self.iter = 0\n",
    "        \n",
    "        # Call each network once to initialize trainable variables\n",
    "        self.trainable_variables = []\n",
    "        for i in range(self.ne):\n",
    "            self.graphPINN.NNs[i](tf.constant([[1., 1.]], dtype=DTYPE))\n",
    "            self.trainable_variables.append(self.graphPINN.NNs[i].trainable_variables)\n",
    "        \n",
    "        # Setup auxiliary variables for vertex values to ensure continuity\n",
    "        self._setupVertexVariables()\n",
    "        \n",
    "        for i, v in enumerate(self.graphPINN.innerVertices):\n",
    "            self.trainable_variables.append([self.vertexVals[i]])\n",
    "        \n",
    "        self.nvar = len(self.trainable_variables)\n",
    "        \n",
    "    def _updateData(self, Xr, X0, Xl, Xu):\n",
    "        self.Xr = Xr\n",
    "        self.X0 = X0\n",
    "        self.Xl = Xl\n",
    "        self.Xu = Xu\n",
    "        \n",
    "        self.nr = Xr.shape[0]\n",
    "        self.n0 = X0.shape[0]\n",
    "        self.nb = Xl.shape[0]\n",
    "\n",
    "    def _setupVertexVariables(self):\n",
    "        \n",
    "        self.vertexVals = []\n",
    "        for _ in self.graphPINN.innerVertices:\n",
    "            self.vertexVals.append(tf.Variable(tf.random.uniform(shape=(self.nb,), dtype=DTYPE), trainable=True))\n",
    "\n",
    "    def _fvals0(self, tx):\n",
    "\n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        for i in range(self.ne):\n",
    "            u.append(self.graphPINN.NNs[i](tx)[:,0])\n",
    "\n",
    "        return u\n",
    "    \n",
    "    \n",
    "    #@tf.function\n",
    "    def _fvals1(self, tx):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            output, gradient = self.graphPINN.NNs[i].get_gradient(tx)\n",
    "            u.append(tf.reshape(output, [-1]))\n",
    "            ut.append(gradient[0,:])\n",
    "            ux.append(gradient[1,:])\n",
    "                            \n",
    "#         return [tf.concat(u, 0)], [tf.concat(ut, 0)], [tf.concat(ux, 0)]\n",
    "        return u, ut, ux\n",
    "\n",
    "    \n",
    "    def _fvals1_vec(self, tx):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            result = tf.vectorized_map(self.graphPINN.NNs[i].get_gradient, tx)\n",
    "            u.append(tf.reshape(result[0], [-1]))\n",
    "            ut.append(tf.reshape(result[1][:,0], [-1]))\n",
    "            ux.append(tf.reshape(result[1][:,1], [-1]))\n",
    "                            \n",
    "        return [tf.concat(u, 0)], [tf.concat(ut, 0)], [tf.concat(ux, 0)]\n",
    "    \n",
    "    \n",
    "    #@tf.function\n",
    "    def _fvals2(self, tx):\n",
    "        \n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        uxx = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            output, grad, hess = self.graphPINN.NNs[i].get_gradient_and_hessian(tx)\n",
    "            u.append(tf.reshape(output, [-1]))\n",
    "            ut.append(tf.reshape(grad[0,:], [-1]))\n",
    "            ux.append(tf.reshape(grad[1,:], [-1]))\n",
    "            uxx.append(hess[:,1,1])\n",
    "                            \n",
    "#         return [tf.concat(u, 0)], [tf.concat(ut, 0)], [tf.concat(ux, 0)], [tf.concat(uxx, 0)]\n",
    "        return u, ut, ux, uxx\n",
    "    \n",
    "\n",
    "    def _fvals2_vec(self, tx):\n",
    "        \n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        uxx = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            result = tf.vectorized_map(self.graphPINN.NNs[i].get_gradient_and_hessian, tx)\n",
    "            u.append(tf.reshape(result[0], [-1]))\n",
    "            ut.append(tf.reshape(result[1][:,0], [-1]))\n",
    "            ux.append(tf.reshape(result[1][:,1], [-1]))\n",
    "            uxx.append(result[2][:,1,1])\n",
    "                            \n",
    "        return [tf.concat(u, 0)], [tf.concat(ut, 0)], [tf.concat(ux, 0)], [tf.concat(uxx, 0)]\n",
    "    \n",
    "    \n",
    "    def _fvals2_for(self, tx):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        uxx = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            for j in range(tx.shape[0]):\n",
    "                output, gradient, hessian = self.graphPINN.NNs[i].get_gradient_and_hessian(tf.reshape(tx[j,:], [1,2]))\n",
    "                u.append(output)\n",
    "                ut.append(gradient[0])\n",
    "                ux.append(gradient[1])\n",
    "                uxx.append(hessian[0, 0])\n",
    "\n",
    "                            \n",
    "        return u, ut, ux, uxx\n",
    "    \n",
    "    def _fvals2_ad(self, t, x):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        ut = []\n",
    "        uxx = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variables representing t and x during this GradientTape\n",
    "                tape.watch(t)\n",
    "                tape.watch(x)\n",
    "\n",
    "                tx = tf.stack([t, x], axis=1)\n",
    "\n",
    "                # Compute current values u(t,x)\n",
    "                u.append(self.graphPINN.NNs[i](tx)[:,0])\n",
    "                ux.append(tape.gradient(u[i], x))\n",
    "                \n",
    "            ut.append(tape.gradient(u[i], t))\n",
    "            uxx.append(tape.gradient(ux[i], x))\n",
    "            \n",
    "            del tape\n",
    "        \n",
    "        return u, ut, ux, uxx\n",
    "    \n",
    "    def determine_losses(self):\n",
    "        # Short-hand notation of mean-squared loss\n",
    "        mse = lambda x : tf.reduce_mean(tf.square(x))\n",
    "        \n",
    "        ###################################\n",
    "        ### Residual loss for all edges ###\n",
    "        ###################################\n",
    "        # u, ut, ux, uxx = self._fvals2(self.Xr[:,0], self.Xr[:,1])\n",
    "        u, ut, ux, uxx = self._fvals2(self.Xr)\n",
    "        \n",
    "        loss_res = 0\n",
    "        for i in range(self.ne):\n",
    "            res_e = pde(u[i], ut[i], ux[i], uxx[i])\n",
    "            loss_res += mse(res_e)\n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### Initial conds for all edges ###\n",
    "        ###################################\n",
    "        \n",
    "        # u = self._fvals0(self.X0[:,0], self.X0[:,1])\n",
    "        u = self._fvals0(self.X0)\n",
    "        \n",
    "        loss_init = 0\n",
    "        for i in range(self.ne):\n",
    "            res_e = u[i] - initial_cond(self.X0[:,1])\n",
    "            loss_init += mse(res_e)\n",
    "        \n",
    "        ###################################\n",
    "        ###   Continuity in vertices    ###\n",
    "        ###################################\n",
    "        \n",
    "        # ul, ult, ulx = self._fvals1(self.Xl[:,0], self.Xl[:,1])\n",
    "        # uu, uut, uux = self._fvals1(self.Xu[:,0], self.Xu[:,1])\n",
    "        ul, ult, ulx = self._fvals1(self.Xl)\n",
    "        uu, uut, uux = self._fvals1(self.Xu)\n",
    "        \n",
    "        \n",
    "        loss_cont = 0\n",
    "        \n",
    "        for i, v in enumerate(self.graphPINN.innerVertices):\n",
    "            \n",
    "            for j in self.graphPINN.Vin[v]:\n",
    "                val = uu[j] - self.vertexVals[i]\n",
    "                loss_cont += mse(val)\n",
    "\n",
    "            for j in self.graphPINN.Vout[v]:\n",
    "                val = ul[j] - self.vertexVals[i]\n",
    "                loss_cont += mse(val)\n",
    "\n",
    "        #####################################\n",
    "        ### Kirchhoff-Neumann in vertices ###\n",
    "        #####################################\n",
    "        \n",
    "        # Kirchhoff-Neumann condition in center nodes\n",
    "        loss_KN = 0\n",
    "        for i in self.graphPINN.innerVertices:\n",
    "            \n",
    "            val = 0\n",
    "            print('Kirchhoff-Neumann in node ', i)\n",
    "            for j in self.graphPINN.Vin[i]:\n",
    "                print('incoming edge:', j)\n",
    "                val += flux(uu[j], uux[j])\n",
    "                \n",
    "            for j in self.graphPINN.Vout[i]:\n",
    "                print('outgoing edge:', j)\n",
    "                val -= flux(ul[j], ulx[j])\n",
    "            loss_KN += mse(val)\n",
    "        \n",
    "        #####################################\n",
    "        ###      Inflow/Outflow conds     ###\n",
    "        #####################################\n",
    "            \n",
    "        loss_D = 0\n",
    "        for i,v in enumerate(self.graphPINN.dirichletNodes):\n",
    "            \n",
    "            if time_dependent:\n",
    "                alpha = self.graphPINN.dirichletAlpha[i](self.Xl[:,0])\n",
    "                beta = self.graphPINN.dirichletBeta[i](self.Xl[:,0])\n",
    "            else:\n",
    "                alpha = self.graphPINN.dirichletAlpha[i]\n",
    "                beta = self.graphPINN.dirichletBeta[i]\n",
    "            \n",
    "            print('\\nin node ', v, 'alpha ', alpha, 'beta ', beta)\n",
    "            val = 0\n",
    "            for j in self.graphPINN.Vin[v]:\n",
    "                print('outflow: ', j)\n",
    "                val += flux(uu[j], uux[j]) \\\n",
    "                    - beta * (uu[j])\n",
    "                #loss_D += mse(val)\n",
    "\n",
    "            for j in self.graphPINN.Vout[v]:\n",
    "                print('inflow: ', j)\n",
    "                val += -flux(ul[j], ulx[j]) \\\n",
    "                    + alpha * (1-ul[j])\n",
    "                #loss_D += mse(val)\n",
    "            loss_D += mse(val)\n",
    "        print(loss_res)\n",
    "        print(loss_init)\n",
    "        print(loss_cont)\n",
    "        print(loss_KN)\n",
    "        print(loss_D)\n",
    "        return loss_res, loss_init, loss_cont, loss_KN, loss_D\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        \n",
    "        loss_res, loss_init, loss_cont, loss_KN, loss_D = self.determine_losses()\n",
    "        \n",
    "        loss = loss_res + loss_init + loss_cont + loss_KN + loss_D\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with\n",
    "            # respect to trainable variables\n",
    "            tape.watch(self.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "\n",
    "        g = tape.gradient(loss, self.trainable_variables)\n",
    "        del tape\n",
    "\n",
    "        return loss, g\n",
    "    \n",
    "    \n",
    "    def solve_with_TFoptimizer(self, optimizer, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        self.callback_init()\n",
    "        \n",
    "        for i in range(N):\n",
    "            loss, g = self.get_grad()\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            for j in range(self.nvar):\n",
    "                optimizer.apply_gradients(zip(g[j], self.trainable_variables[j]))\n",
    "\n",
    "            self.current_loss = loss.numpy()\n",
    "            self.callback()\n",
    "            \n",
    "    def solve_with_ScipyOptimizer(self, method='L-BFGS-B', **kwargs):\n",
    "        \"\"\"This method provides an interface to solve the learning problem\n",
    "        using a routine from scipy.optimize.minimize.\n",
    "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
    "        supported in Tensorflow 2.xx.)\n",
    "        Type conversion is necessary since scipy-routines are written in\n",
    "        Fortran which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
    "\n",
    "        def get_weight_tensor():\n",
    "            \"\"\"Function to return current variables of the model\n",
    "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
    "\n",
    "            weight_list = []\n",
    "            shape_list = []\n",
    "\n",
    "            # Loop over all variables, i.e. weight matrices, bias vectors\n",
    "            # and unknown parameters\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for v in self.trainable_variables[i]:\n",
    "                    shape_list.append(v.shape)\n",
    "                    weight_list.extend(v.numpy().flatten())\n",
    "\n",
    "            weight_list = tf.convert_to_tensor(weight_list)\n",
    "            return weight_list, shape_list\n",
    "\n",
    "\n",
    "        x0, shape_list = get_weight_tensor()\n",
    "\n",
    "        def set_weight_tensor(weight_list):\n",
    "            \"\"\"Function which sets list of weights\n",
    "            to variables in the model.\"\"\"\n",
    "            idx = 0\n",
    "\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for v in self.trainable_variables[i]:\n",
    "                    vs = v.shape\n",
    "\n",
    "                    # Weight matrices\n",
    "                    if len(vs) == 2:\n",
    "                        sw = vs[0] * vs[1]\n",
    "                        new_val = tf.reshape(\n",
    "                            weight_list[idx:idx + sw], (vs[0], vs[1]))\n",
    "                        idx += sw\n",
    "\n",
    "                    # Bias vectors\n",
    "                    elif len(vs) == 1:\n",
    "                        new_val = weight_list[idx:idx+vs[0]]\n",
    "                        idx += vs[0]\n",
    "\n",
    "                    # Variables (in case of parameter identification setting)\n",
    "                    elif len(vs) == 0:\n",
    "                        new_val = weight_list[idx]\n",
    "                        idx += 1\n",
    "\n",
    "                    # Assign variables (Casting necessary since scipy requires float64 type)\n",
    "                    v.assign(tf.cast(new_val, DTYPE))\n",
    "\n",
    "        def get_loss_and_grad(w):\n",
    "            \"\"\"Function that provides current loss and gradient\n",
    "            w.r.t the trainable variables as vector. This is mandatory\n",
    "            for the LBFGS minimizer from tfp.optimizer.\"\"\"\n",
    "\n",
    "            # Update weights in model\n",
    "            set_weight_tensor(w)\n",
    "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
    "            loss, grad = self.get_grad()\n",
    "            # Flatten gradient\n",
    "            grad_flat = []\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for g in grad[i]:\n",
    "                    grad_flat.extend(g.numpy().flatten())\n",
    "\n",
    "            # Store current loss for callback function\n",
    "            self.current_loss = loss\n",
    "\n",
    "            # Return value and gradient of \\phi as tuple\n",
    "            return loss.numpy().astype(np.float64), np.array(grad_flat, dtype=np.float64)\n",
    "\n",
    "        self.callback_init()\n",
    "\n",
    "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
    "                                       x0=x0,\n",
    "                                       jac=True,\n",
    "                                       method=method,\n",
    "                                       callback=self.callback,\n",
    "                                       **kwargs)\n",
    "    \n",
    "    def callback_init(self):\n",
    "        self.t0 = time()\n",
    "        print(' Iter            Loss    Time')\n",
    "        print('-----------------------------')\n",
    "    \n",
    "    def callback(self, xr=None):\n",
    "        if self.iter % 20 == 0:\n",
    "            print('{:05d}  {:10.8e}   {:4.2f}'.format(\n",
    "                self.iter, self.current_loss, time() - self.t0))\n",
    "        self.hist.append(self.current_loss)\n",
    "        self.iter += 1\n",
    "        \n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "        ax.semilogy(range(len(self.hist)), self.hist, 'k-')\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.graphPINN.NNs[0](tf.constant([[1., 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gPINN = GraphPINN(A, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub)\n",
    "N_0 = 1000\n",
    "N_b = 1000\n",
    "N_r = 4000\n",
    "solver = graphPINNSolver(gPINN, X_r, X_0, X_l, X_u)\n",
    "X_r, X_0, X_l, X_u = drawCollocationPoints(N_0, N_b, N_r)\n",
    "#lr = 0.01\n",
    "#optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#print('Start with TF optimizer\\n')\n",
    "#solver.solve_with_TFoptimizer(optim, N=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check accuracy of explicit derivatives in relation to gradient tape (automatic differentiation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "X_test = tf.random.uniform((4000,2), dtype=DTYPE)\n",
    "t1 = time()\n",
    "u, ut, ux, uxx = solver._fvals2(X_test)\n",
    "tdiff1 = time() - t1\n",
    "print(tdiff1)\n",
    "t2 = time()\n",
    "u_ad, ut_ad, ux_ad, uxx_ad = solver._fvals2_ad(X_test[:,0], X_test[:,1])\n",
    "tdiff2 = time() - t2\n",
    "print(tdiff2)\n",
    "print(tdiff1/tdiff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(u)\n",
    "print(ut)\n",
    "print(ux)\n",
    "print(uxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u_ad)\n",
    "print(ut_ad)\n",
    "print(ux_ad)\n",
    "print(uxx_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tf.linalg.norm(tf.concat(u, 0)-tf.concat(u_ad, 0)))\n",
    "print(tf.linalg.norm(tf.concat(ut, 0)-tf.concat(ut_ad, 0)))\n",
    "print(tf.linalg.norm(tf.concat(ux, 0)-tf.concat(ux_ad, 0)))\n",
    "print(tf.linalg.norm(tf.concat(uxx, 0)-tf.concat(uxx_ad, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question remains what is more accurate for the second partial derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eidx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "Nt = 60\n",
    "Nx = 120\n",
    "\n",
    "tspace = tf.linspace(lb[0], ub[0], Nt + 1)\n",
    "xspace = tf.reshape(tf.linspace(lb[1], ub[1], Nx + 1), [-1, 1])\n",
    "T, X = tf.meshgrid(tspace, xspace)\n",
    "TX=tf.stack([tf.reshape(T,-1), tf.reshape(X,-1)],axis=1)\n",
    "U = solver.graphPINN.NNs[Eidx](TX)\n",
    "U = tf.reshape(U, T.shape)\n",
    "ax.plot_surface(T, X, U, cmap='viridis')\n",
    "ax.view_init(19, 135)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solve with Adam optimizer\n",
    "lr = 0.01\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "print('Start with TF optimizer\\n')\n",
    "solver.solve_with_TFoptimizer(optim, N=2001)\n",
    "#for i in range(20):\n",
    "    # WARNING: Update of collocation points currently not supported\n",
    "    # Vertex values are fixed for coordinates at boundary!\n",
    "    #X_r, X_0, X_l, X_u = drawCollocationPoints(N_0, N_b, N_r)\n",
    "    #solver._updateData(X_r, X_0, X_l, X_u)\n",
    "    #solver.solve_with_TFoptimizer(optim, N=301)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End with L-BFGS-B algorithm')\n",
    "coarse = False\n",
    "\n",
    "#X_r, X_0, X_l, X_u = drawCollocationPoints(N_0, N_b, N_r*1)\n",
    "#solver._updateData(X_r, X_0, X_l, X_u)\n",
    "if coarse:\n",
    "    ret = solver.solve_with_ScipyOptimizer(options={'maxiter': 5000,\n",
    "                                     'maxfun': 50000,\n",
    "                                     'maxcor': 50,\n",
    "                                     'maxls': 50,\n",
    "                                     'ftol': 1e-12})\n",
    "else:\n",
    "    ret = solver.solve_with_ScipyOptimizer(options={'maxiter': 5000,\n",
    "                                     'maxfun': 50000,\n",
    "                                     'maxcor': 50,\n",
    "                                     'maxls': 50,\n",
    "                                     'ftol': 1.0*np.finfo(float).eps})\n",
    "print(ret.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "fig = plt.figure(1, figsize=(10, 10))\n",
    "#fig.canvas.layout.width = '100%'\n",
    "#fig.canvas.layout.height = '900px'\n",
    "Nt = 60\n",
    "Nx = 120\n",
    "\n",
    "tspace = tf.linspace(lb[0], ub[0], Nt + 1)\n",
    "xspace = tf.reshape(tf.linspace(lb[1], ub[1], Nx + 1), [-1, 1])\n",
    "def plot_network(j=0):\n",
    "    fig = plt.figure(1,clear=True)\n",
    "    ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "    t0 = tf.ones_like(xspace)*tspace[j]\n",
    "    pos = solver.graphPINN.pos\n",
    "    for i, e in enumerate(solver.graphPINN.E):\n",
    "        xy = pos[e[0]] + xspace*(pos[e[1]] - pos[e[0]]) / (ub[1]-lb[1])\n",
    "        #xy = pos[e[0].numpy()] + xspace*(pos[e[1].numpy()] - pos[e[0].numpy()]) \n",
    "        #xy = V[e[0].numpy()]+xspace*(V[e[1].numpy()]-V[e[0].numpy()])\n",
    "        u = solver.graphPINN.NNs[i](tf.concat([t0,xspace],axis=1))\n",
    "        unum = u.numpy().flatten()\n",
    "        ax.plot(xy[:,0], xy[:,1], unum)\n",
    "        #ax.plot(xy[:,0], xy[:,1], unum * (1-unum))\n",
    "    \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlim([.0,1.0])\n",
    "    #ax.set_zlabel('$u_\\\\theta(x,y)$')\n",
    "    ax.view_init(12, 135)\n",
    "    return u\n",
    "\n",
    "j_slider = widgets.IntSlider(min=0,max=Nt,step=1)\n",
    "interactive_plot = interactive(plot_network, j=j_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "#output.layout.height = '350px'\n",
    "interactive_plot\n",
    "#u = plot_network(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface plot of function values on a single edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eidx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "Nt = 60\n",
    "Nx = 120\n",
    "\n",
    "tspace = tf.linspace(lb[0], ub[0], Nt + 1)\n",
    "xspace = tf.reshape(tf.linspace(lb[1], ub[1], Nx + 1), [-1, 1])\n",
    "T, X = tf.meshgrid(tspace, xspace)\n",
    "TX=tf.stack([tf.reshape(T,-1), tf.reshape(X,-1)],axis=1)\n",
    "U = solver.graphPINN.NNs[Eidx](TX) #-  solver.graphPINN.NNs[1](TX)\n",
    "U = tf.reshape(U, T.shape)\n",
    "ax.plot_surface(T, X, U, cmap='viridis')\n",
    "ax.view_init(19, 135)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
