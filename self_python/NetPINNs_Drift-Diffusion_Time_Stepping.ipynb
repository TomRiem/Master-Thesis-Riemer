{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs on Graphs\n",
    "\n",
    "This notebook accompanies the paper\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we describe and illustrate the methodology proposed in the aforementioned paper.\n",
    "Here, we deal with the solution of a drift-diffuion equation on a metric graph.\n",
    "A metric graph is an undirected graph that consists of a set of vertices $\\mathcal V$ and edges $\\mathcal E$ where in contrast to combinatorial graphs a length $l_e$ is assigned to each edge $e \\in \\mathcal E$.\n",
    "Each edge $e \\in \\mathcal E$ connects a pair of nodes $(v_{e_a},v_{e_b})$ with $v_{e_a}, v_{e_b} \\in \\mathcal V$\n",
    "\n",
    "We consider the drift-diffusion equation posed on each edge\n",
    "\n",
    "$$\n",
    "\\partial_t u = \\partial_x ( \\varepsilon \\partial_x \\rho_e - f(\\rho_e) \\partial_x V_e), \\quad e \\in \\mathcal E\n",
    "$$\n",
    "\n",
    "where $\\rho_e : [0,l_e] \\times (0,T) \\to \\mathbb{R}_+$ describes, on each edge, the concentration of some quantity while $V_e: [0,l_e] \\times(0,T)\\to \\mathbb{R}_+$ is a given potential and $\\varepsilon > 0$ a given constant, typically small.\n",
    "\n",
    "To make this a well-posed problem, we need a set of initial conditions as well as coupling conditions in the vertices.\n",
    "- For vertices $v \\in \\mathcal V_{\\mathcal K} \\subset \\mathcal V$, we apply homogeneous Neumann-Kirchhoff conditions, i.e., there holds\n",
    "$$\n",
    "\\sum_{e \\in \\mathcal{E}_v} J_e \\, n_{e} (v) = 0 \\quad v \\in \\mathcal V_{\\mathcal K},\n",
    "$$\n",
    "where we additionally ask the solution to be continuous over the edges, i.e.\n",
    "$$\n",
    "p_e(v) = p_{e^{'}}(v)\n",
    "\\quad \\text{for all } v \\in \\mathcal{V}_{\\mathcal K}, e, e^{'} \\in \\mathcal{E}_v\n",
    "$$\n",
    "with $\\mathcal E_v$ the edge set incident to the vertex $v$.\n",
    "- For vertices $v \\in \\mathcal V_{\\mathcal D} := \\mathcal V \\setminus \\mathcal{V}_{\\mathcal K}$ the solution fulfills flux boundary conditions\n",
    "$$\n",
    "J_e \\, n_e (v) = - \\alpha_v \\, (1-u_v) + \\beta_v \\, u_v \\quad \\text{for all } e \\in \\mathcal E_v.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as netx\n",
    "from time import time\n",
    "DTYPE='float64'\n",
    "timestepmode = 'implicit'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up class GraphPINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the class `GraphPINN`, which is employed in the subsequent code.\n",
    "The idea is to have one vanilla `PINN` for each edge which are connected via boundary and vertex conditions which are enforced weakly.\n",
    "\n",
    "---\n",
    "\n",
    "First, we define the model for a `PINN` which consists of one scaling layer, a number of fully connected hidden layers and one final output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=3,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = lb.shape[0]\n",
    "        self.output_dim = output_dim\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Scaling layer to map input to the interval [-1, 1]\n",
    "        #self.scale = tf.keras.layers.Lambda(\n",
    "        #    lambda x: 2.0 * (x - lb) / (ub - lb) - 1.0)\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation),\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        #self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "        \n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        #Z = self.scale(X)\n",
    "        Z = X\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we rely on the package [NetworkX](https://networkx.org/).\n",
    "An alternative approach would be to employ [igraph](https://igraph.org/), which seems to be more lightweight.\n",
    "\n",
    "Next, we set up the class for a graph PINN which takes either an adjacency matrix $A$ or a `MultiDiGraph` as input `inp` to define the network as well as the Dirichlet data as a list of pairs $(i, u_i)_{i=1}^{n_D}$ corresponding to $u(v_i) = u_i$.\n",
    "*Notes*:\n",
    "- we define our graph as a `MultiDiGraph` since graphs imported from [OpenStreetMap](https://www.openstreetmap.de/) obey this format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a network which is determined by its\n",
    "adjacency matrix $A \\in \\mathbb{R}_+^{n_v \\times n_v}$ with $n_v$ the number of vertices.\n",
    "Note that this matrix is not symmetric, as it belongs to a *directed* graph.\n",
    "Here, an entry $a_{i,j} > 0$ indicates that there is an edge starting in vertex $i$ and ending in vertex $j$ with length $a_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = 5\n",
    "# Specify adjacency matrix\n",
    "if adj == 1:\n",
    "    A = np.array([[0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ], dtype=np.int16)\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 1.\n",
    "    xmin = 0.\n",
    "    xmax = 0.1\n",
    "    \n",
    "    u0 = lambda x : 0\n",
    "    dirichletNodes = np.array([0, 7])\n",
    "    dirichletAlpha = np.zeros(8)\n",
    "    dirichletBeta = np.zeros(8)\n",
    "    dirichletAlpha[0] = .8\n",
    "    dirichletBeta[7] = .5\n",
    "                              \n",
    "    \n",
    "    eps = 0.01\n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "\n",
    "    def initial_cond(x):\n",
    "        return np.zeros_like(x)\n",
    "    \n",
    "elif adj == 2:\n",
    "    \n",
    "    A = np.array([[0, 1],\n",
    "                 [0, 0]], dtype=np.int16)\n",
    "    pos = np.array([[0,0],[1,0]])\n",
    "    \n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    eps = .01\n",
    "\n",
    "    dirichletNodes = np.array([0, 1])\n",
    "    dirichletAlpha = np.array([0.7, 0.0])\n",
    "    dirichletBeta = np.array([0.0, 0.8])\n",
    "    \n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return tf.zeros_like(x)\n",
    "    \n",
    "elif adj == 5:\n",
    "    A = np.array([[0, 0, 1, 0, 0, 0],\n",
    "                  [0, 0, 1, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 1, 1],\n",
    "                  [0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0]], dtype=np.int16)\n",
    "    \n",
    "    pos = np.array([[0.0, 0.0],\n",
    "                [0.0, 1.0],\n",
    "                [0.5, 0.5],\n",
    "                [0.5+np.sqrt(2)/2, 0.5],\n",
    "                [1.0+np.sqrt(2)/2, 0.0],\n",
    "                [1.0+np.sqrt(2)/2, 1.0]])\n",
    "    # Set boundaries\n",
    "    tmin = 0.\n",
    "    tmax = 10.\n",
    "    xmin = 0.\n",
    "    xmax = 1.\n",
    "    \n",
    "    dirichletNodes = np.array([0, 1, 4, 5])\n",
    "    # Changed this!\n",
    "    dirichletAlpha = np.array([0.9, 0.3, 0., 0., 0., 0.])\n",
    "    dirichletBeta = np.array([0.0, 0.0, 0., 0., 0.8, 0.1])\n",
    "                              \n",
    "    eps = 1e-2\n",
    "    def f(u):\n",
    "        return u * (1-u)\n",
    "    \n",
    "    def df(u):\n",
    "        return 1 - 2*u\n",
    "    \n",
    "    def pde(u, ut, ux, uxx):\n",
    "        return ut - eps * uxx + df(u) * ux\n",
    "    \n",
    "    def flux(u, ux):\n",
    "        return -eps * ux + f(u)\n",
    "    \n",
    "    def initial_cond(x):\n",
    "        return np.zeros_like(x)\n",
    "\n",
    "\n",
    "# Lower bounds\n",
    "lb = np.array([tmin, xmin], dtype=DTYPE)\n",
    "\n",
    "# Upper bounds\n",
    "ub = np.array([tmax, xmax], dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    \n",
    "    def __init__(self, inp, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub, pos=None):\n",
    "        \n",
    "        if isinstance(inp, netx.classes.multidigraph.MultiGraph):\n",
    "            self.G = inp\n",
    "            self.A = netx.adjacency_matrix(G).toarray()\n",
    "            \n",
    "        else:\n",
    "            # Store adjacency matrix\n",
    "            self.A = A\n",
    "            # Define networkx multigraph\n",
    "            self.G = netx.MultiDiGraph(A)\n",
    "        \n",
    "        self.dirichletNodes = dirichletNodes\n",
    "        self.dirichletAlpha = dirichletAlpha\n",
    "        self.dirichletBeta = dirichletBeta\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        # Get number of vertices\n",
    "        self.n_v = self.A.shape[0]\n",
    "        \n",
    "        # Determine lists of edges and lengths\n",
    "        self._determineEdgeList()\n",
    "        \n",
    "        # Determine list of vertices and incoming as well as outgoing edges\n",
    "        self._determineVertexList()\n",
    "        \n",
    "        # Determine graph layout if necessary\n",
    "        if pos is None:\n",
    "            self.pos = netx.kamada_kawai_layout(self.G)\n",
    "        else:\n",
    "            if isinstance(pos, np.ndarray):\n",
    "                self.pos = self.pos_array_to_dict(pos)\n",
    "            else:\n",
    "                raise ValueError('Check pos argument.')\n",
    "        \n",
    "    def _determineEdgeList(self):\n",
    "        \"\"\"Determine edge matrix and weight vector.\n",
    "        This could also be accomplished by a loop over `G.edges`:\n",
    "            for e in G.edges:\n",
    "                print(e)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.E = []\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(self.n_v):\n",
    "            for j in range(i + 1, self.n_v):\n",
    "                aij = self.A[i, j]\n",
    "                if aij > 0:\n",
    "                    #print('Connectivity between i: {} and j: {}'.format(i,j))\n",
    "                    self.E.append([i, j])\n",
    "                    self.W.append(aij)\n",
    "\n",
    "        # Get number of edges\n",
    "        self.ne = len(self.E)\n",
    "        \n",
    "    def _determineVertexList(self):\n",
    "        \n",
    "        self.Vin = [[] for _ in range(self.n_v)]\n",
    "        self.Vout = [[] for _ in range(self.n_v)]\n",
    "        \n",
    "        self.inflowNodes = []\n",
    "        self.outflowNodes = []        \n",
    "        \n",
    "        for i, e in enumerate(self.E):\n",
    "            # Unpack edge\n",
    "            vin, vout = e\n",
    "            self.Vin[vout].append(i)\n",
    "            self.Vout[vin].append(i)\n",
    "            \n",
    "        for i in range(self.n_v):\n",
    "            if self.Vin[i] and (not self.Vout[i]):\n",
    "                self.outflowNodes.append(i)\n",
    "\n",
    "            if (not self.Vin[i]) and self.Vout[i]:\n",
    "                self.inflowNodes.append(i)\n",
    "        self.outflowNodes = np.array(self.outflowNodes)\n",
    "        self.inflowNodes = np.array(self.inflowNodes)\n",
    "        self.innerVertices = np.setdiff1d(np.arange(self.n_v), self.dirichletNodes)\n",
    "            \n",
    "    def plotGraph(self, **kwargs):\n",
    "\n",
    "        netx.draw(self.G, pos=self.pos, with_labels=True, **kwargs)\n",
    "    \n",
    "    def pos_array_to_dict(self, pos):\n",
    "        pos_dict = dict()\n",
    "        for i in range(pos.shape[0]):\n",
    "            pos_dict[i] = pos[i,:]\n",
    "        return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vin: [[], [], [0, 1], [2], [3], [4]]\n",
      "Vout: [[0], [1], [2], [3, 4], [], []]\n",
      "inner vertices [2 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "graph = Graph(A, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub, pos)\n",
    "graph.plotGraph()\n",
    "print('Vin:', graph.Vin)\n",
    "print('Vout:', graph.Vout)\n",
    "print('inner vertices', graph.innerVertices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw collocation points uniformly or take them equidistantly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step size:  0.01\n"
     ]
    }
   ],
   "source": [
    "mode = 'deterministic'\n",
    "#mode = 'uniform'\n",
    "\n",
    "N_0 = 200\n",
    "N_b = 1000\n",
    "N_r = 100\n",
    "if timestepmode == 'implicit':\n",
    "    if mode == 'deterministic':\n",
    "        t_r = tf.linspace(lb[0], ub[0], N_b + 1)\n",
    "        #t_r = tf.cast(tf.linspace(lb[0], ub[0], N_b + 1), DTYPE)\n",
    "        #x_r = tf.cast(tf.linspace(lb[1], ub[1], N_0 + 1), DTYPE)\n",
    "        x_r = tf.linspace(lb[1], ub[1], N_0 + 1)\n",
    "        x_r = tf.reshape(x_r, (-1,1))\n",
    "    elif mode == 'uniform':\n",
    "        t_r = tf.linspace(lb[0], ub[0], N_b + 1)\n",
    "        x_r = tf.random.uniform((N_0, 1), lb[1], ub[1], dtype=DTYPE)    \n",
    "        x_r = tf.reshape(x_r, (-1,1))\n",
    "else: \n",
    "    if mode == 'deterministic':\n",
    "\n",
    "        # Uniform distributed collocation points\n",
    "        t_r = tf.linspace(lb[0], ub[0], N_b+1)\n",
    "        x_r = tf.linspace(lb[1], ub[1], N_0+1)\n",
    "        tt, xx = tf.meshgrid(t_r,x_r)\n",
    "        X_r = tf.concat([tf.reshape(tt,(-1,1)), tf.reshape(xx,(-1,1))], axis=1)\n",
    "\n",
    "    elif mode == 'uniform':\n",
    "\n",
    "        # Set random seed for reproducible results\n",
    "        tf.random.set_seed(0)\n",
    "\n",
    "        X_r = tf.random.uniform((N_r,2), lb, ub, dtype=DTYPE)\n",
    "\n",
    "        # Draw uniform sample points for initial boundary data\n",
    "        t_0 = tf.ones((N_0,1), dtype=DTYPE)*lb[0]\n",
    "        x_0 = tf.random.uniform((N_0,1), lb[1], ub[1], dtype=DTYPE)\n",
    "        X_0 = tf.concat([t_0, x_0], axis=1)\n",
    "\n",
    "        # Boundary data\n",
    "        t_b = tf.random.uniform((N_b,1), lb[0], ub[0], dtype=DTYPE)\n",
    "        x_l = tf.ones((N_b,1), dtype=DTYPE) * lb[1]\n",
    "        X_l = tf.concat([t_b, x_l], axis=1)\n",
    "\n",
    "        x_u = tf.ones((N_b,1), dtype=DTYPE) * ub[1]\n",
    "        X_u = tf.concat([t_b, x_u], axis=1)\n",
    "\n",
    "        X_b = tf.concat([X_l, X_u], axis=0)\n",
    "\n",
    "        # Draw uniformly sampled collocation points\n",
    "        t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
    "        x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
    "        X_r = tf.concat([t_r, x_r], axis=1)\n",
    "        X_data = tf.concat([X_0, X_b, X_r], axis=0)\n",
    "        \n",
    "    # Draw collocation points\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(X_r[:,0].numpy(), X_r[:,1].numpy(),color='red',alpha=0.3)\n",
    "    ax.scatter(X_0[:,0].numpy(), X_0[:,1].numpy(), color='blue', alpha=0.5)\n",
    "    ax.scatter(X_b[:,0].numpy(), X_b[:,1].numpy(), color='cyan', alpha=0.5)\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x')\n",
    "    \n",
    "print('Time step size: ', t_r[1].numpy() - t_r[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully implicit time stepping scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesteppingPINNSolver(object):\n",
    "    def __init__(self, graph, t_r, x_r):\n",
    "        \n",
    "        self.graph = graph\n",
    "        self.ne = self.graph.ne\n",
    "        \n",
    "        self._setupNNs()\n",
    "        \n",
    "        self.t = t_r\n",
    "        self.x = x_r\n",
    "        self.dt = t_r[1].numpy() - t_r[0].numpy()\n",
    "        \n",
    "        self.nx = x_r.shape[0]\n",
    "        self.nt = t_r.shape[0]\n",
    "\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist = []\n",
    "        self.iter = 0\n",
    "        self.current_loss = 999.\n",
    "        \n",
    "        self.idx = 1\n",
    "        \n",
    "        self.U = []\n",
    "        self.uold = []\n",
    "        for i in range(self.ne):\n",
    "            self.uold.append(tf.Variable(initial_cond(self.x)[:, 0]))\n",
    "            self.U.append(1e5*np.ones(shape=(self.nt ,self.nx)))\n",
    "            self.U[i][0, :] = self.uold[i].numpy()\n",
    "            \n",
    "        # Call each network once to initialize trainable variables\n",
    "        self.trainable_variables = []\n",
    "        for i in range(self.ne):\n",
    "            self.NNs[i](tf.constant([[1.]], dtype=DTYPE))\n",
    "            self.trainable_variables.append(self.NNs[i].trainable_variables)\n",
    "            \n",
    "        # Setup auxiliary variables for vertex values to ensure continuity\n",
    "        self._setupVertexVariables()\n",
    "        \n",
    "        for i, v in enumerate(self.graph.innerVertices):\n",
    "            self.trainable_variables.append([self.vertexVals[i]])\n",
    "        \n",
    "        self.nvar = len(self.trainable_variables)\n",
    "        \n",
    "    def _setupNNs(self):\n",
    "        \n",
    "        self.NNs = []\n",
    "        for i, e in enumerate(self.graph.E):\n",
    "            self.NNs.append(PINN(lb=self.graph.lb, ub=self.graph.ub))\n",
    "    \n",
    "        print('Initialized {:d} neural nets.'.format(len(self.NNs)))\n",
    "            \n",
    "\n",
    "    def _setupVertexVariables(self):\n",
    "        \n",
    "        self.vertexVals = []\n",
    "        for _ in self.graph.innerVertices:\n",
    "            # self.vertexVals.append(tf.Variable(tf.random.uniform(shape=(self.nb,)), trainable=True, dtype=DTYPE))\n",
    "            self.vertexVals.append(tf.Variable(tf.random.uniform(shape=(1,), dtype=DTYPE), trainable=True))\n",
    "    \n",
    "    def _fvals0(self, x):\n",
    "\n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        for i in range(self.ne):\n",
    "            u.append(self.NNs[i](x)[:,0])\n",
    "\n",
    "        return u\n",
    "    \n",
    "    def _fvals1(self, x):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variables representing t and x during this GradientTape\n",
    "                tape.watch(x)\n",
    "\n",
    "                # Compute current values u(t,x)\n",
    "                u.append(self.NNs[i](x)[:, 0])\n",
    "            ux.append(tape.gradient(u[i], x)[:, 0])\n",
    "            \n",
    "            del tape\n",
    "                \n",
    "        return u, ux\n",
    "    \n",
    "    def _fvals2(self, x):\n",
    "        \n",
    "        # Initialize lists for values and derivatives\n",
    "        u = []\n",
    "        ux = []\n",
    "        uxx = []\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            \n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variables representing t and x during this GradientTape\n",
    "                tape.watch(x)\n",
    "\n",
    "                # Compute current values u(t,x)\n",
    "                u.append(self.NNs[i](x)[:, 0])\n",
    "                ux.append(tape.gradient(u[i], x)[:, 0])\n",
    "                \n",
    "            uxx.append(tape.gradient(ux[i], x)[:, 0])\n",
    "            \n",
    "            del tape\n",
    "                \n",
    "        return u, ux, uxx\n",
    "\n",
    "    def determine_losses(self):\n",
    "        \n",
    "        # Short-hand notation of mean-squared loss\n",
    "        mse = lambda x : tf.reduce_mean(tf.square(x))\n",
    "        #mse = lambda x : tf.reduce_sum(tf.square(x))\n",
    "\n",
    "        \n",
    "        ###################################\n",
    "        ### Residual loss for all edges ###\n",
    "        ###################################\n",
    "        u, ux, uxx = self._fvals2(self.x)\n",
    "        \n",
    "        loss_res = 0\n",
    "        for i in range(self.ne):\n",
    "            #res_e = u[i] - self.U[i][self.idx-1, :] + self.dt * pde(u[i], 0., ux[i], uxx[i])\n",
    "            #print(u[i])\n",
    "            res_e = u[i] - self.uold[i] + self.dt * pde(u[i], 0., ux[i], uxx[i])\n",
    "\n",
    "            #print(self.U[i][self.idx-1, :])\n",
    "            #loss_res += mse(res_e[1:-1])\n",
    "            loss_res += mse(res_e)\n",
    "\n",
    "            #print(mse(res_e))\n",
    "            #print(self.U[i][self.idx-1, :])\n",
    "        \n",
    "        ###################################\n",
    "        ###   Continuity in vertices    ###\n",
    "        ###################################\n",
    "        \n",
    "        # ul, ult, ulx = self._fvals1(self.Xl[:,0], self.Xl[:,1])\n",
    "        # uu, uut, uux = self._fvals1(self.Xu[:,0], self.Xu[:,1])\n",
    "        loss_cont = 0\n",
    "        \n",
    "        for i, v in enumerate(self.graph.innerVertices):\n",
    "            \n",
    "            for j in self.graph.Vin[v]:\n",
    "                val = u[j][-1] - self.vertexVals[i]\n",
    "                loss_cont += mse(val)\n",
    "\n",
    "            for j in self.graph.Vout[v]:\n",
    "                val = u[j][0] - self.vertexVals[i]\n",
    "                loss_cont += mse(val)\n",
    "                \n",
    "        #####################################\n",
    "        ### Kirchhoff-Neumann in vertices ###\n",
    "        #####################################\n",
    "        \n",
    "        # Kirchhoff-Neumann condition in center nodes\n",
    "        loss_KN = 0\n",
    "        for i in self.graph.innerVertices:\n",
    "            \n",
    "            val = 0\n",
    "            #print('Kirchhoff-Neumann in node ', i)\n",
    "            for j in self.graph.Vin[i]:\n",
    "                #print('incoming edge:', j)\n",
    "                val += flux(u[j][-1], ux[j][-1])\n",
    "                #val += flux(uu[j], uux[j])\n",
    "                \n",
    "            for j in self.graph.Vout[i]:\n",
    "                #print('outgoing edge:', j)\n",
    "                val -= flux(u[j][0], ux[j][0])\n",
    "                #val -= flux(ul[j], ulx[j])\n",
    "            loss_KN += mse(val)\n",
    "        \n",
    "        #####################################\n",
    "        ###      Inflow/Outflow conds     ###\n",
    "        #####################################\n",
    "        \n",
    "        loss_D = 0\n",
    "        for i, v in enumerate(self.graph.dirichletNodes):\n",
    "            \n",
    "            #TODO\n",
    "            alpha = self.graph.dirichletAlpha[v]\n",
    "            beta = self.graph.dirichletBeta[v]\n",
    "            \n",
    "            print('\\nin node ', v, 'alpha ', alpha, 'beta ', beta)\n",
    "            val = 0\n",
    "            #print('\\n', val)\n",
    "            for j in self.graph.Vin[v]:\n",
    "                print('outflow: ', j)\n",
    "                #val += -flux(uu[j], uux[j]) - beta * (uu[j])\n",
    "                val += flux(u[j][-1], ux[j][-1]) - beta * (u[j][-1])\n",
    "                #loss_D += mse(val)\n",
    "            #print(val)\n",
    "                \n",
    "\n",
    "            for j in self.graph.Vout[v]:\n",
    "                print('inflow: ', j)\n",
    "                #val += -flux(ul[j], ulx[j]) + alpha * (1-ul[j])\n",
    "                val += -flux(u[j][0], ux[j][0]) + alpha * (1.-u[j][0])\n",
    "                #val += -flux(ul[j], ulx[j]) + alpha * (1-ul[j])\n",
    "                #loss_D += mse(val)\n",
    "            #print(val, '\\n')\n",
    "            loss_D += mse(val)\n",
    "            \n",
    "        return loss_res, loss_cont, loss_KN, loss_D\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        \n",
    "        loss_res, loss_cont, loss_KN, loss_D = self.determine_losses()\n",
    "        \n",
    "        loss = loss_res + loss_cont + loss_KN + loss_D\n",
    "        #print(loss_res)\n",
    "        #print(loss_cont)\n",
    "        #print(loss_KN)\n",
    "        #print(loss_D)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with\n",
    "            # respect to trainable variables\n",
    "            tape.watch(self.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "\n",
    "        g = tape.gradient(loss, self.trainable_variables)\n",
    "        del tape\n",
    "\n",
    "        return loss, g\n",
    "    \n",
    "    \n",
    "            \n",
    "    def solve_with_TFoptimizer(self, optimizer, eps, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        self.callback_init()\n",
    "        \n",
    "        for i in range(N):\n",
    "            loss, g = self.get_grad()\n",
    "            # Perform gradient descent step\n",
    "            for j in range(self.nvar):\n",
    "                optimizer.apply_gradients(zip(g[j], self.trainable_variables[j]))\n",
    "            #print(self.current_loss)\n",
    "            self.current_loss = loss.numpy()\n",
    "            #print(self.current_loss)\n",
    "            #print(loss)\n",
    "            #print('\\n')\n",
    "\n",
    "            self.callback()\n",
    "            \n",
    "            if self.current_loss < eps:\n",
    "                break\n",
    "                \n",
    "    def solve_with_ScipyOptimizer(self, method='L-BFGS-B', **kwargs):\n",
    "        \"\"\"This method provides an interface to solve the learning problem\n",
    "        using a routine from scipy.optimize.minimize.\n",
    "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
    "        supported in Tensorflow 2.xx.)\n",
    "        Type conversion is necessary since scipy-routines are written in\n",
    "        Fortran which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
    "\n",
    "        def get_weight_tensor():\n",
    "            \"\"\"Function to return current variables of the model\n",
    "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
    "\n",
    "            weight_list = []\n",
    "            shape_list = []\n",
    "\n",
    "            # Loop over all variables, i.e. weight matrices, bias vectors\n",
    "            # and unknown parameters\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for v in self.trainable_variables[i]:\n",
    "                    shape_list.append(v.shape)\n",
    "                    weight_list.extend(v.numpy().flatten())\n",
    "\n",
    "            weight_list = tf.convert_to_tensor(weight_list)\n",
    "            return weight_list, shape_list\n",
    "\n",
    "\n",
    "        x0, shape_list = get_weight_tensor()\n",
    "\n",
    "        def set_weight_tensor(weight_list):\n",
    "            \"\"\"Function which sets list of weights\n",
    "            to variables in the model.\"\"\"\n",
    "            idx = 0\n",
    "\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for v in self.trainable_variables[i]:\n",
    "                    vs = v.shape\n",
    "\n",
    "                    # Weight matrices\n",
    "                    if len(vs) == 2:\n",
    "                        sw = vs[0] * vs[1]\n",
    "                        new_val = tf.reshape(\n",
    "                            weight_list[idx:idx + sw], (vs[0], vs[1]))\n",
    "                        idx += sw\n",
    "\n",
    "                    # Bias vectors\n",
    "                    elif len(vs) == 1:\n",
    "                        new_val = weight_list[idx:idx+vs[0]]\n",
    "                        idx += vs[0]\n",
    "\n",
    "                    # Variables (in case of parameter identification setting)\n",
    "                    elif len(vs) == 0:\n",
    "                        new_val = weight_list[idx]\n",
    "                        idx += 1\n",
    "\n",
    "                    # Assign variables (Casting necessary since scipy requires float64 type)\n",
    "                    v.assign(tf.cast(new_val, DTYPE))\n",
    "\n",
    "        def get_loss_and_grad(w):\n",
    "            \"\"\"Function that provides current loss and gradient\n",
    "            w.r.t the trainable variables as vector. This is mandatory\n",
    "            for the LBFGS minimizer from tfp.optimizer.\"\"\"\n",
    "\n",
    "            # Update weights in model\n",
    "            set_weight_tensor(w)\n",
    "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
    "            loss, grad = self.get_grad()\n",
    "            # Flatten gradient\n",
    "            grad_flat = []\n",
    "            for i in range(len(self.trainable_variables)):\n",
    "                for g in grad[i]:\n",
    "                    grad_flat.extend(g.numpy().flatten())\n",
    "\n",
    "            # Store current loss for callback function\n",
    "            #print(self.current_loss)\n",
    "            self.current_loss = loss\n",
    "            #print(self.current_loss)\n",
    "\n",
    "            # Return value and gradient of \\phi as tuple\n",
    "            return loss.numpy().astype(np.float64), np.array(grad_flat, dtype=np.float64)\n",
    "\n",
    "        self.callback_init()\n",
    "\n",
    "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
    "                                       x0=x0,\n",
    "                                       jac=True,\n",
    "                                       method=method,\n",
    "                                       callback=self.callback,\n",
    "                                       **kwargs)\n",
    "    \n",
    "                \n",
    "    def ts_scheme(self, eps=1e-6):\n",
    "        max_trials = 1\n",
    "        while self.idx < 20: #self.nt:\n",
    "            print('Solve time step {}/{}\\n'.format(self.idx, self.nt))\n",
    "            \n",
    "            trial = 0\n",
    "            while trial < max_trials:\n",
    "                \n",
    "                print('Adam...\\n')\n",
    "                \n",
    "                if self.idx == 1:\n",
    "                    lr = 0.01\n",
    "                    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                    self.solve_with_TFoptimizer(optim, eps=eps, N=4001)\n",
    "                    \n",
    "                else:\n",
    "                    lr = 0.001\n",
    "                    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                    self.solve_with_TFoptimizer(optim, eps=eps, N=501)\n",
    "                self.callback(force=True)\n",
    "                print('LBFGS...\\n')\n",
    "                ret = self.solve_with_ScipyOptimizer(options={'maxiter': 50000,\n",
    "                                     'maxfun': 50000,\n",
    "                                     'maxcor': 50,\n",
    "                                     'maxls': 50,\n",
    "                                     'eps': eps,\n",
    "                                     'ftol': 1.0e3*np.finfo(float).eps,\n",
    "                                     'gtol': 1.0e3*np.finfo(float).eps})\n",
    "                # factr is 10000000 \n",
    "                print(ret.message)\n",
    "                trial += 1\n",
    "                self.callback(force=True)\n",
    "            \n",
    "            u = self._fvals0(self.x)\n",
    "            self.assign_u(self.idx, u)\n",
    "            self.idx += 1\n",
    "            self.iter = 0\n",
    "\n",
    "    def assign_u(self, timestep, u):\n",
    "        \n",
    "        for i in range(self.ne):\n",
    "            #print('assign new u to timestep {}'.format(timestep))\n",
    "            #print(u[i].shape)\n",
    "            self.uold[i].assign(u[i])\n",
    "            self.U[i][timestep, :] = u[i]\n",
    "            \n",
    "    def next_step(self):\n",
    "        u = self._fvals0(self.x)\n",
    "        self.assign_u(self.idx, u)\n",
    "        self.idx += 1\n",
    "\n",
    "        \n",
    "    def callback_init(self):\n",
    "        self.t0 = time()\n",
    "        print(' Iter            Loss    Time')\n",
    "        print('-----------------------------')\n",
    "    \n",
    "    def callback(self, xr=None, force=False):\n",
    "        if self.iter % 100 == 0 or force:\n",
    "            print('{:05d}  {:10.8e}   {:4.2f}'.format(\n",
    "                self.iter, self.current_loss, time() - self.t0))\n",
    "        self.hist.append(self.current_loss)\n",
    "        self.iter += 1\n",
    "        \n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "        ax.semilogy(range(len(self.hist)), self.hist, 'k-')\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 5 neural nets.\n",
      "\n",
      "in node  0 alpha  0.9 beta  0.0\n",
      "inflow:  0\n",
      "\n",
      "in node  1 alpha  0.3 beta  0.0\n",
      "inflow:  1\n",
      "\n",
      "in node  4 alpha  0.0 beta  0.8\n",
      "outflow:  3\n",
      "\n",
      "in node  5 alpha  0.0 beta  0.1\n",
      "outflow:  4\n",
      "Solve time step 1/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "\n",
      "in node  0 alpha  0.9 beta  0.0\n",
      "inflow:  0\n",
      "\n",
      "in node  1 alpha  0.3 beta  0.0\n",
      "inflow:  1\n",
      "\n",
      "in node  4 alpha  0.0 beta  0.8\n",
      "outflow:  3\n",
      "\n",
      "in node  5 alpha  0.0 beta  0.1\n",
      "outflow:  4\n",
      "00000  2.09564450e+00   1.59\n",
      "00100  3.29135123e-03   3.09\n",
      "00200  1.71436874e-03   4.64\n",
      "00300  7.83623621e-04   6.20\n",
      "00400  5.00138516e-04   7.70\n",
      "00500  4.06101810e-03   9.33\n",
      "00600  1.11714002e-03   11.17\n",
      "00700  7.60346457e-04   12.98\n",
      "00800  2.91758307e-03   14.79\n",
      "00900  2.23761842e-04   16.79\n",
      "01000  3.66989545e-04   18.76\n",
      "01100  2.59708115e-04   20.72\n",
      "01200  5.91762070e-04   22.57\n",
      "01300  1.82151841e-04   24.53\n",
      "01400  6.56669579e-04   26.33\n",
      "01500  1.24851648e-04   28.12\n",
      "01600  1.92728475e-04   30.01\n",
      "01700  2.08515405e-04   31.76\n",
      "01800  8.24670204e-04   33.75\n",
      "01900  7.66185739e-04   35.51\n",
      "02000  6.96491782e-04   37.25\n",
      "02100  2.14022774e-04   39.00\n",
      "02200  5.40867687e-04   40.78\n",
      "02300  6.33789383e-04   42.54\n",
      "02400  1.53600704e-04   44.30\n",
      "02500  8.09235659e-05   46.06\n",
      "02600  1.58489564e-04   47.86\n",
      "02700  1.30713527e-04   50.04\n",
      "02800  7.39563560e-05   51.77\n",
      "02900  5.46801143e-05   53.50\n",
      "03000  9.08379973e-05   55.40\n",
      "03100  3.55037582e-05   57.26\n",
      "03200  7.67561446e-04   59.05\n",
      "03300  4.03608147e-05   60.83\n",
      "03400  4.71704122e-05   62.56\n",
      "03500  2.74467426e-04   64.31\n",
      "03600  1.15287642e-04   66.26\n",
      "03700  3.93618531e-05   68.01\n",
      "03800  1.36428623e-03   69.78\n",
      "03900  4.18177195e-05   71.52\n",
      "04000  1.37742190e-04   73.27\n",
      "04001  1.37742190e-04   73.27\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "04100  9.87182338e-06   1.73\n",
      "04200  8.96032699e-07   3.70\n",
      "04300  2.15809192e-07   5.67\n",
      "04400  1.42354983e-07   7.65\n",
      "04500  1.00811680e-07   9.70\n",
      "04600  6.43771860e-08   11.66\n",
      "04700  3.94964686e-08   13.58\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "04741  3.84667406e-08   14.31\n",
      "Solve time step 2/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  1.71381213e-03   0.06\n",
      "00100  9.43000431e-06   1.92\n",
      "00200  6.13774232e-06   3.73\n",
      "00300  4.87774464e-06   5.53\n",
      "00400  4.04777136e-06   7.27\n",
      "00500  3.46185809e-06   9.11\n",
      "00501  3.46185809e-06   9.11\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00600  9.76880941e-07   2.02\n",
      "00700  5.21961160e-07   4.42\n",
      "00800  4.69837306e-07   7.17\n",
      "00900  3.97251738e-07   9.23\n",
      "01000  3.48431372e-07   11.15\n",
      "01100  3.25106119e-07   13.35\n",
      "01200  1.44049694e-07   15.67\n",
      "01300  1.38986483e-07   18.05\n",
      "01400  5.84112887e-08   20.29\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "01436  4.96683029e-08   20.91\n",
      "Solve time step 3/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  4.96562117e-04   0.06\n",
      "00100  1.68911462e-06   1.96\n",
      "00125  9.91480312e-07   2.41\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00200  5.63466600e-08   1.30\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00254  1.69928535e-08   2.24\n",
      "Solve time step 4/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  2.97361386e-04   0.07\n",
      "00064  9.35017098e-07   1.35\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  2.19642217e-07   0.63\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00134  1.56684434e-08   1.23\n",
      "Solve time step 5/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  2.13203488e-04   0.06\n",
      "00055  6.62964274e-07   1.05\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  3.20707760e-08   0.70\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00121  1.58492914e-08   1.05\n",
      "Solve time step 6/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  1.66764107e-04   0.06\n",
      "00048  7.28392152e-07   0.96\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  2.78626762e-08   0.91\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00133  1.37387658e-08   1.55\n",
      "Solve time step 7/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  1.37317908e-04   0.06\n",
      "00050  9.81329443e-07   1.01\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.55711027e-08   0.84\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00112  1.38950341e-08   1.08\n",
      "Solve time step 8/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  1.16965710e-04   0.06\n",
      "00041  8.93828959e-07   0.84\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.53967513e-08   0.99\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00102  1.53966132e-08   1.01\n",
      "Solve time step 9/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  1.02063003e-04   0.06\n",
      "00038  8.97005158e-07   0.78\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00092  1.64890073e-08   0.86\n",
      "Solve time step 10/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  9.06790380e-05   0.06\n",
      "00043  7.11934141e-07   0.92\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.85882604e-08   0.97\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00127  1.30753366e-08   1.43\n",
      "Solve time step 11/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  8.16999548e-05   0.06\n",
      "00040  8.05100705e-07   0.86\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.60151301e-08   1.02\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00105  1.59285685e-08   1.10\n",
      "Solve time step 12/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  7.44178487e-05   0.06\n",
      "00037  9.81842668e-07   0.75\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.74950606e-08   1.07\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00158  8.52677258e-09   2.06\n",
      "Solve time step 13/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  6.84009895e-05   0.06\n",
      "00040  5.58158595e-07   0.80\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.26912873e-08   0.94\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00104  1.26238127e-08   0.99\n",
      "Solve time step 14/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  6.33383968e-05   0.06\n",
      "00043  5.24745556e-07   0.99\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00100  1.37823670e-08   0.98\n",
      "Solve time step 15/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  5.90219836e-05   0.07\n",
      "00037  6.43768263e-07   0.75\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00090  1.53429967e-08   0.82\n",
      "Solve time step 16/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  5.52961266e-05   0.06\n",
      "00040  6.26015123e-07   0.79\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.57805433e-08   0.91\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00117  1.26025223e-08   1.21\n",
      "Solve time step 17/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  5.20428576e-05   0.06\n",
      "00034  9.15868258e-07   0.68\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.18298253e-08   1.07\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00101  1.18298253e-08   1.07\n",
      "Solve time step 18/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  4.91728408e-05   0.06\n",
      "00034  9.00200556e-07   0.67\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00100  1.36647848e-08   1.03\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00128  9.99807692e-09   1.48\n",
      "Solve time step 19/1001\n",
      "\n",
      "Adam...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n",
      "00000  4.66279824e-05   0.06\n",
      "00028  9.43774597e-07   0.59\n",
      "LBFGS...\n",
      "\n",
      " Iter            Loss    Time\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "00075  1.29207538e-08   0.68\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.random.set_seed(0)\n",
    "#gPINN = GraphPINN(A, dirichletNodes, dirichletAlpha, dirichletBeta, lb, ub)\n",
    "solver = TimesteppingPINNSolver(graph, t_r, x_r)\n",
    "solver.determine_losses()\n",
    "#1.0*np.finfo(float).eps\n",
    "#u0 = solver._fvals0(x_r)\n",
    "#u1, u1x = solver._fvals1(x_r)\n",
    "#u2, u2x, u2xx = solver._fvals2(x_r)\n",
    "#print(u1)\n",
    "#print(u1x)\n",
    "#print([u2[i]-u0[i] for i in range(gPINN.ne)])\n",
    "#print([u2x[i]-u1x[i] for i in range(gPINN.ne)])\n",
    "#print(u2xx)\n",
    "#solver.determine_losses()\n",
    "\n",
    "#lr = 0.01\n",
    "#optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#print('Start with TF optimizer\\n')\n",
    "#solver.solve_with_TFoptimizer(optim, N=5001)\n",
    "solver.ts_scheme(1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in node  0 alpha  0.9 beta  0.0\n",
      "inflow:  0\n",
      "\n",
      "in node  1 alpha  0.3 beta  0.0\n",
      "inflow:  1\n",
      "\n",
      "in node  4 alpha  0.0 beta  0.8\n",
      "outflow:  3\n",
      "\n",
      "in node  5 alpha  0.0 beta  0.1\n",
      "outflow:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=4.4357873503401354e-05>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wall time: 7min 53s\n",
    "# Wall time: 1min 6s\n",
    "\n",
    "\n",
    "#solver.determine_losses()\n",
    "solver.loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c77203d7bb4339a926501d683c10fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='j', max=1000), Output()), _dom_classes=('widget-interact"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#Nt = 60\n",
    "#Nx = 120\n",
    "\n",
    "#tspace = tf.linspace(lb[0], ub[0], Nt + 1)\n",
    "#xspace = tf.reshape(tf.linspace(lb[1], ub[1], Nx + 1), [-1, 1])\n",
    "\n",
    "pos = solver.graph.pos\n",
    "xy_list = [pos[e[0]] + x_r*(pos[e[1]] - pos[e[0]]) / (ub[1]-lb[1]) for e in solver.graph.E]\n",
    "\n",
    "def plot_network(j=0, fig=None):\n",
    "    if not fig:\n",
    "        fig = plt.figure(1, clear=True)\n",
    "    else:\n",
    "        fig.clf()\n",
    "        \n",
    "    ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "    \n",
    "    for i, e in enumerate(solver.graph.E):\n",
    "        u = solver.U[i][j, :]\n",
    "        ax.plot(xy_list[i][:,0], xy_list[i][:,1], u)\n",
    "    \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlim([.0,1.0])\n",
    "    ax.view_init(12, 135)\n",
    "\n",
    "#fig.canvas.layout.width = '100%'\n",
    "#fig.canvas.layout.height = '900px'\n",
    "j_slider = widgets.IntSlider(min=0,max=N_b,step=1)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "def interactive_net(j=0):\n",
    "    plot_network(j, fig)\n",
    "    \n",
    "interactive_plot = interactive(interactive_net, j=j_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "#output.layout.height = '350px'\n",
    "interactive_plot\n",
    "#u = plot_network(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3b7cbe305dc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Set up formatting for the movie files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Jan Blechschmidt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1800\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_registered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Requested MovieWriter ({name}) not available\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "#def update_line(num, data, line):\n",
    "#    line.set_data(data[..., :num])\n",
    "#    return line,\n",
    "\n",
    "# Set up formatting for the movie files\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Jan Blechschmidt'), bitrate=1800)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "line_ani = animation.FuncAnimation(fig, interactive_net, 200, interval=50)\n",
    "line_ani.save('sol_pinn.mp4', writer=writer)\n",
    "\n",
    "#fig2 = plt.figure()\n",
    "\n",
    "#x = np.arange(-9, 10)\n",
    "#y = np.arange(-9, 10).reshape(-1, 1)\n",
    "#base = np.hypot(x, y)\n",
    "#ims = []\n",
    "#for add in np.arange(15):\n",
    "#    ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "#im_ani = animation.ArtistAnimation(fig2, ims, interval=50, repeat_delay=3000,\n",
    "#                                   blit=True)\n",
    "#im_ani.save('im.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
