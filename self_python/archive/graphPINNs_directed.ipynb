{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs on directed graphs\n",
    "\n",
    "In this notebook, we focus on the solution of transport equations on directed graphs.\n",
    "To be more precise, we consider the transport equation\n",
    "\n",
    "$$\n",
    "\\partial_t u + \\partial_x (u \\, (1-u)) = \\partial_t u + \\partial_x u - 2 \\, u \\, \\partial_x u = 0\n",
    "$$\n",
    "\n",
    "with appropriate boundary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/blja/miniconda3/envs/tensorflow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up class GraphPINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the class `GraphPINN`, which is employed in the subsequent code.\n",
    "The idea is to have one vanilla `PINN` for each edge which are connected via boundary and vertex conditions which are enforced weakly.\n",
    "\n",
    "---\n",
    "\n",
    "First, we define the model for a `PINN` which consists of one scaling layer, a number of fully connected hidden layers and one final output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class PINN(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self, lb, ub,\n",
    "                 output_dim=1,\n",
    "                 num_hidden_layers=8,\n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.input_dim = lb.shape[0]\n",
    "        self.output_dim = output_dim\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Define NN architecture\n",
    "        \n",
    "        # Scaling layer to map inpot to the interval [-1, 1]\n",
    "        self.scale = tf.keras.layers.Lambda(\n",
    "            lambda x: 2.0 * (x - lb) / (ub - lb) - 1.0)\n",
    "        \n",
    "        # Inititialize num_hidden_layers many fully connected dense layers\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation),\n",
    "                                             kernel_initializer=kernel_initializer) for _ in range(self.num_hidden_layers)]\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        \n",
    "        Z = self.scale(X)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "            \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we rely on the package [NetworkX](https://networkx.org/).\n",
    "An alternative approach would be to employ [igraph](https://igraph.org/), which seems to be more lightweight.\n",
    "\n",
    "Next, we set up the class for a graph PINN which takes either an adjacency matrix $A$ or a `MultiDiGraph` as input `inp` to define the network as well as the Dirichlet data as a list of pairs $(i, u_i)_{i=1}^{n_D}$ corresponding to $u(v_i) = u_i$.\n",
    "*Notes*:\n",
    "- we define our graph as a `MultiDiGraph` since graphs imported from [OpenStreetMap](https://www.openstreetmap.de/) obey this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class GraphPINN():\n",
    "    \n",
    "    def __init__(self, inp, dirichletData, lb, ub):\n",
    "        \n",
    "        if isinstance(inp, nx.classes.multidigraph.MultiGraph):\n",
    "            self.G = inp\n",
    "            self.A = nx.adjacency_matrix(G).toarray()\n",
    "            \n",
    "        elif tf.is_tensor(A) and len(A.shape)==2:\n",
    "            # Store adjacency matrix\n",
    "            self.A = A\n",
    "            # Define networkx multigraph\n",
    "            self.G = nx.MultiDiGraph(A.numpy())\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Check input type of GraphPINN.')\n",
    "        \n",
    "        self.dirichletData = dirichletData\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        # Get number of vertices\n",
    "        self.n_v = self.A.shape[0]\n",
    "        \n",
    "        # Determine lists of edges and lengths\n",
    "        self._determineEdgeList()\n",
    "        \n",
    "        # Determine list of vertices and incoming as well as outgoing edges\n",
    "        self._determineVertexList()\n",
    "        \n",
    "        # Setup list of neural networks\n",
    "        self._setupNNs()\n",
    "        \n",
    "        # Setup list of vertex values in inner nodes\n",
    "        self._setupVertexVariables()\n",
    "        \n",
    "    def _determineEdgeList(self):\n",
    "        \"\"\"Determine edge matrix and weight vector.\n",
    "        This could also be accomplished by a loop over `G.edges`:\n",
    "            for e in G.edges:\n",
    "                print(e)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.E = []\n",
    "        self.W = []\n",
    "        \n",
    "        for i in range(self.n_v):\n",
    "            for j in range(i + 1, self.n_v):\n",
    "                aij = self.A[i, j]\n",
    "                if aij > 0:\n",
    "                    #print('Connectivity between i: {} and j: {}'.format(i,j))\n",
    "                    self.E.append([i, j])\n",
    "                    self.W.append(aij.numpy())\n",
    "\n",
    "        # Get number of edges\n",
    "        self.n_e = len(self.E)\n",
    "        \n",
    "    def _determineVertexList(self):\n",
    "        self.Vin = []\n",
    "        self.Vout = []\n",
    "        self.innerVertices = []\n",
    "\n",
    "        for i in range(self.n_v):\n",
    "            # Get incoming edges of vertex i\n",
    "            tmp = np.where(self.A[:,i]>0)[0]\n",
    "            self.Vin.append(tmp.tolist())\n",
    "            \n",
    "            # Get outgoing edeges of vertex i\n",
    "            tmp = np.where(self.A[i,:]>0)[0]\n",
    "            self.Vout.append(tmp.tolist())\n",
    "            \n",
    "            if self.Vin[i] and self.Vout[i]:\n",
    "                self.innerVertices.append(i)\n",
    "            \n",
    "    def _setupNNs(self):\n",
    "        \n",
    "        self.NNs = []\n",
    "        for i, e in enumerate(self.E):\n",
    "            self.NNs.append(PINN(lb=self.lb, ub=self.ub))\n",
    "    \n",
    "        print('Initialized {:d} neural nets.'.format(len(self.NNs)))\n",
    "    \n",
    "    def _setupVertexVariables(self):\n",
    "        \n",
    "        self.vertexVals = []\n",
    "        for i in self.innerVertices:\n",
    "            self.vertexVals.append(tf.Variable(0.0, trainable=True, dtype=DTYPE))\n",
    "        \n",
    "    def plotGraph(self, **kwargs):\n",
    "        \n",
    "        # Determine graph layout if necessary\n",
    "        if not hasattr(self, 'pos'):\n",
    "            self.pos = nx.kamada_kawai_layout(self.G)\n",
    "            \n",
    "        nx.draw(self.G, pos=self.pos, with_labels=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a network which is determined by its\n",
    "adjacency matrix $A \\in \\mathbb{R}_+^{n_v \\times n_v}$ with $n_v$ the number of vertices.\n",
    "Note that this matrix is not symmetric, as it belongs to a *directed* graph.\n",
    "Here, an entry $a_{i,j} > 0$ indicates that there is an edge starting in vertex $i$ and ending in vertex $j$ with length $a_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = 'float32'\n",
    "\n",
    "# Specify adjacency matrix\n",
    "A = tf.constant([[0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ], dtype=tf.int16)\n",
    "\n",
    "dirichletData = [(0, .0)]\n",
    "\n",
    "# Set boundaries\n",
    "tmin = 0.\n",
    "tmax = 20.\n",
    "xmin = 0.\n",
    "xmax = 1.\n",
    "\n",
    "# Lower bounds\n",
    "lb = tf.constant([tmin, xmin], dtype=DTYPE)\n",
    "\n",
    "# Upper bounds\n",
    "ub = tf.constant([tmax, xmax], dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 9 neural nets.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "gPINN = GraphPINN(A, dirichletData, lb, ub)\n",
    "gPINN.plotGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up class for time-dependent equations\n",
    "\n",
    "We have to handle different kinds of boundary and vertex conditions:\n",
    "- all inner vertices: Kirchhoff-Neumann conditon\n",
    "- initial time conditions on all edges\n",
    "- Dirichlet conditions on selected vertices as long as it is an inflowing node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphPINNSolver(object):\n",
    "    def __init__(self, graphPINN, X_r):\n",
    "        \n",
    "        # Store collocation points\n",
    "        self.N_r = X_r.shape[0]\n",
    "        \n",
    "        self.graphPINN = graphPINN\n",
    "        \n",
    "        n_e = self.graphPINN.n_e\n",
    "        \n",
    "        self.t = [ X_r[:,0:1]\n",
    "                      for i in range(n_e)]\n",
    "        self.x = [ X_r[:,1:2]\n",
    "                      for i in range(n_e)]\n",
    "        \n",
    "        self.t0nodes = np.where(X_r[:,0] < lb[0]+1e-13)[0]\n",
    "        self.x0nodes = np.where(X_r[:,1] < lb[1]+1e-13)[0]\n",
    "        self.xLnodes = np.where(X_r[:,1] > ub[1]-1e-13)[0]\n",
    "\n",
    "        # Initialize history of losses and global iteration counter\n",
    "        self.hist = []\n",
    "        self.iter = 0\n",
    "        \n",
    "        # Call each network once to initialize trainable variables\n",
    "        self.trainable_variables = []\n",
    "        for i in range(n_e):\n",
    "            self.graphPINN.NNs[i](tf.constant([[1., 1.]]))\n",
    "            self.trainable_variables.append(self.graphPINN.NNs[i].trainable_variables)\n",
    "\n",
    "        #self.trainable_variables = [self.graphPINN.NNs[i].trainable_variables\n",
    "        #                            for i in range(n_e)]\n",
    "        for i, v in enumerate(self.graphPINN.innerVertices):\n",
    "            self.trainable_variables.append(self.graphPINN.vertexVals[i])\n",
    "\n",
    "    def get_fvals1(self, t=None, x=None):\n",
    "\n",
    "        # Initilize lists for values and derivatives\n",
    "        u = []\n",
    "        u1x = []\n",
    "        u1t = []\n",
    "        \n",
    "        if t is None:\n",
    "            t = self.t\n",
    "        if x is None:\n",
    "            x = self.x\n",
    "            \n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variables representing t and x during this GradientTape\n",
    "            tape.watch(t)\n",
    "            tape.watch(x)\n",
    "            \n",
    "            # Compute current values u(t,x)\n",
    "            u = []\n",
    "            for i in range(self.graphPINN.n_e):\n",
    "                u.append(self.graphPINN.NNs[i](tf.stack([t[i][:,0], x[i][:,0]], axis=1)))\n",
    "                \n",
    "        for i in range(self.graphPINN.n_e):\n",
    "            u1t.append(tape.gradient(u[i], t[i]))\n",
    "            u1x.append(tape.gradient(u[i], x[i]))\n",
    "\n",
    "        del tape\n",
    "\n",
    "        return u, u1t, u1x\n",
    "    \n",
    "    def get_fvals2(self, t=None, x=None):\n",
    "\n",
    "        # Initilize lists for values and derivatives\n",
    "        u = []\n",
    "        u1x = []\n",
    "        u2x = []\n",
    "        u1t = []\n",
    "        \n",
    "        if t is None:\n",
    "            t = self.t\n",
    "        if x is None:\n",
    "            x = self.x\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variables representing t and x during this GradientTape\n",
    "            tape.watch(t)\n",
    "            tape.watch(x)\n",
    "            \n",
    "            # Compute current values u(t,x)\n",
    "            u = []\n",
    "            for i in range(self.graphPINN.n_e):\n",
    "                u.append(self.graphPINN.NNs[i](tf.stack([t[i][:,0], x[i][:,0]], axis=1)))\n",
    "                u1x.append(tape.gradient(u[i], x[i]))\n",
    "                \n",
    "        for i in range(self.graphPINN.n_e):\n",
    "            u1t.append(tape.gradient(u[i], t[i]))\n",
    "            u2x.append(tape.gradient(u1x[i], x[i]))\n",
    "        \n",
    "        del tape\n",
    "\n",
    "        return u, u1t, u1x, u2x\n",
    "\n",
    "    def fun_r2(self, u, u1t, u1x, u2x):\n",
    "        ret = []\n",
    "        for i in range(self.graphPINN.n_e):\n",
    "            #ret.append(u1t[i] + u1x[i] - 0.1*u2x[i]-1.)\n",
    "            ret.append(u1t[i] + u1x[i] - 2*u[i]*u1x[i])\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def fun_r1(self, u, u1t, u1x):\n",
    "        ret = []\n",
    "        for i in range(self.graphPINN.n_e):\n",
    "            ret.append(u1t[i] + u1x[i] - 2*u[i]*u1x[i])\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_fn(self):\n",
    "        \n",
    "        # Get values of functions and derivatives\n",
    "        #u, u1t, u1x, u2x = self.get_fvals2()\n",
    "        \n",
    "        # Compute phi_r\n",
    "        #r = self.fun_r2(u, u1t, u1x, u2x)\n",
    "        \n",
    "        # Get values of functions and derivatives\n",
    "        u, u1t, u1x = self.get_fvals1()\n",
    "        \n",
    "        # Compute phi_r\n",
    "        r = self.fun_r1(u, u1t, u1x)\n",
    "        phi_r = tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = phi_r\n",
    "        \n",
    "        x0nodes = slice(0,N_t+1)\n",
    "        xLnodes = slice(-N_t-1,None)\n",
    "        t0nodes = slice(0,None,N_t+1)\n",
    "        \n",
    "        # Initial time condition\n",
    "        loss += tf.reduce_mean(tf.square(u[0][t0nodes] - 0.0))\n",
    "        for i in range(1,self.graphPINN.n_e):\n",
    "            loss += tf.reduce_mean(tf.square(u[i][t0nodes]-0.0))\n",
    "        #    loss += tf.reduce_mean(tf.square(u[i][t0nodes] - tf.sin(X_r[t0nodes,1]*np.pi)))\n",
    "        \n",
    "        # Continuity in center node\n",
    "        for i,v in enumerate(self.graphPINN.innerVertices):\n",
    "            for j in self.graphPINN.Vin[v]:\n",
    "                loss += tf.reduce_mean(tf.square(u[j][xLnodes] - self.graphPINN.vertexVals[i]))\n",
    "            for j in self.graphPINN.Vout[v]:\n",
    "                loss += tf.reduce_mean(tf.square(u[j][x0nodes] - self.graphPINN.vertexVals[i]))\n",
    "\n",
    "        \n",
    "        if False:\n",
    "            # Vertex 1\n",
    "            loss += tf.reduce_mean(tf.square(u[0][xLnodes] - u[1][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[0][xLnodes] - u[2][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[1][x0nodes] - u[2][x0nodes]))        \n",
    "            # Vertex 2\n",
    "            loss += tf.reduce_mean(tf.square(u[1][xLnodes] - u[3][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[1][xLnodes] - u[4][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[3][x0nodes] - u[4][x0nodes]))\n",
    "            # Vertex 3\n",
    "            loss += tf.reduce_mean(tf.square(u[3][xLnodes] - u[5][x0nodes]))\n",
    "            # Vertex 4\n",
    "            loss += tf.reduce_mean(tf.square(u[2][xLnodes] - u[4][xLnodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[2][xLnodes] - u[6][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[4][xLnodes] - u[6][x0nodes]))\n",
    "            # Vertex 5\n",
    "            loss += tf.reduce_mean(tf.square(u[6][xLnodes] - u[7][x0nodes]))\n",
    "            # Vertex 6\n",
    "            loss += tf.reduce_mean(tf.square(u[5][xLnodes] - u[7][xLnodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[5][xLnodes] - u[8][x0nodes]))\n",
    "            loss += tf.reduce_mean(tf.square(u[7][xLnodes] - u[8][x0nodes]))\n",
    "\n",
    "        \n",
    "        # Kirchhoff-Neumann condition in center nodes\n",
    "        for i in self.graphPINN.innerVertices:\n",
    "            tmp = 0\n",
    "            for j in self.graphPINN.Vin[i]:\n",
    "                tmp += u1x[j][xLnodes]\n",
    "            for j in self.graphPINN.Vout[i]:\n",
    "                tmp -= u1x[j][x0nodes]\n",
    "            loss += tf.reduce_mean(tf.square(tmp))\n",
    "        \n",
    "        if False:\n",
    "            # Vertex 1\n",
    "            loss += tf.reduce_mean(tf.square(u1x[0][xLnodes] - u1x[1][x0nodes] - u1x[2][x0nodes]))\n",
    "            # Vertex 2\n",
    "            loss += tf.reduce_mean(tf.square(u1x[1][xLnodes] - u1x[3][x0nodes] - u1x[4][x0nodes]))\n",
    "            # Vertex 3\n",
    "            loss += tf.reduce_mean(tf.square(u1x[3][xLnodes] - u1x[5][x0nodes]))\n",
    "            # Vertex 4\n",
    "            loss += tf.reduce_mean(tf.square(u1x[2][xLnodes] + u1x[4][xLnodes] - u1x[6][x0nodes]))\n",
    "            # Vertex 5\n",
    "            loss += tf.reduce_mean(tf.square(u1x[6][xLnodes] - u1x[7][x0nodes]))\n",
    "            # Vertex 6\n",
    "            loss += tf.reduce_mean(tf.square(u1x[5][xLnodes] + u1x[7][xLnodes] - u1x[8][x0nodes]))\n",
    "\n",
    "\n",
    "        \n",
    "        # Multiple leaves are Dirichlet nodes\n",
    "        \n",
    "        # Dirichlet node is vertex 0\n",
    "        alpha = 1.\n",
    "        loss += tf.reduce_mean(tf.square(u[0][x0nodes]-(alpha)))\n",
    "        #beta = 0.01\n",
    "        #        loss += tf.cond(u[8][xLnodes] > 0.5, tf.reduce_mean(tf.square(u[8][xLnodes]-(1.-beta))), 0.0)\n",
    "\n",
    "        return loss    \n",
    "    \n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with\n",
    "            # respect to trainable variables\n",
    "            tape.watch(self.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "\n",
    "        g = tape.gradient(loss, self.trainable_variables)\n",
    "        del tape\n",
    "\n",
    "        return loss, g\n",
    "    \n",
    "    \n",
    "    def solve_with_TFoptimizer(self, optimizer, N=1001):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "\n",
    "        for i in range(N):\n",
    "            loss, g = self.get_grad()\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            for j in range(self.graphPINN.n_e):\n",
    "                optimizer.apply_gradients(zip(g[j], self.trainable_variables[j]))\n",
    "\n",
    "            self.current_loss = loss.numpy()\n",
    "            self.callback()\n",
    "            \n",
    "\n",
    "    def solve_with_ScipyOptimizer(self, method='L-BFGS-B', **kwargs):\n",
    "        \"\"\"This method provides an interface to solve the learning problem\n",
    "        using a routine from scipy.optimize.minimize.\n",
    "        (Tensorflow 1.xx had an interface implemented, which is not longer\n",
    "        supported in Tensorflow 2.xx.)\n",
    "        Type conversion is necessary since scipy-routines are written in\n",
    "        Fortran which requires 64-bit floats instead of 32-bit floats.\"\"\"\n",
    "\n",
    "        def get_weight_tensor():\n",
    "            \"\"\"Function to return current variables of the model\n",
    "            as 1d tensor as well as corresponding shapes as lists.\"\"\"\n",
    "\n",
    "            weight_list = []\n",
    "            shape_list = []\n",
    "\n",
    "            # Loop over all variables, i.e. weight matrices, bias vectors\n",
    "            # and unknown parameters\n",
    "            for i in range(self.graphPINN.n_e):\n",
    "                for v in self.graphPINN.NNs[i].variables:\n",
    "                    shape_list.append(v.shape)\n",
    "                    weight_list.extend(v.numpy().flatten())\n",
    "\n",
    "            weight_list = tf.convert_to_tensor(weight_list)\n",
    "            return weight_list, shape_list\n",
    "\n",
    "        x0, shape_list = get_weight_tensor()\n",
    "\n",
    "        def set_weight_tensor(weight_list):\n",
    "            \"\"\"Function which sets list of weights\n",
    "            to variables in the model.\"\"\"\n",
    "            idx = 0\n",
    "            for i in range(self.graphPINN.n_e):\n",
    "                for v in self.graphPINN.NNs[i].variables:\n",
    "                    vs = v.shape\n",
    "\n",
    "                    # Weight matrices\n",
    "                    if len(vs) == 2:\n",
    "                        sw = vs[0] * vs[1]\n",
    "                        new_val = tf.reshape(\n",
    "                            weight_list[idx:idx + sw], (vs[0], vs[1]))\n",
    "                        idx += sw\n",
    "\n",
    "                    # Bias vectors\n",
    "                    elif len(vs) == 1:\n",
    "                        new_val = weight_list[idx:idx+vs[0]]\n",
    "                        idx += vs[0]\n",
    "\n",
    "                    # Variables (in case of parameter identification setting)\n",
    "                    elif len(vs) == 0:\n",
    "                        new_val = weight_list[idx]\n",
    "                        idx += 1\n",
    "\n",
    "                    # Assign variables (Casting necessary since scipy requires float64 type)\n",
    "                    v.assign(tf.cast(new_val, tf.float32))\n",
    "        \n",
    "        def get_loss_and_grad(w):\n",
    "            \"\"\"Function that provides current loss and gradient\n",
    "            w.r.t the trainable variables as vector. This is mandatory\n",
    "            for the LBFGS minimizer from tfp.optimizer.\"\"\"\n",
    "\n",
    "            # Update weights in model\n",
    "            set_weight_tensor(w)\n",
    "            # Determine value of \\phi and gradient w.r.t. \\theta at w\n",
    "            loss, grad = self.get_grad()\n",
    "\n",
    "            # Flatten gradient\n",
    "            grad_flat = []\n",
    "            for i in range(self.graphPINN.n_e):\n",
    "                for g in grad[i]:\n",
    "                    grad_flat.extend(g.numpy().flatten())\n",
    "\n",
    "            # Store current loss for callback function\n",
    "            self.current_loss = loss\n",
    "\n",
    "            # Return value and gradient of \\phi as tuple\n",
    "            return loss.numpy().astype(np.float64), np.array(grad_flat, dtype=np.float64)\n",
    "\n",
    "        return scipy.optimize.minimize(fun=get_loss_and_grad,\n",
    "                                       x0=x0,\n",
    "                                       jac=True,\n",
    "                                       method=method,\n",
    "                                       callback=self.callback,\n",
    "                                       **kwargs)\n",
    "\n",
    "            \n",
    "    \n",
    "    def callback(self, xr=None):\n",
    "        if self.iter % 50 == 0:\n",
    "            print('It {:05d}: loss = {:10.8e}'.format(\n",
    "                self.iter, self.current_loss))\n",
    "        self.hist.append(self.current_loss)\n",
    "        self.iter += 1\n",
    "        \n",
    "\n",
    "    def plot_loss_history(self, ax=None):\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(7, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "        ax.semilogy(range(len(self.hist)), self.hist, 'k-')\n",
    "        ax.set_xlabel('$n_{epoch}$')\n",
    "        ax.set_ylabel('$\\\\phi^{n_{epoch}}$')\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solve easy instationary equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw collocation points uniformly or take them equidistantly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0 = 40\n",
    "N_t = 40\n",
    "N_x = 40\n",
    "#N_r = 50\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Uniform distributed points for initial boundary data\n",
    "t_0 = tf.ones((N_0+1,1), dtype=DTYPE)*lb[0]\n",
    "x_0 = tf.reshape(tf.linspace(lb[1],ub[1],N_0+1), (N_0+1, 1))\n",
    "X_0 = tf.concat([t_0, x_0], axis=1)\n",
    "\n",
    "# Uniform distributed collocation points\n",
    "t_r = tf.linspace(lb[0], ub[0], N_t+1)\n",
    "x_r = tf.linspace(lb[1], ub[1], N_x+1)\n",
    "tt, xx = tf.meshgrid(t_r,x_r)\n",
    "X_r = tf.concat([tf.reshape(tt,(-1,1)), tf.reshape(xx,(-1,1))], axis=1)\n",
    "# Evaluate intitial condition at x_0\n",
    "#u_0 = fun_u_0(x_0)\n",
    "\n",
    "\n",
    "# Draw uniform sample points for initial boundary data\n",
    "#x_0 = tf.random.uniform((N_0,1), lb[1], ub[1], dtype=DTYPE)\n",
    "\n",
    "# Draw uniformly sampled collocation points\n",
    "#t_r = tf.random.uniform((N_r,1), lb[0], ub[0], dtype=DTYPE)\n",
    "#x_r = tf.random.uniform((N_r,1), lb[1], ub[1], dtype=DTYPE)\n",
    "#X_r = tf.concat([t_r, x_r], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = graphPINNSolver(gPINN,X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with TF optimizer\n",
      "\n",
      "It 00000: loss = 1.07641430e+01\n",
      "It 00050: loss = 1.36594549e-01\n",
      "It 00100: loss = 7.56100416e-02\n",
      "It 00150: loss = 6.85818493e-02\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "PyEval_EvalFrameEx returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_zeros\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    685\u001b[0m   \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m   \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mzeros_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;34m\"\"\"Per-device cache for scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_caches_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-d22e81cf08cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start with TF optimizer\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_with_TFoptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m401\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-9914b35a414a>\u001b[0m in \u001b[0;36msolve_with_TFoptimizer\u001b[0;34m(self, optimizer, N)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# Perform gradient descent step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-9914b35a414a>\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0;34m\"\"\"Process output gradients and call the backward function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# Solve with Adam optimizer\n",
    "lr = 0.001\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "print('Start with TF optimizer\\n')\n",
    "pinn.solve_with_TFoptimizer(optim, N=401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End with L-BFGS-B algorithm\n",
      "It 00450: loss = 4.47113812e-02\n",
      "It 00500: loss = 4.26397100e-02\n",
      "It 00550: loss = 4.18264568e-02\n",
      "It 00600: loss = 4.12348174e-02\n",
      "It 00650: loss = 4.04789113e-02\n",
      "It 00700: loss = 3.98918875e-02\n",
      "It 00750: loss = 3.95008363e-02\n",
      "It 00800: loss = 3.92067581e-02\n",
      "It 00850: loss = 3.88129801e-02\n",
      "It 00900: loss = 3.85536328e-02\n",
      "It 00950: loss = 3.84500585e-02\n",
      "It 01000: loss = 3.83712202e-02\n",
      "It 01050: loss = 3.83127555e-02\n",
      "It 01100: loss = 3.82587835e-02\n",
      "It 01150: loss = 3.82047296e-02\n",
      "It 01200: loss = 3.81392837e-02\n",
      "It 01250: loss = 3.80549282e-02\n",
      "It 01300: loss = 3.79697531e-02\n",
      "It 01350: loss = 3.78957987e-02\n",
      "It 01400: loss = 3.77972126e-02\n",
      "It 01450: loss = 3.77177224e-02\n",
      "It 01500: loss = 3.76309603e-02\n",
      "It 01550: loss = 3.75523940e-02\n",
      "It 01600: loss = 3.74636091e-02\n",
      "It 01650: loss = 3.73903252e-02\n",
      "It 01700: loss = 3.73246893e-02\n",
      "It 01750: loss = 3.72874439e-02\n",
      "It 01800: loss = 3.72556895e-02\n",
      "It 01850: loss = 3.72109823e-02\n",
      "It 01900: loss = 3.71592008e-02\n",
      "It 01950: loss = 3.70887741e-02\n",
      "It 02000: loss = 3.70172299e-02\n",
      "It 02050: loss = 3.69242951e-02\n",
      "It 02100: loss = 3.67139317e-02\n",
      "It 02150: loss = 3.65994871e-02\n",
      "It 02200: loss = 3.65143903e-02\n",
      "It 02250: loss = 3.64015363e-02\n",
      "It 02300: loss = 3.63177992e-02\n",
      "It 02350: loss = 3.61999199e-02\n",
      "It 02400: loss = 3.60808671e-02\n",
      "It 02450: loss = 3.60067599e-02\n",
      "It 02500: loss = 3.59144211e-02\n",
      "It 02550: loss = 3.58391330e-02\n",
      "It 02600: loss = 3.57615985e-02\n",
      "It 02650: loss = 3.56689617e-02\n",
      "It 02700: loss = 3.56292389e-02\n",
      "It 02750: loss = 3.55930701e-02\n",
      "It 02800: loss = 3.55111137e-02\n",
      "It 02850: loss = 3.54605503e-02\n",
      "It 02900: loss = 3.53778228e-02\n",
      "It 02950: loss = 3.53175327e-02\n",
      "It 03000: loss = 3.52749676e-02\n",
      "It 03050: loss = 3.52295041e-02\n",
      "It 03100: loss = 3.51659730e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-98802e3df608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'End with L-BFGS-B algorithm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_with_ScipyOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-132-9914b35a414a>\u001b[0m in \u001b[0;36msolve_with_ScipyOptimizer\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                        \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                                        \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                                        **kwargs)\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-9914b35a414a>\u001b[0m in \u001b[0;36mget_loss_and_grad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mset_weight_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# Determine value of \\phi and gradient w.r.t. \\theta at w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;31m# Flatten gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-9914b35a414a>\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1264\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m-> 1266\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('End with L-BFGS-B algorithm')\n",
    "pinn.solve_with_ScipyOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877e823387e043218297e4549237e070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cade83b7754c7cbdfbafe97797a63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='j', max=60), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "Nt = 60\n",
    "Nx = 30\n",
    "\n",
    "tspace = tf.linspace(lb[0], ub[0], Nt + 1)\n",
    "xspace = tf.reshape(tf.linspace(lb[1], ub[1], Nx + 1), [-1, 1])\n",
    "\n",
    "def plot_network(j=0):\n",
    "    fig = plt.figure(1,clear=True)\n",
    "    ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "    t0 = tf.ones_like(xspace)*tspace[j]\n",
    "    pos = pinn.graphPINN.pos\n",
    "    for i, e in enumerate(pinn.graphPINN.E):\n",
    "        xy = pos[e[0]] + xspace*(pos[e[1]] - pos[e[0]]) \n",
    "        #xy = pos[e[0].numpy()] + xspace*(pos[e[1].numpy()] - pos[e[0].numpy()]) \n",
    "        #xy = V[e[0].numpy()]+xspace*(V[e[1].numpy()]-V[e[0].numpy()])\n",
    "        u = pinn.graphPINN.NNs[i](tf.concat([t0,xspace],axis=1))\n",
    "        unum = u.numpy().flatten()\n",
    "        ax.plot(xy[:,0], xy[:,1], unum)\n",
    "        #ax.plot(xy[:,0], xy[:,1], unum * (1-unum))\n",
    "    \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_zlim([0.0,1.0])\n",
    "    #ax.set_zlabel('$u_\\\\theta(x,y)$')\n",
    "    ax.view_init(19, 135)\n",
    "    return u\n",
    "\n",
    "#fig.canvas.layout.width = '100%'\n",
    "#fig.canvas.layout.height = '900px'\n",
    "j_slider = widgets.IntSlider(min=0,max=Nt,step=1)\n",
    "interactive_plot = interactive(plot_network, j=j_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "#output.layout.height = '350px'\n",
    "interactive_plot\n",
    "#u = plot_network(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.graphPINN.vertexVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([20.,  1.], dtype=float32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
