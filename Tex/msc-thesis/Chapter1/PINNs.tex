\section{Physics-Informed Neural Networks}
\label{ch1:sec4}

In this section we describe physics-informed neural networks, abbreviated by PINNs, which are neural networks, introduced in \cite{RaissiPerdikarisKarniadakisPart1:2017}, that are trained to solve supervised learning tasks while respecting any given law of physics described by general non-linear partial differential equations. PINNs enable the solution of a wide range of problems in computational science, represent a pioneering technology leading to the development of new classes of numerical solution methods for PDEs, and can be considered as a mesh-free alternative to traditional approaches and as a novel data-driven approach to model inversion and system identification \cite[p.~3]{RaissiPerdikarisKarniadakis:2019}. \\
Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier-Stokes equations are a set of partial differential equations derived from the conservation laws that describe the physics of many phenomena of scientific and engineering interest. They can be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. However, these equations cannot be solved exactly and therefore numerical methods must be used such as a finite volume method, where these equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization. \\
As discussed in the previous section, machine learning methods with deep neural networks offer a promising way to approximate any function, therefore it is only logical to use them also for the approximation of the solution of differential equations. If one follows the classical approach, in which one trains a network purely by feeding it with data obtained, for example, from empirical tests of the underlying system, two problems arise. First, when analysing complex physical, biological or technical systems, the cost of data acquisition is often prohibitive and one is faced with the challenge of drawing conclusions and making decisions based on incomplete information. For the often resulting small amount of data, most of the modern machine learning methods are not robust enough and offer no convergence guarantee, which is why the task of training a network with a few high-dimensional input and output values seems naive. Second, neural networks do not in general take into account the physical properties underlying the system generated from the physical law, and the degree of approximation accuracy they provide still depends heavily on a careful specification of the problem geometry and the initial and boundary conditions. Without this prior information, the solution is not unique and may lose physical correctness. Exactly this prior information, such as the principal physical laws governing the time-dependent dynamics of a system, or some empirically validated rules or other expertise, can act as a control agent that restricts the space of admissible solutions to a manageable size, which leads to the idea of a physics-informed neural network. This type of neural networks uses the governing physical equations in training, i.e. PINNs are designed to be trained to satisfy both, the given training data and the given differential equations. In turn, encoding such structured information in a learning algorithm leads to an increase in the information content of the data which the algorithm encounters, so that it can move quickly towards the correct solution and generalise well, even when only a small data set is available, which is in some sense a key property of PINNs. Therefore, with some knowledge of the physical properties of the problem and some form of training data, PINNs can be used to find an optimal solution with high accuracy. Moreover, PINNs can be used for data-driven discovery of partial differential equations or system identification, which is concerned with finding parameters that best describe the observed data of a test of a system, \cite[pp.~1-2]{RaissiPerdikarisKarniadakisPart1:2017}. However, this is not relevant for this work and will therefore not be described. \\
We concentrate on the problem of computing data-driven approximative solutions to non-linear partial differential equations, by encoding explicitly the differential equation formulation in the neural network. We consider the following general form of a differential equation:
\begin{equation}
    \label{PINN PDE}
    u_t + \mathcal{H} \left[ u \right] = 0, \quad x \in \Omega, \quad t \in \left[ 0, T \right], 
\end{equation}
where $u(t,x)$ is the latent solution of the corresponding system, $u_t$ is its derivative with respect to the time $t$ in the period $\left[ 0, T \right]$, $\mathcal{H} \left[ \cdot \right]$ is a non-linear differential operator, $x$ is an independent, possibly multi-dimensional variable, defined over the domain $\Omega \subset \mathbb{R}^{n}$. \\
A distinction is made between discrete time and continuous time models for PINNs, depending on the type and arrangement of the available data. We focus on the latter. For this purpose let $f(x,t)$ be defined as the left-hand-side of \cref{PINN PDE}, i.e.
\begin{equation}
    \label{Residual Network}
    f(t,x) = u_t + \mathcal{H} \left[ u \right].
\end{equation}
The main innovation of PINNs is that in \cref{Residual Network}, $u$ is approximated by a deep neural network and thus \cref{Residual Network} becomes a so-called residual network, which encodes the governing physical equations, takes the output of the neural network that approximates $u$ and computes a residual. This means that in the basic formulation of PINNs, two networks are combined: the surrogate network $u_\Theta$, which approximates $u$ and therefore receives the same input parameters $(t,x)$, and the resulting residual network $f_\Theta$, which emerges when $u_\Theta$ is substituted into \cref{Residual Network}, i.e. $f_\Theta(t,x) = (u_\Theta)_t + \mathcal{H} \left[ u_\Theta \right]$. In principle, any type of neural network can be used for the surrogate network $u_\Theta$. The most common are simple (fully-connected) deep feed-forward neural networks, see e.g. \cite{RaissiPerdikarisKarniadakis:2019}, \cite{MishraMolinaro:2021}, \cite{Markidis:2021}. The choice of the activation functions in each layer is critical for the performance of PINNs. As shown by \cite{MishraMolinaro:2021} PINNs require sufficiently smooth activation functions such as the sigmoid function, \cref{Sigmoid}, or the hyperbolic tangent function. Non-smooth activation functions, such as ReLU, \cref{ReLU}, lead to non-convergent methods, i.e. in the limit of an infinite training dataset a well-trained PINN with ReLU-like activation functions, the solution does not converge to the exact solution of the differential equation, see \cite{MishraMolinaro:2021}. To obtain the residual network $f_\Theta$ from the surrogate network $u_\Theta$, one uses recent developments in automatic differentiation - one of the most useful but perhaps underused techniques in scientific computing - to differentiate the neural network $u_\Theta(t,x)$ with respect to its input values $t$ and $x$. The application of automatic differentiation to elegantly and efficiently formulate the derivative by time of the surrogate network $(u_\Theta)_t$ and the application of the operator $\mathcal{H} \left[ u_\Theta \right]$ in the residual network $f_\Theta(t,x)$ is the main strength of PINNs. We note that the networks $u_\Theta$ and $f_\Theta$ share the same weights $W$ and biases $b$, but generally not the same activation functions, since in the residual network $f_\Theta$ the derivatives of the activation functions of $u_\Theta$ are generally considered. Since $f_\Theta$ contains only $u_\Theta$ as a neural network and no other network, $u_\Theta$ and $f_\Theta$ also have the same network topology, but the propagation of information through the neural networks is different. \\
PINNs are deep-learning networks that have to be trained. However, the residual network $f_\Theta$, which is the characteristic feature of PINNs, is officially not trained, since its only function is to provide the surrogate network $u_\Theta$ with the residual. The surrogate network $u_\Theta$ of a PINN is trained, which afterwards returns an approximate solution $u_\Theta$ of a differential equation for an input point $(t,x)$, which we call collocation point. The weights $W$ and biases $b$ of the surrogate network $u_\Theta$ can be learned by minimizing the following cost function
\begin{equation}
    \label{MSE PINN}
    MSE = MSE_f + MSE_u, 
\end{equation}
where
\begin{equation*}
    MSE_f = \frac{1}{N_f} \sum^{N_f}_{i = 1} \lVert f_\Theta (t^{f}_i, x^{f}_i) \rVert^{2}_{2}
\end{equation*}
and
\begin{equation*}
    MSE_u = \frac{1}{N_u} \sum^{N_u}_{i = 1} \lVert u_\Theta(t^{u}_i, x^{u}_i) - u_i \rVert^{2}_{2}.
\end{equation*}
The data $\{t^{u}_i, x^{u}_i, u_i \}_{i = 1, \ldots, N_u}$ denotes the initial and boundary training data on $u_\Theta$. This data ensures by minimizing $MSE_u$ that $u_\Theta$ takes over the initial and boundary conditions of the differential equation. The data is generated by using, for example, for $u_i$ the value $u(0,x^{u}_i)$ explicitly required by an initial condition for all points $x^{u}_i \in \Omega$, i.e. $t^{u}_i = 0$ for all $i = 1, \ldots, N_u$. With the collocation points $\{t^{f}_i, x^{f}_i \}_{i = 1, \ldots, N_f}$, the structured information imposed by the partial differential equation is to be enforced by $MSE_f$. We note that of course the value of $MSE$ also depends on both the size of the data sets, i.e. $N_u + N_f$, and the distribution of collocation points $\{t^{f}_i, x^{f}_i \}_{i = 1, \ldots, N_f}$ (with that $\{t^{u}_i, x^{u}_i \}_{i = 1, \ldots, N_u}$). One of the most used data set distributions is the uniform distribution, where the collocation points are uniformly spaced on $\left[ 0, T \right] \times \Omega$ as on a uniform grid. In addition, simulation or experimental data can also flow in for training the network in a supervised manner, which is for example necessary for data assimilation, inverse problems and super-resolution. We see that the learning of the network $u_\Theta$, which is done by minimizing \cref{MSE PINN}, follows both a supervised approach through $MSE_u$ and an unsupervised approach through $MSE_f$. We nevertheless refer to the learning of PINNs as supervised learning, since both by introducing labelled data  $u_i = u(t^{u}_i, x^{u}_i)$ into the optimization problem and by introducing $y_i=0$ for each collocation point $\{t^{f}_i, x^{f}_i \}_{i = 1, \ldots, N_f}$, we can form a supervised learning cost function from $MSE_f$. 

In order to minimize $MSE$ in the training phase, two minimizers in succession are typically used: at first the Adam optimizer, see e.g. \cite{KingmaBa:2017} and after that L-BFGS-B optimizer \cite{ByrdLuNocedalZhu:1995}, which is a limited-memory version of BFGS to handle bound constrained optimization problems with many variables.







In the training phase, an optimization process targeting the residual minimization determines the weights and
biases of the surrogate network. Typically, we use two optimizers in succession: the Adam optimizer as first and
then a Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer [6]. BFGS uses the Hessian matrix (curvature in highly
dimensional space) to calculate the optimization direction and provides more accurate results. However, if used directly
without using the Adam optimizer can rapidly converge to a local minimum (for the residual) without exiting. For this
reason, the Adam optimizer is used first to avoid local minima, and then the solution is refined by BFGS. We note that
the typical BFGS used in PINNs is the L-BFGS-B: L-BFGS is a limited-memory version of BFGS to handle problems with
many variables, such as DL problems; the BFGS-B is a variant of BFGS for bound constrained optimization problems. In
our work, we tested several optimizers, including Newton and Powell methods, and found that L-BFGS-B provides by
far the highest accuracy and faster convergence in all our test problems. L-BFGS-B is currently the most critical
technology for PINNs.


Once a PINN is trained, the inference from the trained PINN can be used to replace traditional numerical solvers in scientific computing. In this so-called inference or prediction step, the input includes independent variables like simulation time step and simulation domain positions. The output is the solution of the governing equations at the time and position specified by the input. Therefore, PINNs are a gridless method because any point in in the domain can be taken as input without requiring the definition of a mesh. Moreover, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need of being retrained. For this reason, the computational cost does not scale with the number of grid points like many traditional computational methods.


Despite the recent introduction of PINNs, several PINN frameworks for PDE solutions exist. All the major PINN
frameworks are written in Python and rely either on TensorFlow [1] or PyTorch [29] to express the neural network
architecture and exploit auto-differentiation used in the residual network. Together with TensorFlow, SciPy [43] is
often used to use high-order optimizers such as L-BFGS-B. Two valuable PINN Domain-Specific Languages (DSL) are
DeepXDE [19] and sciANN [11]. DeepXDE is an highly customizable framework with TensorFlow 1 and 2 backend and it
supports basic and fractional PINNs in complex geometries. sciANN is a DSL based on and similar to Keras [10]. In this
work, we use the DeepXDE DSL



Burgers equation

Bild einf√ºgen!

TensorFlow nennen und andere PINN sachen

Markidis Paper lesen