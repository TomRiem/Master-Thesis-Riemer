\section{Physics-Informed Neural Networks}
\label{ch1:sec4}

In this section we describe physics-informed neural networks, abbreviated by PINNs, which  were introduced in \cite{RaissiPerdikarisKarniadakisPart1:2017} and are neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general non-linear partial differential equations. PINNs enable the solution of a wide range of problems in computational science, represent a pioneering technology leading to the development of new classes of numerical solution methods for PDEs, and can be considered as a mesh-free alternative to traditional approaches and as a novel data-driven approach to model inversion and system identification \cite[p.~3]{RaissiPerdikarisKarniadakis:2019}. \\
Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier-Stokes equations are a set of partial differential equations derived from the conservation laws that describe the physics of many phenomena of scientific and engineering interest. They can be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. However, these equations cannot be solved exactly and therefore numerical methods must be used such as a finite volume method, where these equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization. \\
As discussed in the previous section, machine learning methods with deep neural networks offer a promising way to approximate any map, therefore it is only logical to use them also for the approximation of the solution of differential equations. If one follows the classical approach, in which one trains a network purely by feeding it with data obtained, for example, from empirical tests of the underlying system, two problems arise. At first, when analysing complex physical, biological or technical systems, the cost of data acquisition is often prohibitive and one is faced with the challenge of drawing conclusions and making decisions based on incomplete information. For the often resulting small amount of data, most of the modern machine learning methods are not robust enough and offer no convergence guarantee, which is why the task of training a network with a few high-dimensional input and output values seems naive. Second, neural networks generally do not in general take into account the physical properties underlying the system generated from the physical law, and the degree of approximation accuracy they provide still depends heavily on a careful specification of the problem geometry and the initial and boundary conditions. Without this prior information, the solution is not unique and may lose physical correctness. Exactly this prior information, such as the principal physical laws governing the time-dependent dynamics of a system, or some empirically validated rules or other expertise, can act as a control agent that restricts the space of admissible solutions to a manageable size, which leads to the idea of a physics-informed neural network. This type of neural networks uses the governing physical equations in training, i.e. PINNs are designed to be trained to satisfy both the given training data and the given equations. In turn, encoding such structured information in a learning algorithm leads to an increase in the information content of the data which the algorithm encounters, so that it can move quickly towards the correct solution and generalise well, even when only a few training examples are available. Therefore, with some knowledge of the physical properties of the problem and some form of training data, PINNs can be used to find an optimal solution with high accuracy \cite{RaissiPerdikarisKarniadakisPart1:2017}. Moreover, PINNs can be used for data-driven discovery of partial differential equations or system identification, which is concerned with finding parameters that best describe the observed data of a test of a system. However, this is not relevant for this work and will therefore not be described. \\



If one follows the classical approach, in which one trains a network purely by feeding it with data obtained, for example, from empirical tests of the underlying system, two problems arise. 


a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information.



Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information

However, more often than not, in the course of analysing complex physical, biological or engineering systems, the cost of data acquisition is prohibitive, and we are inevitably faced with the challenge of drawing conclusions and making decisions under partial information.

At first sight, the task of training a deep learning algorithm to accurately identify a non-linear map from a few – potentially very high-dimensional – input and output data pairs seems at best naive. Coming to our rescue, for many cases pertaining to the modelling of physical and biological systems, there a exist a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice. Let it be the principled physical laws that govern the time-dependent dynamics of a system, or some empirical validated rules or other domain expertise, this prior information can act as a regularization agent that constrains the space of admissible solutions to a manageable size. In return, encoding such structured information into a learning algorithm results in amplifying the information content of the data that the algorithm sees, enabling it to quickly steer itself towards the right solution and generalize well even when only a few training examples are available.



In this work we take a different approach by employing deep neural networks and leverage their well known capability as universal function approximators. In this setting, we can directly tackle non-linear problems without the need for committing to any prior assumptions, linearization, or local time-stepping. We exploit recent developments in automatic differentiation – one of the most useful but perhaps underused techniques in scientific computing – to differentiate neural networks with respect to their input coordinates and model parameters to obtain physics informed neural networks. 

This simple yet powerful construction allows us to tackle a wide range of problems in computational science and introduces a potentially disruptive technology leading to the development of new data-efficient and physics-informed learning machines, new classes of numerical solvers for partial differential equations, as well as new data-driven approaches for model inversion and systems identification

Given noisy measurements of the system, we are interested in the solution of two distinct problems. The first problem is that of predictive inference, filtering and smoothing, or data driven solutions of partial differential equations which states: given fixed model parameters  what can be said about the unknown hidden state of the system?



Therefore, a key property of physics-informed neural networks is that they can be effectively trained using small data sets; a setting often encountered in the study of physical systems for which the cost of data acquisition may be prohibitive

Automatic differentiation in general, and the back-propagation algorithm in particular, is currently the dominant approach for training deep models by taking their derivatives with respect to the parameters (e.g., weights and biases) of the models. Here, we use the exact same automatic differentiation techniques, employed by the deep learning community, to physics-inform neural networks by taking their derivatives with respect to their input coordinates (i.e., space and time) where the physics is described by partial differential equations.

Throughout this work we have been using relatively simple deep feed-forward neural networks architectures with hyperbolic tangent activation functions and no additional regularization

Burgers equation