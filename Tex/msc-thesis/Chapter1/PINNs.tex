\section{Physics-Informed Neural Networks}
\label{ch1:sec4}

In this section we describe physics-informed neural networks, abbreviated by PINNs, which are neural networks, introduced in \cite{RaissiPerdikarisKarniadakisPart1:2017}, that are trained to solve supervised learning tasks while respecting any given law of physics described by general non-linear partial differential equations. PINNs enable the solution of a wide range of problems in computational science, represent a pioneering technology leading to the development of new classes of numerical solution methods for PDEs, and can be considered as a mesh-free alternative to traditional approaches and as a novel data-driven approach to model inversion and system identification \cite[p.~3]{RaissiPerdikarisKarniadakis:2019}. \\
Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier-Stokes equations are a set of partial differential equations derived from the conservation laws that describe the physics of many phenomena of scientific and engineering interest. They can be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. However, these equations cannot be solved exactly and therefore numerical methods must be used such as a finite volume method, where these equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization. \\
As discussed in the previous section, machine learning methods with deep neural networks offer a promising way to approximate any function, therefore it is only logical to use them also for the approximation of the solution of differential equations. If one follows the classical approach, in which one trains a network purely by feeding it with data obtained, for example, from empirical tests of the underlying system, two problems arise. First, when analysing complex physical, biological or technical systems, the cost of data acquisition is often prohibitive and one is faced with the challenge of drawing conclusions and making decisions based on incomplete information. For the often resulting small amount of data, most of the modern machine learning methods are not robust enough and offer no convergence guarantee, which is why the task of training a network with a few high-dimensional input and output values seems naive. Second, neural networks do not in general take into account the physical properties underlying the system generated from the physical law, and the degree of approximation accuracy they provide still depends heavily on a careful specification of the problem geometry and the initial and boundary conditions. Without this prior information, the solution is not unique and may lose physical correctness. Exactly this prior information, such as the principal physical laws governing the time-dependent dynamics of a system, or some empirically validated rules or other expertise, can act as a control agent that restricts the space of admissible solutions to a manageable size, which leads to the idea of a physics-informed neural network. This type of neural networks uses the governing physical equations in training, i.e. PINNs are designed to be trained to satisfy both, the given training data and the given differential equations. In turn, encoding such structured information in a learning algorithm leads to an increase in the information content of the data which the algorithm encounters, so that it can move quickly towards the correct solution and generalise well, even when only a small data set is available, which is in some sense a key property of PINNs. Therefore, with some knowledge of the physical properties of the problem and some form of training data, PINNs can be used to find an optimal solution with high accuracy. Moreover, PINNs can be used for data-driven discovery of partial differential equations or system identification, which is concerned with finding parameters that best describe the observed data of a test of a system, \cite[pp.~1-2]{RaissiPerdikarisKarniadakisPart1:2017}. However, this is not relevant for this work and will therefore not be described. \\
We concentrate on the problem of computing data-driven approximative solutions to non-linear partial differential equations, by encoding explicitly the differential equation formulation in the neural network. We consider the following general form of a differential equation:
\begin{equation}
    \label{PINN PDE}
    u_t + \mathcal{H} \left[ u \right] = 0, \quad x \in \Omega, \quad t \in [0, T], 
\end{equation}
where $u(t,x)$ is the latent solution of the corresponding system, $u_t$ is its derivative with respect to the time $t$ in the period $\left[ 0, T \right]$, $\mathcal{H} \left[ \cdot \right]$ is a non-linear differential operator, $x$ is an independent, possibly multi-dimensional variable, defined over the domain $\Omega \subset \mathbb{R}^{n}$. \\
A distinction is made between discrete time and continuous time models for PINNs, depending on the type and arrangement of the available data. We focus on the latter. For this purpose let $f(x,t)$ be defined as the left-hand-side of \cref{PINN PDE}, i.e.
\begin{equation}
    \label{Residual Network}
    f(t,x) = u_t + \mathcal{H} \left[ u \right].
\end{equation}
The main innovation of PINNs is that in \cref{Residual Network}, $u(t,x)$ is approximated by a deep neural network and thus \cref{Residual Network} becomes a so-called residual network, which encodes the governing physical equations, takes the output of the neural network that approximates $u(t,x)$ and computes a residual. This means that in the basic formulation of PINNs, two networks are combined: the surrogate network $\widetilde{u}(t,x)$, which approximates $u(t,x)$, and the resulting residual network $\widetilde{f}(t,x)$, which emerges when $\widetilde{u}(t,x)$ is substituted into \cref{Residual Network}, i.e. $\widetilde{f}(t,x) = \widetilde{u}_t + \mathcal{H} \left[ \widetilde{u} \right]$. In principle, any type of neural network can be used for the surrogate network $\widetilde{u}(t,x)$. The most common are simple (fully-connected) deep feed-forward neural networks, see e.g. \cite{RaissiPerdikarisKarniadakis:2019}, \cite{MishraMolinaro:2021}, \cite{Markidis:2021}. The choice of the activation functions in each layer is critical for the performance of PINNs. As shown by \cite{MishraMolinaro:2021} PINNs require sufficiently smooth activation functions such as the sigmoid function, \cref{Sigmoid}, or the hyperbolic tangent function. Non-smooth activation functions, such as ReLU, \cref{ReLU}, lead to non-convergent methods, i.e. in the limit of an infinite training dataset a well-trained PINN with ReLU-like activation functions, the solution does not converge to the exact solution of the differential equation, see \cite{MishraMolinaro:2021}. To obtain the residual network $\widetilde{f}(t,x)$ from the surrogate network $\widetilde{u}(t,x)$, one uses recent developments in automatic differentiation - one of the most useful but perhaps underused techniques in scientific computing - to differentiate the neural network $\widetilde{u}(t,x)$ with respect to its input values $t$ and $x$. The application of automatic differentiation to elegantly and efficiently formulate the derivative by time of the surrogate network $\widetilde{u}_t$ and the application of the operator $\mathcal{H} \left[ \widetilde{u} \right]$ in the residual network $\widetilde{f}(t,x)$ is the main strength of PINNs. We note that the networks $\widetilde{u}(t,x)$ and $\widetilde{f}(t,x)$ share the same weights $W$ and biases $b$, but generally not the same activation functions, since in the residual network $\widetilde{f}(t,x)$ the derivatives of the activation functions of $\widetilde{u}(t,x)$ are generally considered.  \\

PINNs are now deep-learning networks that have to be trained. However, the residual network, which is the characteristic feature of PINNs, is officially not trained, since its only function is to provide the surrogate network with the residual. The surrogate network of a PINN is trained, which then returns an approximate solution of a differential equation for an input point in the integration domain, which we call the collocation point.

The weights and biases of the neural networks f and u can be then learned by minimizing the following cost function
\begin{equation*}
    MSE = MSE_f + MSE_u, 
\end{equation*}
where
\begin{equation*}
    MSE_f = \frac{1}{N_f} \sum^{N_f}_{i = 1} \lVert f(t^{f}_i, x^{f}_i) \rVert^{2}_{2}
\end{equation*}
and
\begin{equation*}
    MSE_u = \frac{1}{N_u} \sum^{N_u}_{i = 1} \lVert u(t^{u}_i, x^{u}_i) - u_i \rVert^{2}_{2}.
\end{equation*}




The loss MSEu corresponds to the initial and boundary data while MSEf enforces the structure imposed by equation (2) at a finite set of collocation points.

This term $MSE_f$ requires the structured information represented by the partial differential equations to be satisfied in the training process.

Here, $\{t^{u}_i, x^{u}_i, u_i \}_{i = 1, \ldots, N_u}$ denote the initial and boundary training data on u and  $\{t^{f}_i, x^{f}_i \}_{i = 1, \ldots, N_f}$ specify the collocations points for f. The loss MSEu corresponds to the initial and boundary data while MSEf enforces the structure imposed by equation (2) at a finite set of collocation points. 



The basic formulation of the PINN training does not require labeled data, e.g., results from other simulations or experimental data, and is unsupervised: PINNs only require the evaluation of the residual function [23]. Providing simulation or experimental data for training the network in a supervised manner is also possible and necessary for so data-assimilation [36], inverse problems [24], super resolution [5, 44], and discrete PINNs [35]. The supervised approach is often used for solving ill-defined problems when for instance we lack boundary conditions or an Equation of State (EoS) to close a system of equations (for instance, EoS for the fluid equations [48]). In this study, we only focus on the basic PINNs as we are interested in solving PDEs without relying on other simulations to assist the DL network training. A common case in scientific applications is that we solve the same PDE with different source terms at each time step. For instance, in addition to other computational kernels, Molecular Dynamics (MD) code and semi-implicit fluid and plasma codes, such as GROMACS [42], Nek5000 [30], and iPIC3D [22], calculate the Poisson equation for the electrostatic and pressure solver [27] and divergence cleaning operations at each cycle.

Once a PINN is trained, the inference from the trained PINN can be used to replace traditional numerical solvers in scientific computing. In this so-called inference or prediction step, the input includes independent variables like simulation time step and simulation domain positions. The output is the solution of the governing equations at the time and position specified by the input. Therefore, PINNs are a gridless method because any point in in the domain can be taken as input without requiring the definition of a mesh. Moreover, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need of being retrained. For this reason, the computational cost does not scale with the number of grid points like many traditional computational methods.






We find that
fully-connected surrogate/approximator networks with more than three layers produce similar performance results in
the first thousand training epochs. The choice of activation function is critical for PINN performance: depending on the
smoothness of the source term, different activation functions provide considerably different accuracy and convergence.

As shown by Ref. [23], PINNs requires sufficiently
smooth activation functions. PINNs with ReLU and other non-smooth activation functions, such as ELU and
SELU (Exponential and Scaled Exponential Linear Units) are not “consistent/convergent" methods: in the
limit of an infinite training dataset a well-trained PINN with ReLU-like activation functions, the solution does not
converge to the exact solution [23]

In the training phase, an optimization process targeting the residual minimization determines the weights and
biases of the surrogate network. Typically, we use two optimizers in succession: the Adam optimizer as first and
then a Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer [6]. BFGS uses the Hessian matrix (curvature in highly
dimensional space) to calculate the optimization direction and provides more accurate results. However, if used directly
without using the Adam optimizer can rapidly converge to a local minimum (for the residual) without exiting. For this
reason, the Adam optimizer is used first to avoid local minima, and then the solution is refined by BFGS. We note that
the typical BFGS used in PINNs is the L-BFGS-B: L-BFGS is a limited-memory version of BFGS to handle problems with
many variables, such as DL problems; the BFGS-B is a variant of BFGS for bound constrained optimization problems. In
our work, we tested several optimizers, including Newton and Powell methods, and found that L-BFGS-B provides by
far the highest accuracy and faster convergence in all our test problems. L-BFGS-B is currently the most critical
technology for PINNs.

Despite the recent introduction of PINNs, several PINN frameworks for PDE solutions exist. All the major PINN
frameworks are written in Python and rely either on TensorFlow [1] or PyTorch [29] to express the neural network
architecture and exploit auto-differentiation used in the residual network. Together with TensorFlow, SciPy [43] is
often used to use high-order optimizers such as L-BFGS-B. Two valuable PINN Domain-Specific Languages (DSL) are
DeepXDE [19] and sciANN [11]. DeepXDE is an highly customizable framework with TensorFlow 1 and 2 backend and it
supports basic and fractional PINNs in complex geometries. sciANN is a DSL based on and similar to Keras [10]. In this
work, we use the DeepXDE DSL

Throughout this work we have been using relatively simple deep feed-forward neural networks architectures with hyperbolic tangent activation functions and no additional regularization









Throughout this work we have been using relatively simple deep feed-forward neural networks architectures with hyperbolic tangent activation functions and no additional regularization

Burgers equation

Bild einfügen!

Markidis Paper lesen