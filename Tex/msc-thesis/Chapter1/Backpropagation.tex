\chapter{Backpropagation}

In this chapter we want to specify an iterative scheme that implements the explicit partial derivatives of the loss function.

\begin{itemize}
    \item residual loss term for each edge $e \in \mathcal{E}$ \begin{equation*} \phi_{e,r} (X_e) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} (r_{\theta_e} (X_e^i))^2 \end{equation*} where \begin{equation*} r_{\theta_e} (X_e^i) = r_{\theta_e} (x_e^i, t_e^i) = \partial_t \rho_{\theta_{e}} (x_{e}^i , t_{e}^i) - \varepsilon \partial_{xx} \rho_{\theta_e}(x_e^i, t_e^i) - \partial_x [f(\rho_{\theta_e}(x_e^i, t_e^i)) \partial_x V_e(x_e^i, t_e^i)] \end{equation*} for $X_e = \{X_e^i\}_{i=1}^{n_e} = \{(t_e^i, x_e^i)\}_{i=1}^{n_e} \subset [0,\ell_e] \times (0, T)$.
    \item a misfit term enforcing the initial conditions for each edge $e \in \mathcal{E}$ \begin{equation*} \phi_{e,0} (X_{e,0}) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0} (\rho_{\theta_e} (0,x_{e,0}^i) - \rho_{e,0}(x_{e,0}^i))^2, \end{equation*} where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of boundary points along $t=0$ and $\rho_{e,0}$ \ldots
    \item a misfit term enforcing the Kirchhoff-Neumann conditions for each vertex $v \in \mathcal{V}_\mathcal{K}$ \begin{equation*} \phi_{v,K} (X_{v,b}) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} (\sum_{e \in \mathcal{E}_v} (- \varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v))^2, \end{equation*} where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset (0,T)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced, $\mathcal{E}_v$ is the set of edges incident to node $v$ and the derivative is taken into the outgoing direction,
    \item a misfit term enforcing the flux boundary conditions for each vertex $v \in \mathcal{V}_\mathcal{K}$ \begin{align*} \phi_{v,D} (X_{v,b}) & \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} ( \sum_{e\in \mathcal{E}_v} (-\varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v) + \\ & \quad + \alpha_v(t_{v,b}^i) (1-\rho_v^i) - \beta_v(t_{v,b}^i) \rho_v^i)^2 \end{align*}
    \item a misfit term enforcing the continuity in the vertices $v \in \mathcal{V}_\mathcal{K}$ \begin{equation*} \phi_{v,c} (X_{v,b}) \coloneqq \frac{1}{\abs{\mathcal{E}_v}} \sum_{e \in \mathcal{E}_v} \frac{1}{n_b} \sum_{i=1}^{n_b} ( \rho_{\theta_e} (t_{v,b}^i, v) - \rho_{v}^i)^2, \end{equation*} with $X_{v,b}$ as introduced before. Here, we introduce for each $v \in \mathcal{V}_\mathcal{K}$ some additional variables $\{\rho_{v}^i\}_{i=1}^{n_b}$, that are appended to our parameter set $\theta$ and determined through the optimization process in our model.
\end{itemize}

\begin{flalign} \label{eq:loss}
    \Phi(X_{data}) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D}(X_{v,b}) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}} \big( \phi_{v,K} (X_{v,b}) + \phi_{v,D}(X_{v,b}) \big) + \\
    & \quad \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}} \big( \phi_{e,r} (X_{e,r}) + \phi_{e,0} (X_{e,0}) \big). 
\end{flalign}

\subsection{Backpropagation in FNN}

\begin{equation*}
    a^{[1]}(x,t) = \begin{pmatrix} x \\ t \end{pmatrix} \in \mathbb{R}^2
\end{equation*}

\begin{equation*}
    a^{[l]}(x,t) = \sigma^{[l]} (z^{[l]}(x,t)) = \sigma^{[l]} (W^{[l]} a^{[l-1]}(x,t) + b^{[l]}) \in \mathbb{R}^{n_l}, 
\end{equation*}

where $W^{[l]} \in \mathbb{R}^{n_l \times n_{l-1}}, \; b^{[l]} \in \mathbb{R}^{n_l}$ and $z^{[l]}(x,t) = W^{[l]} a^{[l-1]}(x,t) + b^{[l]} \in \mathbb{R}^{n_l}$ for $l = 2, \ldots, L$.

\begin{equation*}
    \rho_{\theta_e}(x, t) = a^{[L]}(x,t) = \sigma^{[L]} (W^{[L]} a^{[L-1]}(x,t) + b^{[L]}) \in \mathbb{R}^{1} 
\end{equation*}




\begin{align*}
    \partial_x \rho_{\theta_e}(x, t) & =  \partial_x a^{[L]}(x,t) = \partial_x \sigma^{[L]} (W^{[L]} a^{[L-1]}(x,t) + b^{[L]}) = \\
    & =  \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[L]} (z^{[L]}(x,t)) \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} x} z^{[L]}(x,t) = \\ 
    & =  \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[L]} (z^{[L]}(x,t)) \, \cdot \, W^{[L]} \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} x} a^{[L-1]}(x,t) = \\
    & =  \ldots = \\
    & =  \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[L]} (z^{[L]}(x,t)) \, \cdot \, W^{[L]} \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[L-1]} (z^{[L-1]}(x,t)) \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \\
    & \quad \cdot \, \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[2]} (z^{[2]}(x,t)) \, \cdot \, W^{[2]} \, \cdot \,\frac{\mathrm{d}}{\mathrm{d} x} a^{[1]}(x,t) 
\end{align*}
    


\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d} x} a^{[1]}(x,t) = \frac{\mathrm{d}}{\mathrm{d} x} \begin{pmatrix} x \\ t \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}
\end{equation*}

\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d} z} \sigma^{[l]} (z^{[l]}(x,t)) = D^{[l]} \in \mathbb{R}^{n_l \times n_l}, \quad D_{i, i}^{[l]} = {\sigma^{[l]}}^{\prime} (z_{i}^{[l]})
\end{equation*}


\begin{equation*}
    \partial_x \rho_{\theta_e}(x, t) = D^{[L]} \, \cdot \, W^{[L]} \, \cdot \, D^{[L-1]} \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \, D^{[2]} \, \cdot \, W^{[2]} \, \cdot \, \begin{pmatrix} 1 \\ 0 \end{pmatrix} \in \mathbb{R}
\end{equation*}

\begin{equation*}
    \partial_t \rho_{\theta_e}(x, t) = D^{[L]} \, \cdot \, W^{[L]} \, \cdot \, D^{[L-1]} \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \, D^{[2]} \, \cdot \, W^{[2]} \, \cdot \, \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\end{equation*}

Derivatives I need to compute:

\begin{equation*}
    \partial_{x x} \rho_{\theta_e}(x, t) = ???
\end{equation*}

\begin{equation*}
    \partial_x [f(\rho_{\theta_e}(x, t)) \partial_x V_e(x, t)] = ???
\end{equation*}

\begin{equation*}
    \partial_{w_{j, k}}  = ???
\end{equation*}

\begin{equation*}
    \partial_{w_{j, k}} [f(\rho_{\theta_e}(x, t)) \partial_x V_e(x, t)] = ???
\end{equation*}
