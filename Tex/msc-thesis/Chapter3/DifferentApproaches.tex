\chapter{Different Approaches for Solving the Drift-Diffusion Equation on Metric Graphs using PINNs}

In this chapter we present different approaches of using a physics informed neural network to solve a set of drift-diffusion equations defined in a given metric graph to given initial and boundary conditions, as described in \cref{ch1:sec2}. These approaches differ in their methodology for training the neural networks used to approximate the solution \cref{Drift-Diffusion-equation} on an individual edge $e \in \mathcal{E}$ under the given conditions. In \cref{ch3:sec1}, we discuss a approach in which the approximation problem on all edges are considered simultaneously in the learning phase. This means that for all edges we combine the deviation of the approximate solution to the drift-diffusion equation and the deviation of the approximate solution to the initial and boundary conditions in one cleverly defined cost function and focus only on solving this single resulting cost function in the learning phase. In \cref{ch3:sec2}, we discuss an approach similar to the idea in \cite{JagtapKharazmiKarniadakis:2020}, where we define a cost function for the approximation problem on each edge of the graph, optimize them one by one, store for each edge certain relevant data for the interconnection of the edges, and repeat this process several times. Thus we have a set of cost functions whose number corresponds to the number of edges, whereby the individual cost functions are minimized several times in succession in the learning phase. The two approaches thus differ in the definition of the cost functions, into which the corresponding neural network and the initial and boundary conditions are incorporated differently, and the sequence in the learning phase, simultaneously or periodically, with which the weights and biases of the corresponding neural network are modified. Furthermore, we use different types and topologies of neural networks for these two approaches in the corresponding sections. We perform numerical experiments for each combination of PINN approach and neural network, and compare the solutions with those generated by the FEM presented in \cref{ch2}. We are particularly interested in the accuracy and efficiency of the different approaches so that we can determine a superior method for solving the approximation problem. In \cref{ch3:sec3}, we compare the performance of a PINN with explicitly calculated derivatives used inside the Hamiltonian, and thus in the corresponding residual network and cost function, with that of a PINN that uses automatic differentiation for this purpose, in order to demonstrate the capabilities of \lstinline!TensorFlow!. \\
In order to compare all the results of all numerical experiments, we use the same problem setup for each experiment, i.e. the set of drift-diffusion equations is considered on the same metric graph $\Gamma$ under the same initial and boundary conditions in each experiment. The metric graph $\Gamma = (\mathcal{V}, \mathcal{E})$ is defined by the following adjacency matrix: 
\begin{equation}
    \label{NumericalExperimentGraph}
    A^{\Gamma} = 
    \begin{blockarray}{ccccccc}
        v_1 & v_2 & v_3 & v_4 & v_5 & v_6 \\
        \begin{block}{(cccccc)c}
            0 & 0 & 1 & 0 & 0 & 0 & v_1 \\
            0 & 0 & 1 & 0 & 0 & 0 & v_2 \\
            0 & 0 & 0 & 1 & 0 & 0 & v_3 \\
            0 & 0 & 0 & 0 & 1 & 1 & v_4 \\
            0 & 0 & 0 & 0 & 0 & 0 & v_5 \\
            0 & 0 & 0 & 0 & 0 & 0 & v_6 \\
        \end{block}
    \end{blockarray}
\end{equation}
The graph $\Gamma$ consists of $6$ vertices $\mathcal{V} = \{ v_i \}_{i = 1,\ldots, 6}$ and $5$ edges $\mathcal{E} = \{ e_i \}_{i = 1,\ldots, 5}$ and is of course directed since $A^{\Gamma}$ is not symmetric. The graph $\Gamma$ is illustrated in \cref{fig7}. 
\begin{figure}[H]
    \begin{center}
        \begin{tikzpicture}
            % vertices
            \node[shape=circle,draw=black] (v1) at (-2.4,1.4) {$v_1$};
            \node[shape=circle,draw=black] (v5) at (2.4,1.4) {$v_5$};
            \node[shape=circle,draw=black] (v3) at (-1,0) {$v_3$};
            \node[shape=circle,draw=black] (v4) at (1,0) {$v_4$};
            \node[shape=circle,draw=black] (v2) at (-2.4,-1.4) {$v_2$};
            \node[shape=circle,draw=black] (v6) at (2.4,-1.4) {$v_6$};
            
            % edges
            \path [->](v1) edge node[above] {$e_1$} (v3);
            \path [->](v2) edge node[below] {$e_2$} (v3);
            \path [->](v3) edge node[above] {$e_3$} (v4);
            \path [->](v4) edge node[above] {$e_4$} (v5);
            \path [->](v4) edge node[below] {$e_5$} (v6);
        \end{tikzpicture}
    \end{center}
    \caption{Will come soon \ldots}
    \label{fig7}
\end{figure}
As we can see, the vertices $v_1$, $v_2$, $v_5$ and $v_6$ are the exterior vertices, i.e. $\mathcal{V}_{\mathcal{D}} = \{v_1, v_2, v_5, v_6\}$, and the vertices $v_3$ and $v_4$ are the interior vertices, i.e. $\mathcal{V}_{\mathcal{K}} = \{v_3, v_4\}$, of the graph $\Gamma$. \\
As an approximation problem, we consider the set of drift-diffusion equations, \cref{Drift-Diffusion-equation}, on all edges $e \in \mathcal{E}$ of the graph $\Gamma = (\mathcal{V}, \mathcal{E})$, defined by \cref{NumericalExperimentGraph}, under the conditions \cref{eq:Kirchhoff_Neumann_condition}, \cref{continuous on vertices}, \cref{eq:Dirichlet_conditions} and \cref{eq:initial_conditions} and under the following assumptions:
\begin{assumption} 
    \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The time interval ends at $T = 10$ and the graph $\Gamma$ is equilateral (see \cref{metric graph equilateral}) with $\ell = 1$. 
        \item Let $\varepsilon = 0.01$ in \cref{Drift-Diffusion-equation}.
        \item The mobility in \cref{Drift-Diffusion-equation} is given by $f(\rho_e(t,x)) = \rho_e (t,x)(1-\rho_e (t,x))$.
        \item $\partial_x V_e (t,x) = 1$ for all $(t,x) \in (0, 10) \times [0,1]$ holds in \cref{Drift-Diffusion-equation} for the potential $V_e$ of each edge $e \in \mathcal{E}$. 
        \item On the exterior vertices $\mathcal{V}_{\mathcal{D}}$, we have for flux boundary conditions, \cref{eq:Dirichlet_conditions}, constant influx rates which are specified for $v_1$ by $\alpha_{v_1}(t) = 0.9$ and $\beta_{v_1}(t) = 0$, for $v_2$ by $\alpha_{v_2}(t) = 0.3$ and $\beta_{v_2}(t) = 0$, for $v_5$ by $\alpha_{v_5}(t) = 0$ and $\beta_{v_5}(t) = 0.8$ and for $v_6$ by $\alpha_{v_6}(t) = 0$ and $\beta_{v_6}(t) = 0.1$.
        \item $\rho_e(0,x) = 0$ for each edge $e \in \mathcal{E}$. 
    \end{enumerate}
\end{assumption}
The aim is to compare the performance of the above mentioned two different approaches with different types of neural networks and different topologies of the neural networks. The performance of each approach is determined by its accuracy and its efficiency. To compare the accuracy we use as ground truth the values generated by the finite volume method described in \cref{ch2} with the time discretization $N_t = 5000$ and the space discretization $N_e = 1001$ as a benchmark and measure the difference between them and the values of the trained neural network of the corresponding approach evaluated at the same grid points given by the discretization. In order to compare the efficiency, we measure on the one hand the memory usage needed to create the neural networks and train them, and on the other hand we measure the time needed to execute the implementation of each approach, considering the training and the evaluating of the values at the grid points. \\
The numerical experiments are implemented in \lstinline!Python! 3.8.8 and were run on a Lenovo ThinkPad L490, 64 bit Windows system, 1.8 Ghz Intel Core i7-8565U and 32 GB RAM.
% \lstinline!Jupyter! notebooks 6.3.0 (see \cite{Kluyver:2016}), 
% \lstinline!TensorFlow! 2.5.2.

\input{Chapter3/OneNN.tex}
\input{Chapter3/EdgeNN.tex}
\input{Chapter3/ADvsExplicit.tex}