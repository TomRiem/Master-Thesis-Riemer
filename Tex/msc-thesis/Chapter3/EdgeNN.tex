\section{One Neural Network for each edge}
\label{ch3:sec2}

In this section, we present a PINN approach to solving the set of drift-diffusion equations on a metric graph, in which a cost function is defined for each edge of the correspond graph and by redefining the misfit terms of the cost function for each edge, which are to enforce the vertex conditions, the solution of the approximation problem on the entire graph is ensured. The idea for this approach comes from \cite{JagtapKharazmiKarniadakis:2020}, in which so-called conservative physics-informed neural networks, abbreviated cPINNs, on discrete domains for non-linear conservation laws were presented. In \cite{JagtapKharazmiKarniadakis:2020} the domain on which the relevant conservation law is defined is split into several adjacent subdomains and one neural network is used to solve the conservation law in a subdomain as PINN. The conservation property is achieved on the whole domain by enforcing the flux continuity in the strong form along the intersections of these subdomains. Apart from the flux continuity condition, an average solution given by two different neural networks is also enforced at the common interface between two sub-domains. The cost function of cPINN is defined for each subdomain, which has a similar structure as the PINN cost function in each subdomain, but equipped with these two interface conditions. Consequently, one has as many cost functions as subdomains into which one has split the original domain. These cPINN cost functions are then minimized several times in succession with an optimization method, such as SGD, with respect to their learnable parameters and certain values, which are needed e.g. for the average solution at the common interface between two sub-domains, are stored. The splitting into several subdomains allows to use neural networks with different architecture for each subdomain to obtain the solution of the same underlying PDE, and also allows to choose different non-learnable hyperparameters such as the type of optimization method for minimization in the learning phase. Also there is a possibility to train the networks in parallel, i.e., simultaneously, which is very important in terms of achieving computational efficiency. The domain decomposition in the cPINN approach together with the use of an individual neural network for the different subdomains offers as an advantage the possibility for the reduction of the approximation error.




\begin{equation}
    \label{vertex funcions}
    \phi_{v}(X_{v,b}) = \begin{cases} \phi_{v,K}  \left( X_{v,b} \right) +  \phi_{v,c}  \left( X_{v,b} \right)& \text{if } v \in \mathcal{V}_{\mathcal{K}}, \\ \phi_{v,D}  \left( X_{v,b} \right) & \text{if } v \in \mathcal{V}_{\mathcal{D}}, \end{cases}
\end{equation}


We define for each edge the cost function $e = (v^{o}_e, v^{t}_e) \in \mathcal{E}$
\begin{equation}
    \label{eq:cost:2}
    \phi_{\theta_e} \left( X_{data} \right) \coloneqq \phi_{e,r}  \left( X_e \right) + \phi_{e,0}  \left( X_{e,0} \right) + \phi_{v^{o}_e}(X_{v,b}) + \phi_{v^{t}_e}(X_{v,b})
\end{equation}