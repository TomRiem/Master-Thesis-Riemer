\section{One Neural Network for each edge}
\label{ch3:sec2}


The idea for this approach comes from \cite{JagtapKharazmiKarniadakis:2020}, in which so-called conservative physics-informed neural networks, abbreviated cPINNs, on discrete domains for non-linear conservation laws were presented. In \cite{JagtapKharazmiKarniadakis:2020} the domain on which the relevant conservation law is defined is split into several adjacent subdomains and one neural network is used to solve the conservation law in a subdomain as PINN. The conservation property is achieved on the whole domain by enforcing the flux continuity in the strong form along the intersections of these subdomains. Apart from the flux continuity condition, an average solution given by two different neural networks is also enforced at the common interface between two sub-domains.


The loss function of cPINN is defined sub-domain wise, which has a similar structure as the PINN loss function in each sub-domain but is endowed with the interface conditions.

We propose a conservative physics-informed neural network (cPINN) on discrete domains for non linear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, wherePINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar non linear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node. Published by Elsevier B.V.



Fig. 1 shows the schematic of cPINN algorithm, where along with NN and PDE part, additional interface conditions are also contributing to the loss function. The interface condition includes flux continuity conditions in strong form as well as enforcing the average solution given by two NNs along the common interface. Although it is not necessary to enforce the average solution along the common interface, our numerical experiments reveal that it will drastically speed-up the convergence rate. Fig. 2 shows the schematic representation of PINN and cPINN methods. Unlike PINN, cPINN divides the domain into number of small sub-domains, in which we can employ completely different neural networks (here we refer to as sub-PINN networks) with different architecture to obtain the solution of the same underlying PDE. Such domain decomposition also offers easy parallelization of the network, which is quite important in terms of achieving computational efficiency. Another aspect of the proposed algorithm is that we can freely choose network hyper-parameters like optimization method, activation function, depth or width of the network depending on some intuitive knowledge of the solution regularity in each sub-domain. In case of smooth zones, we can use a shallow network, whereas a deep neural network can be employed in a region where a complex nature of the solution is expected.

The cPINN formulation in fact considers the flux continuity at the interfaces of each sub-domain, which is inspired by the conservation laws. The total solution in this setting is reconstructed by stitching all the solutions in each sub-domain by using the proper interface conditions. This approach can be extended to a more general case, hereafter called as Mortar PINN for joining the non-overlapping decomposed domains, where the interface conditions are not necessarily coming from the conservation laws and rather depending on the corresponding governing equation in each particular problem.

\begin{equation}
    \label{vertex funcions}
    \phi_{v}(X_{v,b}) = \begin{cases} \phi_{v,K}  \left( X_{v,b} \right) +  \phi_{v,c}  \left( X_{v,b} \right)& \text{if } v \in \mathcal{V}_{\mathcal{K}}, \\ \phi_{v,D}  \left( X_{v,b} \right) & \text{if } v \in \mathcal{V}_{\mathcal{D}}, \end{cases}
\end{equation}


We define for each edge the cost function $e = (v^{o}_e, v^{t}_e) \in \mathcal{E}$
\begin{equation}
    \label{eq:loss:2}
    \phi_{\theta_e} \left( X_{data} \right) \coloneqq \phi_{e,r}  \left( X_e \right) + \phi_{e,0}  \left( X_{e,0} \right) + \phi_{v^{o}_e}(X_{v,b}) + \phi_{v^{t}_e}(X_{v,b})
\end{equation}