\section{One Neural Network for each edge}
\label{ch3:sec2}

In this section, we present a PINN approach for solving the set of drift-diffusion equations on a metric graph, where a cost function for the approximation of $\rho_e \colon (0,T) \times [0, \ell_e] \to [0, 1]$ is defined for each edge $e \in \mathcal{E}$ of the graph $\Gamma = (\mathcal{V}, \mathcal{E})$, where the included misfit terms are defined in such a way that the solution of the approximation problem on the entire graph is ensured. The idea for this approach is adopted from \cite{JagtapKharazmiKarniadakis:2020}, in which so-called conservative physics-informed neural networks, abbreviated cPINNs, on discrete domains for non-linear conservation laws were presented. In \cite{JagtapKharazmiKarniadakis:2020} the domain on which the relevant conservation law is defined is split into several adjacent subdomains and one neural network is used as a PINN to solve the conservation law in one subdomain. The conservation property is achieved on the whole domain by enforcing the flux continuity in the strong form along the intersections of these subdomains. Apart from the flux continuity condition, an average solution given by two different neural networks is also enforced at the common interface between two sub-domains. The cost function of cPINN is defined for each subdomain, which has a similar structure as the PINN cost function in each subdomain, but these two interface conditions are incorporated by misfit terms. Consequently, one has as many cost functions as subdomains into which one has split the original domain. These cPINN cost functions are then minimized several times in succession with an optimization method, such as SGD, with respect to their learnable parameters dependent on the corresponding subdomain, and certain values, which are needed e.g. for the average solution at the common interface between two sub-domains, are stored. The splitting into several subdomains allows to use neural networks with different architectures for each subdomain to obtain the solution of the same underlying PDE, and also allows to choose different non-learnable hyperparameters such as the type of optimization method for the minimization in the learning phase. Also there is a possibility to train the networks in parallel, i.e., simultaneously, which is very important in terms of achieving computational efficiency. The domain decomposition in the cPINN approach together with the use of an individual neural network for the different subdomains offers as an advantage the possibility for the reduction of the approximation error. \\
We adopt for our approach this idea of splitting the domain into several subdomains and receiving an individual cost function for each subdomain. For this purpose, we associate the individual subdomains with the individual edges of the considered graph, i.e. we construct now a cost function for each edge of the considered graph. These cost functions for all edges are minimized several times in succession, always training only the learnable parameters that are needed to approximate the solution on exactly this edge.\\
We now construct the cost function for the approximation of the solution of \cref{Drift-Diffusion-equation} on a single edge, which we denote by $\hat{e} \in \mathcal{E}$ to distinguish it more easily from the other edges $e \in \mathcal{E} \setminus \{ \hat{e}\}$. In the following, we denote the cost function for an individual edge by $\Phi_{\theta_{\hat{e}}}$, where $\theta_{\hat{e}}$ denotes the learnable parameters which are used for the approximation of $\rho_{\hat{e}}$. Like the cost functions given by \cref{eq:loss:1} and \cref{MSE PINN}, the cost function $\Phi_{\theta_{\hat{e}}}$ consists also of several misfit terms. \\
Since we require that the corresponding neural network approximates the solution of the drift-diffusion equation on the edge e, therefore we incorporate the mean squared error of the residual network given by \cref{Drift-Diffusion residual network}

the term given by Equation 3 into our cost function.  

Two misfit terms always appear the same, term 1 and term 2, where we can simply incorporate equation 1 and equation 2 into the cost function here. 

Each edge is defined by two vertices which it connects and by which it is linked to other edges. When we construct a cost function for an individual edge, both vertices must be checked to see if they are either an interior or an exterior vertex, since it follows that different conditions apply and thus different terms are incorporated into the cost function. 

Fortunately, we can incorporate two terms from Section 2, the term for the residual network and the term for the initial condition. This is because these terms are defined for each edge individually and the average of these terms over all edges is incorporated in the final cost function. Here we do not incorporate the average of all edges into the cost function, but only the term for one edge. This is to ensure that the trained neural network approximates the solution of the equation on this edge and satisfies the initial conditions.  \\

What the interface conditions are in \cite{JagtapKharazmiKarniadakis:2020} are here the vertex conditions that couple the edges at a vertex, i.e., \cref{eq:Kirchhoff_Neumann_condition} and \cref{continuous on vertices} on the interior vertices $\mathcal{V}_\mathcal{K}$ and \cref{eq:Dirichlet_conditions} on the exterior vertices $\mathcal{V}_\mathcal{D}$. 

Each edge is defined by two vertices which it connects and by which it is linked to other edges. When we construct a cost function for an individual edge, both vertices must be checked to see if they are either an interior or an exterior vertex, since it follows that different conditions apply and thus different terms are incorporated into the cost function. 

To enforce the Kirchhoff-Neumann condition on an interior vertex $v \in \mathcal{V}_\mathcal{K}$, \cref{eq:Kirchhoff_Neumann_condition}, we use the misfit term given by \cref{misfit:Kirchhoff}, but we rewrite it, so that it is apparent that only the learnable parameters, which are necessary for the approximation of the solution on the corresponding edge, are trained. The misfit term for an interior vertex $v \in \mathcal{V}_\mathcal{K}$ is given by
\begin{equation} 
    \label{misfit:Kirchhoff.}
    \phi_{v,K}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b}  \left( \sum_{e\in \mathcal{E}_v} J_e(t,v) n_e (v) \right)^2, 
\end{equation} 
where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset \left( 0,T \right)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced. We note that the derivative is taken into the outgoing direction. \\

When a cost function is trained for one edge, in the optimization procedure the learnable parameters of the networks of the other edges are fixed



If we set up a cost function for an individual edge, then we have to check what these vertices are, because consequently different conditions apply and are included in the cost function. 
\begin{equation}
    \label{vertex funcions}
    \phi_{v}(X_{v,b}) = \begin{cases} \phi_{v,K}  \left( X_{v,b} \right) +  \phi_{v,c}  \left( X_{v,b} \right)& \text{if } v \in \mathcal{V}_{\mathcal{K}}, \\ \phi_{v,D}  \left( X_{v,b} \right) & \text{if } v \in \mathcal{V}_{\mathcal{D}}, \end{cases}
\end{equation}


We define for each edge the cost function $e = (v^{o}_e, v^{t}_e) \in \mathcal{E}$
\begin{equation}
    \label{eq:cost:2}
    \phi_{\theta_e} \left( X_{data} \right) \coloneqq \phi_{e,r}  \left( X_e \right) + \phi_{e,0}  \left( X_{e,0} \right) + \phi_{v^{o}_e}(X_{v,b}) + \phi_{v^{t}_e}(X_{v,b})
\end{equation}