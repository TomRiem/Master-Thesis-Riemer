\section{One Neural Network for all edges}
\label{ch3:sec1}

In this section we construct, in some sense, a global cost function, which is used to train the weights and biases of a neural network by minimizing it in the learning phase, and we perform numerical tests with three different types of neural networks used in this $PINN$ approach. In the following we denote this cost function by $\Phi_\theta$, where $\theta$ only refers to the set of trainable parameters of the used neural network and does not obtain any information about the used type of neural network or the topology of the used neural network. The idea of this approach in this section is that in the learning phase the drift-diffusion equations on all edges and all initial and vertex conditions are considered at once such that all edges are seen as being merged together and thus the problem of approximating the solution of the set of drift-diffusion equations on a metric graph is treated as a whole. Therefore, we construct only one cost function $\Phi_\theta$, which consists of several misfit terms, as in \cref{MSE Pinn}, by incorporating the mean-squared-error of a residual network as well as a number of misfit terms which enforce the initial and vertex conditions at a set of collocation points. Of course, we want to exploit the special structure of the domain given by the metric graph for the construction of this cost function $\Phi_\theta$. We note that we set up this cost function $\Phi_\theta$ for the general case and we do not refer for the moment to the values given by the metric graph defined for the numerical experiments by \cref{NumericalExperimentGraph} or by the \cref{graph_assumptions}. \\

We start with the definition of the residual network, which returns the error of the neural network with respect to the fulfillment of \cref{Drift-Diffusion-equation} in the cost function $\Phi_\theta$ at a collocation point. For that, of course, we need a surrogate network, which we denote by $\rho_{\theta_e} \colon \mathbb{R}^2 \to \mathbb{R}$, and which is supposed to approximate the solution $\rho_e$ of \cref{Drift-Diffusion-equation} on an individual edge $e \in \mathcal{E}$. We note that although the surrogate network $\rho_{\theta_e}$ is defined over the entire space $R^2$, it can be assumed that only values from $\left( 0, T \right) \times [0, \ell_e]$ are used, since this is the definition domain of the solution $\rho_e$ to be approximated. Here $\theta_e$ denotes the trainable parameters that are involved in the approximation of $\rho_e$ on an individual edge $e \in \mathcal{E}$, and $\theta_e$ also contains no information about the used type of neural network or the topology of the used neural network. If we insert the surrogate network $\rho_{\theta_e}$ into the right-hand side of \cref{eq:Hamiltonian}, we obtain as a residual network for each individual edge $e \in \mathcal{E}$
\begin{equation}
    \label{Drift-Diffusion residual network}
    r_{\theta_e} \left( t,x \right)=\partial_t \rho_{\theta_e} \left( t,x \right) - \partial_x   \left(  \varepsilon \partial_x  \rho_{\theta_e} \left( t,x \right) - f \left( \rho_{\theta_e} \left( t,x \right) \right) \partial_x V \left( t,x \right) \right).
\end{equation}
Using \cref{Drift-Diffusion residual network} we enforce the structured information imposed by \cref{Drift-Diffusion-equation} for each individual edge $e \in \mathcal{E}$ via
\begin{equation} 
    \label{misfit:residual}
    \phi_{e,r}  \left( X_e \right) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} r_{\theta_e}  \left( x_e^i, t_e^i \right)^2,
\end{equation} 
where $X_e = \{ \left( t_e^i, x_e^i \right)\}_{i=1}^{n_e} \subset \left( 0, T \right) \times [0, \ell_e]$ is a set of time-space collocation points that are drawn randomly or chosen equidistantly. \\
To enforce the Kirchhoff-Neumann conditions, \cref{eq:Kirchhoff_Neumann_condition}, we define the following misfit term for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ 
\begin{equation} 
    \label{misfit:Kirchhoff}
    \phi_{v,K}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b}  \left( \sum_{e \in \mathcal{E}_v}  \left( - \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f \left( \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) \partial_x V_e \left( t_{v,b}^i, v \right) \right) \, n_e  \left( v \right) \right)^2, 
\end{equation} 
where $X_{v,b} = \left\{ t_{v,b}^i \right\}_{i=1}^{n_b} \subset \left( 0,T \right)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced. The values $\left\{ \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right\}_{i=1}^{n_b}$ are either equal to $\left\{ \rho_{\theta_e}  \left( t_{v,b}^i, 0 \right) \right\}_{i=1}^{n_b}$ if the interior vertex $v \in \mathcal{V}_\mathcal{K}$ is an origin vertex of the edge $e$ (i.e. $\operatorname{o}(e) = v$), or equal to $\left\{ \rho_{\theta_e}  \left( t_{v,b}^i, \ell_e \right) \right\}_{i=1}^{n_b}$ if $v$ is a terminal vertex of the edge $e$ (i.e. $\operatorname{t}(e) = v$). Of course this also applies to the values $\left\{ \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right\}_{i=1}^{n_b}$. We note that the derivatives are taken into the outgoing direction. \\
In order to enforce the continuity in the interior vertices $\mathcal{V}_\mathcal{K}$, required by \cref{continuous on vertices}, we introduce two different misfit terms to accomplish this. In the numerical experiments, we draw attention to which of the two misfit terms is used. The first misfit term is defined for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ by 
\begin{equation} 
    \label{misfit:continuity}
    \phi_{v,c}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{e \in \mathcal{E}_v} \sum_{i=1}^{n_b} \left(  \rho_{\theta_e}  \left( t_{v,b}^i, v \right) - \rho_{v}^i \right)^2,
\end{equation} 
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. Here, we introduce for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ some additional trainable parameters $\left\{ \rho_{v}^i \right\}_{i=1}^{n_b}$, that are appended to $\theta$ and which are also trained by minimizing the resulting cost function $\Phi_\theta$. The other misfit term is defined for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ by 
\begin{equation} 
    \label{misfit:continuity:average}
    \phi_{v,c}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b}  \sum_{i=1}^{n_b} \left( \sum_{e \in \mathcal{E}_v} \left( \rho_{\theta_e}  \left( t_{v,b}^i, v \right) - \frac{1}{\abs{\mathcal{E}_v}} \sum_{e \in \mathcal{E}_v} \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) \right)^2.
\end{equation}
Here, on average over all time collocation points $\left\{ t_{v,b}^i \right\}_{i=1}^{n_b}$, the value $\rho_{\theta_e}  \left( t_{v,b}^i, v \right)$ of each edge $e \in \mathcal{E}_v$ should be equal to the average of all edges connected to this interior vertex $v \in \mathcal{V}_\mathcal{K}$. Both misfit terms have their numerical advantages and disadvantages. The misfit term defined by \cref{misfit:continuity} is not so complex, from which can follow that the derivatives with respect to the trainable parameters $\theta$ in the used optimization method is easier to compute, but therefore adds \cref{misfit:continuity} just as much trainable parameters as collocation points to the set of trainable parameters. No further trainable parameters need to be added for the misfit term given by \cref{misfit:continuity:average}, but it is more complex for that. \\
We enforce the flux vertex conditions given by \cref{eq:Dirichlet_conditions} for each exterior vertex $v \in \mathcal{V}_\mathcal{D}$ by defining the following misfit term  
\begin{equation}
    \label{misfit:Dirichlet}
    \begin{aligned} 
        \phi_{v,D}  \left( X_{v,b} \right) \coloneqq & \frac{1}{n_b} \sum_{i=1}^{n_b} \bigg( \sum_{e \in \mathcal{E}_v} \left(- \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f\left(\rho_{\theta_e}  \left( t_{v,b}^i, v \right)\right) \partial_x V_e\left( t_{v,b}^i, v \right) \right) n_e  \left( v \right) + \\
        & \alpha_v \left( t_{v,b}^i \right)  \left( 1- \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) - \beta_v \left( t_{v,b}^i \right) \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \bigg)^2
    \end{aligned}
\end{equation}
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. \\
To enforce the initial conditions for each edge $e \in \mathcal{E}$, \cref{eq:initial_conditions}, we define the following misfit term  
\begin{equation} 
    \label{misfit:initial}
    \phi_{e,0}  \left( X_{e,0} \right) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0}  \left( \rho_{\theta_e}  \left( 0,x_{e,0}^i \right) - \rho_{e,0} \left( x_{e,0}^i \right) \right)^2, 
\end{equation} 
where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of collocation points along $t=0$. \\ 
Now we combine all misfit terms defined above to form the cost function $\Phi_\theta$ by adding the different mean values over the units in which the concerned misfit term is applicable: 
\begin{equation}
    \label{eq:loss:1}
    \begin{aligned} 
        \Phi_{\theta} \left( X_{data} \right) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D} \left( X_{v,b} \right) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}}  \left(  \phi_{v,K}  \left( X_{v,b} \right) + \phi_{v,c} \left( X_{v,b} \right)  \right) + \\
        & \quad + \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}}  \left(  \phi_{e,r}  \left( X_{e,r} \right) + \phi_{e,0}  \left( X_{e,0} \right)  \right), 
    \end{aligned}
\end{equation}
where $X_{data}$ represents the union of the different collocation points $X_e$, $X_{v,b}$ and $X_{e,0}$. In the learning phase, this cost function $\Phi_{\theta}$ is minimized with respect to the trainable parameters $\theta$, in the hope that the resulting trained surrogate network $\rho_{\theta_e}$ for each edge $e \in \mathcal{E}$ will approximate the solution of the drift diffusion equation given by \cref{Drift-Diffusion-equation} well to a certain degree under the given initial and vertex conditions. 

So far we have not specified what type of neural network with which topology we use for $\rho_{\theta_e}$, i.e. to approximate the solution of \cref{Drift-Diffusion-equation} on an individual edge $e \in \mathcal{E}$ under the given initial and vertex conditions. Of course, we have an infinite freedom of choice, the only constraints being the dimension of the input $(x,t) \in \mathbb{R}^2$, the dimension of the output $\rho_{\theta_e}(t, x) \in \mathbb{R}$ and the differentiability of the corresponding neural network up to order two (due to $\partial_{xx} \rho_{\theta_e}$ appears in \cref{Drift-Diffusion-equation}). We now introduce three different types of neural network for the approximation of $\rho_e \colon (0, T) \times [0, \ell_e] \to \mathbb{R}$, which we use in the numerical experiments. \\
Since we have to find an approximation $\rho_{\theta_e}$ for each edge $e \in \mathcal{E}$ and since these approximations $\left\{ \rho_{\theta_e} \right\}_{e \in \mathcal{E}}$ are individually included in the cost function $\Phi_{\theta}$, it is quite evident to use one single neural network for the approximation of the solution of \cref{Drift-Diffusion-equation} on one individual edge $e$. This results in the fact that we use as many neural networks as we have edges of the graph $\Gamma$. In this case, $\theta_e$ describes the trainable parameters of a single neural network and $\theta$ is the union of the trainable parameters of all neural networks. Thus, during the learning phase, all trainable parameters $\theta$ are modified when the cost function $\Phi_{\theta}$ is minimized, and therefore consider the used optimization methods $\theta$ as the optimization variable. Using one neural network for the approximation of the solution on one edge offers the possibility to select individual hyperparameters like the topology or the activation functions for the different neural networks. One can use a deep neural network for an edge for which the solution can have a complex structure, while a shallow neural network can be used on the edges with relatively simple and smooth solutions. In this work, however, we will not cover such a case due to the effort of finding the best hyperparameters for each neural network, we just use the same type of neural network with the same topology for the approximation of $\rho_e$ each edge $e \in \mathcal{E}$. \\

% Noch die richtigen Hyperparameter hinzufÃ¼gen
We adopt this approach of using a single neural network to approximate $\rho_e$ on a single edge $e \in \mathcal{E}$ for the numerical experiments and we use two different types of neural networks for that. For the first numerical experiment, we use for $\rho_{\theta_e}$ a feed-forward neural network, denoted by $\operatorname{fnn}_{\theta_e}$, with $L$-layers for each edge $e \in \mathcal{E}$, which is defined by 
\begin{equation} 
    \label{one_for_each}
    \begin{gathered}
        \operatorname{fnn}_{\theta_e} \colon \mathbb{R}^2 \to \mathbb{R}, \\
        \\
        \operatorname{fnn}_{\theta_e}(x, t) = \sigma_L(W^L \sigma_{L-1}(W^{L-1}\sigma_{L-2}(\cdots \sigma_{1}(W^{1}x^0 +b^1) \cdots) + b^{L-1}) + b^{L}),
    \end{gathered} 
\end{equation} 
where $x^0 = (t, x)^{\mathrm{T}} \in (0, T) \times [0, \ell_e] \subset \mathbb{R}^2$ and $\theta_e = \left\{ \left\{ W^l \right\}_{l = 1, \ldots, L}, \left\{ b^l \right\}_{l = 1, \ldots, L} \right\}$ with $W^l \in \mathbb{R}^{n_l \times n_{l-1}}$ and $b^l \in \mathbb{R}^{n_l}$, where $n_0 = 2$ and $n_L = 1$. The total set of all trainable parameters for the resulting $PINN$ approach is therefore $\theta = \bigcup_{e \in \mathcal{E}} \ \theta_e$. Feed-forward neural networks have already been used in various $PINN$ setups, so this choice can be considered as reasonable.  \\
For the second numerical experiment, we use for $\rho_{\theta_e}$ a neural network, which is defined by the following architecture and the following forward propagation
\begin{equation} 
    \label{Resnet1}
    \begin{gathered}
        R_{\theta_e} \colon \mathbb{R}^2 \to \mathbb{R}, \\
        \\
        R_{\theta_e}(x^0) = \frac{1}{2} {x^0}^{\mathrm{T}} A x^0 + {x^{L}}^{\mathrm{T}} w + c^{\mathrm{T}} x^0,
    \end{gathered} 
\end{equation} 
with
\begin{equation}
    \label{Resnet2} 
    \begin{gathered}
        x^1 = \sigma_1(W^1 x^{0} + b^1) \in \mathbb{R}^m, \\
        \\
        x^l = x^{l-1} + h \, \sigma_l(W^l x^{l-1} + b^l) \in \mathbb{R}^m \quad \text{for} \quad l = 2, \ldots, L, 
    \end{gathered} 
\end{equation} 
where $h > 0$ is called stepsize and is a hyperparameter, $x^0 = (t, x)^{\mathrm{T}} \in (0, T) \times [0, \ell_e] \subset \mathbb{R}^2$, $W^1 \in \mathbb{R}^{m \times 2}$ and $W^l \in \mathbb{R}^{m \times m}$ for $l = 2, \ldots, L$, $b^l \in \mathbb{R}^{m}$ for $l = 1, \ldots, L$ and $A \in \mathbb{R}^{2 \times 2}$, $w \in \mathbb{R}^m$, $c \in \mathbb{R}^2$ are also trainable parameters, i.e. $\theta_e = \left\{ \left\{ W^l \right\}_{l = 1, \ldots, L}, \left\{ b^l \right\}_{l = 1, \ldots, L}, A, w, c \right\}$ and $\theta = \bigcup_{e \in \mathcal{E}} \ \theta_e$. The idea of using this type of neural network originates from \cite{RuthottoOsherLiNurbekyanFung2020}, in which high-dimensional mean field games and mean field control models are approximately solved by combining Lagrangian and Eulerian viewpoints and by using a tailored neural network parameterization for the potential of the solution, which can be understood as a density, to avoid any spatial discretisation. The neural network given by the forward propagation in \cref{Resnet2} is also called residual network and abbreviated with $ResNet$, \cite{HeZhangRenSun:2015}. We refer to it only as $ResNet$ to avoid confusion with the residual network of a $PINN$. $ResNet$s have been successful in a wide range of machine learning tasks and have been found to be trainable even for many layers. By interpreting \cref{Resnet2} as a forward Euler discretization of an initial value problem, the continuous limit of a $ResNet$ is amenable to mathematical analysis, \cite[p.~6]{RuthottoOsherLiNurbekyanFung2020}. \\

For a further numerical experiment we use a completely different approach of utilizing a neural network to approximate the solutions of the set of drift-diffusion equations. We use one single feed-forward neural network with $L \in \mathbb{N}$ layers for the approximation of the solution of \cref{Drift-Diffusion-equation} under the given initial and vertex conditions on all edges of the graph $\Gamma$, i.e. this network returns for a point $(t,x) \in \mathbb{R}^2$ the values $\rho_{\theta_{e_i}}(x, t)$ for all edges $\mathcal{E} = \left\{ e_i \right\}_{i = 1, \ldots, E}$, where $E = \abs{\mathcal{E}}$. We define this neural network as follows
\begin{equation} 
    \label{one_for_all}
    \begin{gathered}
        \operatorname{FNN}_{\theta} \colon \mathbb{R}^2 \to \mathbb{R}^E \\
        \\
        \operatorname{FNN}_{\theta}(x, t) = \sigma_L(W^L \sigma_{L-1}(W^{L-1}\sigma_{L-2}(\cdots \sigma_{1}(W^{1}x^0 +b^1) \cdots) + b^{L-1}) + b^{L}) \in \mathbb{R}^E, 
    \end{gathered} 
\end{equation} 
where $x^0 = (t, x)^{\mathrm{T}} \in (0, T) \times [0, \ell_e] \subset \mathbb{R}^2$ and $\theta = \left\{ \left\{ W^l \right\}_{l = 1, \ldots, L}, \left\{ b^l \right\}_{l = 1, \ldots, L} \right\}$ with $W^l \in \mathbb{R}^{n_l \times n_{l-1}}$ and $b^l \in \mathbb{R}^{n_l}$, where $n_0 = 2$ and $n_L = E$. The individual approximations are given by 
\begin{equation*}
    \rho_{\theta_{e_i}}(x, t) = \left[ \operatorname{FNN}_{\theta}(x, t) \right]_i \in \mathbb{R},
\end{equation*}
i.e. the $i$-th entry of the networks output $\operatorname{FNN}_{\theta}(x, t) \in \mathbb{R}^E$ is the approximation of the solution on the $i$-th edge. We see that in this case $\theta$ consists only of the weights and biases of this single network, as indicated by the index $\theta$ in $\operatorname{FNN}_{\theta}$. \\
The use of a single neural network is motivated by the hope that the neurons in the hidden layers will learn the structure of the entire graph and the resulting communication of the edges via the vertices, i.e. that all interactions within the graph are taken into account by the neurons after the learning phase. Furthermore, we hope that the computational cost is reduced, since only the weights and biases of this single network need to be trained, provided the depth and width of the neural network $\operatorname{FNN}_{\theta}$ are not too large. The fact that $\operatorname{FNN}_{\theta}(x, t)$ generates the output for all edges at the same time is also an advantage, as in an implementation only the execution of one network is necessary instead of several. However, we point out that this approach only makes sense if the graph is equilateral, see \cref{graph_assumptions}, because then the same collocation points $X_e = \{ \left( t_e^i, x_e^i \right)\}_{i=1}^{n_e}$ and $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0}$ can be used for the misfit terms given by \cref{misfit:residual} and \cref{misfit:initial} for all edges $e \in \mathcal{E}$. 


With the cost function defined by \cref{eq:loss:1} and the three different types of neural networks defined above, we perform numerical experiments in which these neural networks are trained by this cost function for the approximation problem, which is given by the set of drift-diffusion equations given by \cref{Drift-Diffusion-equation} on the metric graph defined by \cref{NumericalExperimentGraph} under the initial and vertex conditions and the given \cref{graph_assumptions}. The time required for the learning phase is measured and then the deviation of the values generated by these neural networks at the grid points from the values generated by the FVM at these grid points is measured. \\
As mentioned before, these are implemented with \lstinline!Python 3.8.8!. All collocation points listed after that are randomly generated using \lstinline!tf.random.uniform!, which is provided by the package \lstinline!Tensorflow! and uses an normal distribution. Further we set \lstinline!dtype = 'float64'!. As time-space collocation points $X_e$ for \cref{misfit:residual} we use $n_e = 4000$ randomly selected points $\left\{ \left( t_e^i, x_e^i \right) \right\}_{i=1}^{n_e} \subset \left( 0, T \right) \times [0, \ell_e]$. As space collocation points $X_{e,0}$ for \cref{misfit:initial} we use $n_0 = 1000$ randomly selected points $ \left\{ x_{e,0}^i \right\}_{i=1}^{n_0} \subset [0, \ell_e]$. For \cref{misfit:Kirchhoff}, \cref{misfit:Dirichlet}, \cref{misfit:continuity} or \cref{misfit:continuity:average} we use $n_b = 1000$ randomly chosen points $\left\{ t_{0,b}^i \right\}_{i=1}^{n_b} \subset \left( 0,T \right)$ as time snapshots $X_{0,b}$ for origin vertices, i.e. $v = 0$, and $n_b = 1000$ randomly chosen points as time snapshots $X_{\ell,b}$ for terminal vertices, i.e. $v = 0$ (we recall that the graph $\Gamma$ is equilateral).











% Kollokationspunkte 

We note that, as mentioned in \cref{ch1:sec4}, all derivatives of surrogate models with respect to the input parameters are computed with $AD$ provided by \lstinline!Tensorflow! via the method \lstinline!tf.GradientTape!. We use 


For the minimization of the cost function $\Phi_{\theta}$, we first use a variant of the gradient descent method, in which the direction is given by the Adam optimizer from the module \lstinline!Keras!, see \cite{Chollet:2015}, which belongs to the package \lstinline!Tensorflow!, see \cite{TensorFlow}. We change for the different types of neural networks the input parameter \lstinline!learning_rate!, which obviously specifies the learning rate of the method, and leave all other input parameters of the Adam optimizer by default. This method stops after 2000 iterations. After that, we use the L-BFGS-B optimizer from the package \lstinline!SciPy!, see \cite{SciPy:2020}, and we set for all networks as parameters in the options \lstinline!maxiter = 5000! which specifies the maximum number of iterations, \lstinline!maxfun = 50000! which specifies the maximum number of function evaluations, \lstinline!maxcor = 50! which specifies the maximum number of stored variable metric corrections used in the L-BFGS approximation of the Hessian of the cost function $\Phi_{\theta}$, \lstinline!maxls = 50! which specifies the maximum number of line search steps per iteration and \lstinline!ftol = 1.0*np.finfo(float).eps! which specifies a parameter in a stopping criterion that aborts the method if the values of the cost function $\Phi_{\theta}$ between two iterates are too small.


\begin{table}[H]\label{tab:one_cost_function}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l l }
            \toprule
            Neural Network given by & One each egde & $ResNet$ & One  \\ 
            \midrule
            Error & $0.15$ & $0.21$ & $0.54$ \\ 
            \midrule
            Time in seconds & $0.15$ & $0.21$ & $0.54$  \\ 
            \midrule
            Number of Iterations & $72$ & $68$ & $72$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of different types of neural networks.}
\end{table}