\section{One Neural Network for all edges}
\label{ch3:sec1}

In this section we construct a, in some sense, global cost function, which considers the drift-diffusion equations on all edges and all initial and boundary conditions at once and thereby considers the edges as interconnected and the problem on the graph as a total. In the following we denote the cost function by $\phi_\theta$, where $\theta$ only refers to the learnable parameters of the used neural network and does not obtain any information about the used type of neural network or the topology of the used neural network. The cost function to be constructed $\phi_\theta$ consists of several parts, as in \cref{MSE PINN}, by incorporating the mean squared error of a residual network at a set of collocation points as well as a number of misfit terms which enforce both the boundary and initial conditions weakly. Of course, we want to exploit the special structure of the domain given by the metric graph for the construction of the cost function in thus also in the PINN. \\
Let's start with the easy part first, the definition of the residual network, which returns us a residual in the cost function $\phi_\theta$ for the network to be learned. From now on, we will denote the surrogate network by $\rho_{\theta_e}$, which is supposed to approximate the solution $\rho_e$ of \cref{eq:Hamiltonian} on an individual edge $e \in \mathcal{E}$. Of course, $\rho_{\theta_e}$ receives the same input values $(t, x) \in (0, T) \times e$ as $\rho_e$ and also returns a real-valued number. Here $\theta_e$ denotes the set of all weights and biases that depend on the approximation of $\rho_e$ on an individual edge $e \in \mathcal{E}$, and $\theta_e$ also contains no information about the used type of neural network or the topology of the used neural network. If we insert the surrogate network $\rho_{\theta_e}(t,x)$ into the right-hand side of \cref{eq:Hamiltonian}, we obtain as a residual network for each edge $e \in \mathcal{E}$
\begin{equation}
    \label{Drift-Diffusion residual network}
    r_{\theta_e}(t,x)=\partial_t \rho_{\theta_e}(t,x) - \partial_x  ( \varepsilon \partial_x  \rho_{\theta_e}(t,x) - f(\rho_{\theta_e}(t,x)) \partial_x V(t,x)).
\end{equation}
From \cref{Drift-Diffusion residual network} we form the residual misfit term for each edge $e \in \mathcal{E}$ of the cost function via
\begin{equation*} 
    \phi_{e,r} (X_e) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} (r_{\theta_e} (X_e^i))^2
\end{equation*} 
where $r_{\theta_e} (X_e^i) = r_{\theta_e} (x_e^i, t_e^i)$ for $X_e = \{X_e^i\}_{i=1}^{n_e} = \{(t_e^i, x_e^i)\}_{i=1}^{n_e} \subset (0, T) \times e$, which is a set of time-space collocation points that are drawn randomly or chosen equidistantly. \\
To enforce the Kirchhoff-Neumann conditions, \cref{eq:Kirchhoff_Neumann_condition}, we introduce the following misfit term for each vertex $v \in \mathcal{V}_\mathcal{K}$ 
\begin{equation*} 
    \phi_{v,K} (X_{v,b}) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} (\sum_{e \in \mathcal{E}_v} (- \varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v))^2, 
\end{equation*} 
where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset (0,T)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced. We note that the derivative is taken into the outgoing direction. \\
We introduce a misfit term enforcing the continuity in the interior vertices $v \in \mathcal{V}_\mathcal{K}$ 
\begin{equation*} 
    \phi_{v,c} (X_{v,b}) \coloneqq \frac{1}{\abs{\mathcal{E}_v}} \sum_{e \in \mathcal{E}_v} \frac{1}{n_b} \sum_{i=1}^{n_b} ( \rho_{\theta_e} (t_{v,b}^i, v) - \rho_{v}^i)^2,
\end{equation*} 
with $X_{v,b}$ as introduced before. Here, we introduce for each $v \in \mathcal{V}_\mathcal{K}$ some additional learnable variables $\{\rho_{v}^i\}_{i=1}^{n_b}$, that are appended to our parameter set $\theta$ and determined through the optimization process in our model. \\
a misfit term enforcing the flux boundary conditions for each vertex $v \in \mathcal{V}_\mathcal{D}$ 
\begin{align*} 
    \phi_{v,D} (X_{v,b}) & \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} ( \sum_{e\in \mathcal{E}_v} (-\varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v) + \\ & \quad + \alpha_v(t_{v,b}^i) (1-\rho_v^i) - \beta_v(t_{v,b}^i) \rho_v^i)^2 
\end{align*}
a misfit term enforcing the initial conditions for each edge $e \in \mathcal{E}$ 
\begin{equation*} 
    \phi_{e,0} (X_{e,0}) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0} (\rho_{\theta_e} (0,x_{e,0}^i) - \rho_{e,0}(x_{e,0}^i))^2, 
\end{equation*} 
where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of boundary points along $t=0$ and $\rho_{e,0}$ \ldots

\begin{align*} \label{eq:loss}
    \Phi(X_{data}) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D}(X_{v,b}) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}} \big( \phi_{v,K} (X_{v,b}) + \phi_{v,c}(X_{v,b}) \big) + \\
    & \quad + \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}} \big( \phi_{e,r} (X_{e,r}) + \phi_{e,0} (X_{e,0}) \big). 
\end{align*}



The model is then trained based on the minimization of a loss function $\phi_\theta$ that incorporates the residual at a set of collocation points as well as a number of misfit terms which enforce both the boundary and initial conditions weakly.

As the domain in the metric graph case is more structured then a standard two or three-dimensional domain we want to use the network structure in the setup of the PINN on networks.
To be precise we assume that we identify a feed-forward neural network $\rho_{\theta_e}$ of the form  with the solution on an individual edge $e \in \mathcal{E}$ where we have $\theta_e$ as the weights associated with the solution of the drift-diffusion equation defined on edge $e \in \mathcal{E}$.

Having introduced our model, we obtain now the loss function $\phi(X_{Data})$ consisting of

\begin{itemize}
    \item residual loss term for each edge $e \in \mathcal{E}$ \begin{equation*} \phi_{e,r} (X_e) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} (r_{\theta_e} (X_e^i))^2 \end{equation*} where \begin{equation*} r_{\theta_e} (X_e^i) = r_{\theta_e} (x_e^i, t_e^i) = \partial_t \rho_{\theta_{e}} (x_{e}^i , t_{e}^i) - \varepsilon \partial_{xx} \rho_{\theta_e}(x_e^i, t_e^i) - \partial_x [f(\rho_{\theta_e}(x_e^i, t_e^i)) \partial_x V_e(x_e^i, t_e^i)] \end{equation*} for $X_e = \{X_e^i\}_{i=1}^{n_e} = \{(t_e^i, x_e^i)\}_{i=1}^{n_e} \subset [0,\ell_e] \times (0, T)$.
    \item a misfit term enforcing the initial conditions for each edge $e \in \mathcal{E}$ \begin{equation*} \phi_{e,0} (X_{e,0}) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0} (\rho_{\theta_e} (0,x_{e,0}^i) - \rho_{e,0}(x_{e,0}^i))^2, \end{equation*} where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of boundary points along $t=0$ and $\rho_{e,0}$ \ldots
    \item a misfit term enforcing the Kirchhoff-Neumann conditions for each vertex $v \in \mathcal{V}_\mathcal{K}$ \begin{equation*} \phi_{v,K} (X_{v,b}) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} (\sum_{e \in \mathcal{E}_v} (- \varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v))^2, \end{equation*} where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset (0,T)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced, $\mathcal{E}_v$ is the set of edges incident to node $v$ and the derivative is taken into the outgoing direction,
    \item a misfit term enforcing the flux boundary conditions for each vertex $v \in \mathcal{V}_\mathcal{D}$ \begin{align*} \phi_{v,D} (X_{v,b}) & \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b} ( \sum_{e\in \mathcal{E}_v} (-\varepsilon \partial_x \rho_{\theta_e} (t_{v,b}^i, v) + f(\rho_{\theta_e} (t_{v,b}^i, v)) \partial_x V_e(t_{v,b}^i, v)) \, n_e (v) + \\ & \quad + \alpha_v(t_{v,b}^i) (1-\rho_v^i) - \beta_v(t_{v,b}^i) \rho_v^i)^2 \end{align*}
    \item a misfit term enforcing the continuity in the vertices $v \in \mathcal{V}_\mathcal{K}$ \begin{equation*} \phi_{v,c} (X_{v,b}) \coloneqq \frac{1}{\abs{\mathcal{E}_v}} \sum_{e \in \mathcal{E}_v} \frac{1}{n_b} \sum_{i=1}^{n_b} ( \rho_{\theta_e} (t_{v,b}^i, v) - \rho_{v}^i)^2, \end{equation*} with $X_{v,b}$ as introduced before. Here, we introduce for each $v \in \mathcal{V}_\mathcal{K}$ some additional variables $\{\rho_{v}^i\}_{i=1}^{n_b}$, that are appended to our parameter set $\theta$ and determined through the optimization process in our model.
\end{itemize}

\begin{align*} \label{eq:loss}
    \Phi(X_{data}) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D}(X_{v,b}) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}} \big( \phi_{v,K} (X_{v,b}) + \phi_{v,c}(X_{v,b}) \big) + \\
    & \quad + \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}} \big( \phi_{e,r} (X_{e,r}) + \phi_{e,0} (X_{e,0}) \big). 
\end{align*}

