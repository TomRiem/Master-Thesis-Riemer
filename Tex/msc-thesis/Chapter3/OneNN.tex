\section{One Neural Network for all edges}
\label{ch3:sec1}

In this section we construct, in some sense, a global cost function, which is used to modify the weights and biases of a neural network appropriately by minimizing it in the learning phase. In the following we denote this cost function by $\Phi_\theta$, where $\theta$ only refers to the set of learnable parameters of the used neural network and does not obtain any information about the used type of neural network or the topology of the used neural network. The idea of this approach in this section is that in the learning phase the drift-diffusion equations on all edges and all initial and boundary conditions are considered at once such that all edges are seen as being merged together and thus the problem of approximating the solution of the set of drift-diffusion equations on a metric graph is treated as a whole. Therefore, we construct only one cost function $\Phi_\theta$, which consists of several misfit terms, as in \cref{MSE PINN}, by incorporating the mean-squared-error of a residual network as well as a number of misfit terms which enforce the initial and boundary conditions at a set of collocation points. Of course, we want to exploit the special structure of the domain given by the metric graph for the construction of this cost function $\Phi_\theta$. We note that we construct the cost function $\Phi_\theta$ for the general case and do not adopt the parameters with respect to the previously mentioned graph, since this approach can of course also be applied to any metric graph. \\
Let's start with the easy part first, the definition of the residual network, which returns us a residual in the cost function $\Phi_\theta$ for the neural network to be learned. From now on, we denote the surrogate network by $\rho_{\theta_e}$, which is supposed to approximate the solution $\rho_e$ of \cref{eq:Hamiltonian} on an individual edge $e \in \mathcal{E}$. Of course, $\rho_{\theta_e}$ receives the same input values $ \left( t, x \right) \in  \left( 0, T \right) \times [0, \ell_e]$ as $\rho_e$ and also returns a real-valued number. Here $\theta_e$ denotes the set of all weights and biases that are involved in the approximation of $\rho_e$ on an individual edge $e \in \mathcal{E}$, and $\theta_e$ also contains no information about the used type of neural network or the topology of the used neural network. If we insert the surrogate network $\rho_{\theta_e} \left( t,x \right)$ into the right-hand side of \cref{eq:Hamiltonian}, we obtain as a residual network for each individual edge $e \in \mathcal{E}$
\begin{equation}
    \label{Drift-Diffusion residual network}
    r_{\theta_e} \left( t,x \right)=\partial_t \rho_{\theta_e} \left( t,x \right) - \partial_x   \left(  \varepsilon \partial_x  \rho_{\theta_e} \left( t,x \right) - f \left( \rho_{\theta_e} \left( t,x \right) \right) \partial_x V \left( t,x \right) \right).
\end{equation}
Using \cref{Drift-Diffusion residual network} we form the residual misfit term for each individual edge $e \in \mathcal{E}$ via
\begin{equation} 
    \label{misfit:residual}
    \phi_{e,r}  \left( X_e \right) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} r_{\theta_e}  \left( x_e^i, t_e^i \right)^2,
\end{equation} 
where $X_e = \{ \left( t_e^i, x_e^i \right)\}_{i=1}^{n_e} \subset \left( 0, T \right) \times [0, \ell_e]$ is a set of time-space collocation points that are drawn randomly or chosen equidistantly. \\
To enforce the Kirchhoff-Neumann conditions, \cref{eq:Kirchhoff_Neumann_condition}, we define the following misfit term for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ 
\begin{equation} 
    \label{misfit:Kirchhoff}
    \phi_{v,K}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b}  \left( \sum_{e \in \mathcal{E}_v}  \left( - \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f \left( \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) \partial_x V_e \left( t_{v,b}^i, v \right) \right) \, n_e  \left( v \right) \right)^2, 
\end{equation} 
where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset \left( 0,T \right)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced. We note that the derivatives are taken into the outgoing direction. \\
In order to enforce the continuity in the interior vertices $\mathcal{V}_\mathcal{K}$, required by \cref{continuous on vertices}, we introduce two different misfit terms to accomplish this. In the numerical experiments, we draw attention to which of the two misfit terms is used. The first misfit term is defined for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ by 
\begin{equation} 
    \label{misfit:continuity}
    \phi_{v,c}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{e \in \mathcal{E}_v} \sum_{i=1}^{n_b} \left(  \rho_{\theta_e}  \left( t_{v,b}^i, v \right) - \rho_{v}^i \right)^2,
\end{equation} 
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. Here, we introduce for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ some additional learnable parameters $\{\rho_{v}^i\}_{i=1}^{n_b}$, that are appended to the set of weights and biases $\theta$ and which are also trained by minimizing the resulting cost function $\Phi_\theta$. The other misfit is defined for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ by 
\begin{equation} 
    \label{misfit:continuity:average}
    \phi_{v,c}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b}  \sum_{i=1}^{n_b} \left( \sum_{e \in \mathcal{E}_v} \left( \rho_{\theta_e}  \left( t_{v,b}^i, v \right) - \frac{1}{\abs{\mathcal{E}_v}} \sum_{e \in \mathcal{E}_v} \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) \right)^2.
\end{equation}
Here, on average over all time collocation points $\{t_{v,b}^i\}_{i=1}^{n_b}$, the value $\rho_{\theta_e}  \left( t_{v,b}^i, v \right)$ of each edge $e \in \mathcal{E}_v$ should be equal to the average of all edges connected to this interior vertex $v \in \mathcal{V}_\mathcal{K}$. Both misfit terms have their numerical advantages and disadvantages. The misfit term defined by \cref{misfit:continuity} is not so complex, from which can follow that the derivatives with respect to the learnable parameters in the optimization method is easier to compute, but therefore it introduces just as much learnable parameters as collocation points to the set of learnable parameters. No further learnable parameters need to be defined for the misfit term given by \cref{misfit:continuity:average}, but it is more complex for that. \\
We enforce the flux boundary conditions for each exterior vertex $v \in \mathcal{V}_\mathcal{D}$, \cref{eq:Dirichlet_conditions} by defining the following misfit term  
\begin{align} 
    \label{misfit:Dirichlet}
    \phi_{v,D}  \left( X_{v,b} \right) \coloneqq & \frac{1}{n_b} \sum_{i=1}^{n_b} \bigg( \sum_{e \in \mathcal{E}_v} \left(- \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f\left(\rho_{\theta_e}  \left( t_{v,b}^i, v \right)\right) \partial_x V_e\left( t_{v,b}^i, v \right) \right) n_e  \left( v \right) + \\
    & \alpha_v \left( t_{v,b}^i \right)  \left( 1- \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) - \beta_v \left( t_{v,b}^i \right) \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \bigg)^2
\end{align}
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. \\
To enforce the initial conditions for each edge $e \in \mathcal{E}$, \cref{eq:initial_conditions}, we define the following misfit term  
\begin{equation} 
    \label{misfit:initial}
    \phi_{e,0}  \left( X_{e,0} \right) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0}  \left( \rho_{\theta_e}  \left( 0,x_{e,0}^i \right) - \rho_{e,0} \left( x_{e,0}^i \right) \right)^2, 
\end{equation} 
where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of collocation points along $t=0$. \\ 

We average each of the above mentioned misfit terms over all units where these terms hold and sum up the averages to form $\Phi_\theta$:
\begin{align} 
    \label{eq:loss:1}
    \Phi_{\theta} \left( X_{data} \right) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D} \left( X_{v,b} \right) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}}  \left(  \phi_{v,K}  \left( X_{v,b} \right) + \phi_{v,c} \left( X_{v,b} \right)  \right) + \\
    & \quad + \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}}  \left(  \phi_{e,r}  \left( X_{e,r} \right) + \phi_{e,0}  \left( X_{e,0} \right)  \right), 
\end{align}
where $X_{data}$ represents the union of the different collocation points $X_e$, $X_{v,b}$ and $X_{e,0}$. 

We note that, as mentioned in \cref{ch1:sec4}, all derivatives of surrogate models with respect to the input parameters are computed with automatic differentiation provided by \lstinline!Tensorflow! via the method \lstinline!tf.GradientTape!. 


For the minimization of the cost function $\Phi_{\theta}$, we first use a variant of the gradient descent method, in which the direction is given by the Adam optimizer from the module \lstinline!Keras!, see \cite{Chollet:2015}, which belongs to the package \lstinline!Tensorflow!, see \cite{TensorFlow}. We change for the different types of neural networks the input parameter \lstinline!learning_rate!, which obviously specifies the learning rate of the method, and leave all other input parameters of the Adam optimizer by default. This method stops after 2000 iterations. After that, we use the L-BFGS-B optimizer from the package \lstinline!SciPy!, see \cite{SciPy:2020}, and we set for all networks as parameters in the options \lstinline!maxiter = 5000! which specifies the maximum number of iterations, \lstinline!maxfun = 50000! which specifies the maximum number of function evaluations, \lstinline!maxcor = 50! which specifies the maximum number of stored variable metric corrections used in the L-BFGS approximation of the Hessian of the cost function $\Phi_{\theta}$, \lstinline!maxls = 50! which specifies the maximum number of line search steps per iteration and \lstinline!ftol = 1.0*np.finfo(float).eps! which specifies a parameter in a stopping criterion that aborts the method if the values of the cost function $\Phi_{\theta}$ between two iterates are too small.



% Kollokationspunkte 
% Code 
% Adam und LBFGs

\subsection{One Neural Network to rule them all}
\label{ch3:sec1:subsec1}




\subsection{One Neural Network for each edge}
\label{ch3:sec1:subsec2}

Also, a set of neural networks offers the possibility to choose for the networks on the various edges different training parameters such as residual points but also different hyperparameters such as activation functions, width and depth of the corresponding network. One can also use a deep neural network for an edge for which the solution can have a complex structure, while a shallow neural network can be used on the edges with relatively simple and smooth solutions.




