\section{One Neural Network for all edges}
\label{ch3:sec1}

In this section we construct a, in some sense, global cost function, which is used to enable a type of neural network to modify its learnable parameters appropriately by minimizing it. In the following we denote this cost function by $\phi_\theta$, where $\theta$ only refers to the learnable parameters of the used neural network and does not obtain any information about the used type of neural network or the topology of the used neural network. The idea of this approach in this section is that in the learning phase the drift-diffusion equations on all edges and all initial and boundary conditions are considered at once such that all edges are seen as being merged together and thus the problem of solving the drift-diffusion equations on all edges is treated as a whole. 
Therefore, we construct only one cost function $\phi_\theta$, which consists of several parts, as in \cref{MSE PINN}, by incorporating the mean squared error of a residual network at a set of collocation points as well as a number of misfit terms which enforce both the boundary and initial conditions weakly. Of course, we want to exploit the special structure of the domain given by the metric graph for the construction of the cost function in thus also in the PINN. We note that we construct the cost function for the general case and do not adopt the parameters with respect to the previously mentioned graph, since this approach can of course also be applied to any metric graph. \\
Let's start with the easy part first, the definition of the residual network, which returns us a residual for the network to be learned in the cost function $\phi_\theta$. From now on, we will denote the surrogate network by $\rho_{\theta_e}$, which is supposed to approximate the solution $\rho_e$ of \cref{eq:Hamiltonian} on an individual edge $e \in \mathcal{E}$. Of course, $\rho_{\theta_e}$ receives the same input values $ \left( t, x \right) \in  \left( 0, T \right) \times e$ as $\rho_e$ and also returns a real-valued number. Here $\theta_e$ denotes the set of all weights and biases that are involved in the approximation of $\rho_e$ on an individual edge $e \in \mathcal{E}$, and $\theta_e$ also contains no information about the used type of neural network or the topology of the used neural network. If we insert the surrogate network $\rho_{\theta_e} \left( t,x \right)$ into the right-hand side of \cref{eq:Hamiltonian}, we obtain as a residual network for each individual edge $e \in \mathcal{E}$
\begin{equation}
    \label{Drift-Diffusion residual network}
    r_{\theta_e} \left( t,x \right)=\partial_t \rho_{\theta_e} \left( t,x \right) - \partial_x   \left(  \varepsilon \partial_x  \rho_{\theta_e} \left( t,x \right) - f \left( \rho_{\theta_e} \left( t,x \right) \right) \partial_x V \left( t,x \right) \right).
\end{equation}
From \cref{Drift-Diffusion residual network} we form the residual misfit term for each individual edge $e \in \mathcal{E}$ of the cost function $\phi_\theta$ via
\begin{equation} 
    \label{misfit:residual}
    \phi_{e,r}  \left( X_e \right) \coloneqq \frac{1}{n_e} \sum_{i=1}^{n_e} r_{\theta_e}  \left( X_e^i \right)^2,
\end{equation} 
where $r_{\theta_e}  \left( X_e^i \right) = r_{\theta_e}  \left( x_e^i, t_e^i \right)$ for $X_e = \{X_e^i\}_{i=1}^{n_e} = \{ \left( t_e^i, x_e^i \right)\}_{i=1}^{n_e} \subset  \left( 0, T \right) \times e$, which is a set of time-space collocation points that are drawn randomly or chosen equidistantly. \\
To enforce the Kirchhoff-Neumann conditions, \cref{eq:Kirchhoff_Neumann_condition}, we introduce the following misfit term for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ 
\begin{equation} 
    \label{misfit:Kirchhoff}
    \phi_{v,K}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{i=1}^{n_b}  \left( \sum_{e \in \mathcal{E}_v}  \left( - \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f \left( \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) \partial_x V_e \left( t_{v,b}^i, v \right) \right) \, n_e  \left( v \right) \right)^2, 
\end{equation} 
where $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b} \subset  \left( 0,T \right)$ is a set of time snapshots where the Kirchhoff-Neumann conditions are enforced. We note that the derivative is taken into the outgoing direction. \\
We introduce a misfit term for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ to enforce continuity in the interior vertices 
\begin{equation} 
    \label{misfit:continuity}
    \phi_{v,c}  \left( X_{v,b} \right) \coloneqq \frac{1}{n_b} \sum_{e \in \mathcal{E}_v} \sum_{i=1}^{n_b} \left(  \rho_{\theta_e}  \left( t_{v,b}^i, v \right) - \rho_{v}^i \right)^2,
\end{equation} 
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. Here, we introduce for each interior vertex $v \in \mathcal{V}_\mathcal{K}$ some additional learnable variables $\{\rho_{v}^i\}_{i=1}^{n_b}$, that are appended to our parameter set $\theta$ and determined through the optimization process in our model. \\
We enforce the flux boundary conditions for each exterior vertex $v \in \mathcal{V}_\mathcal{D}$, \cref{eq:Dirichlet_conditions} by the misfit term  
\begin{align} 
    \label{misfit:Dirichlet}
    \phi_{v,D}  \left( X_{v,b} \right) \coloneqq & \frac{1}{n_b} \sum_{i=1}^{n_b} \bigg( \sum_{e \in \mathcal{E}_v} \left(- \varepsilon \partial_x \rho_{\theta_e}  \left( t_{v,b}^i, v \right) + f\left(\rho_{\theta_e}  \left( t_{v,b}^i, v \right)\right) \partial_x V_e\left( t_{v,b}^i, v \right) \right) n_e  \left( v \right) + \\
    & \alpha_v \left( t_{v,b}^i \right)  \left( 1- \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \right) - \beta_v \left( t_{v,b}^i \right) \rho_{\theta_e}  \left( t_{v,b}^i, v \right) \bigg)^2
\end{align}
with $X_{v,b} = \{t_{v,b}^i\}_{i=1}^{n_b}$ as introduced before. \\
To enforce the initial conditions for each edge $e \in \mathcal{E}$, \cref{eq:initial_conditions}, we introduce the following misfit term  
\begin{equation} 
    \label{misfit:initial}
    \phi_{e,0}  \left( X_{e,0} \right) \coloneqq \frac{1}{n_0} \sum_{i=1}^{n_0}  \left( \rho_{\theta_e}  \left( 0,x_{e,0}^i \right) - \rho_{e,0} \left( x_{e,0}^i \right) \right)^2, 
\end{equation} 
where $X_{e,0} = \{x_{e,0}^i\}_{i=1}^{n_0} \subset [0, \ell_e]$ is a set of boundary points along $t=0$. \\ 


We average each misfit term over all units where these terms are involved and combine the averages to form $\Phi_{\theta}$:
\begin{align*} 
    \label{eq:loss}
    \Phi_{\theta} \left( X_{data} \right) & =  \frac{1}{\abs{\mathcal{V}_\mathcal{D}}} \sum_{v \in \mathcal{V}_\mathcal{D}} \phi_{v,D} \left( X_{v,b} \right) + \frac{1}{\abs{\mathcal{V}_\mathcal{K}}} \sum_{v \in \mathcal{V}_\mathcal{K}}  \left(  \phi_{v,K}  \left( X_{v,b} \right) + \phi_{v,c} \left( X_{v,b} \right)  \right) + \\
    & \quad + \frac{1}{\abs{\mathcal{E}}} \sum_{e \in \mathcal{E}}  \left(  \phi_{e,r}  \left( X_{e,r} \right) + \phi_{e,0}  \left( X_{e,0} \right)  \right), 
\end{align*}
where $X_{data}$ represents the union of the different collocation points $X_e$, $X_{v,b}$ and $X_{e,0}$. 








