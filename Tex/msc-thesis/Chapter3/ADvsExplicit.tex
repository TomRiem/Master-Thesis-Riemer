\section{Automatic differentiation vs explicit derivatives}
\label{ch3:sec3}

Below we provide an explicit calculation for the Jacobian or gradient and the Hessian matrix of an L-layer feed-forward neural network, from which partial derivatives up to order 2 can be obtained. 

In the following, $f_{\theta}(x)$ denotes the model prediction for the input value $x = x^0 \in \mathbb{R}^{n_0}$, which is given by 
\begin{equation}
    \label{model prediction}
    f_{\theta}(x) = x^{L} = f^{L}_{\theta_L}(x^{L-1}) = \sigma_{L} (W^{L} x^{L-1} + b^{L}) \in \mathbb{R}^{n_L} 
\end{equation}
and 
\begin{equation*}
    x^{l} = \sigma_{l} (W^{l} x^{l-1} + b^{l}) \in \mathbb{R}^{n_l}, 
\end{equation*}
for $0 \leq l \leq L-1$.

We are interested in the derivative of the FNN $f_{\theta} \colon \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}$ with respect to the input value x, i.e. we want to find the following linear mapping
\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d} x} f(x) \colon \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}
\end{equation*}
which can be represented by the Jacobian of $f_{\theta}$
\begin{equation*}
    \mathbb{J}= \begin{pmatrix} \dfrac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{n}} \end{pmatrix} = \begin{pmatrix} \nabla^{T} f_{1}(\mathbf{x}) \\ \vdots \\ \nabla^{T} f_{m}(\mathbf{x})\end{pmatrix} = \begin{pmatrix} \dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{n}} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{n}} \end{pmatrix} 
\end{equation*}
where:  is the transpose of the gradient of the -th component.


\subsection{Backpropagation in FNN}

\begin{equation*}
    a^{[0]}(x) = \begin{pmatrix} x \\ t \end{pmatrix} \in \mathbb{R}^2
\end{equation*}

\begin{equation*}
    a^{l}(x) = \sigma_{l} (z^{l}(x)) = \sigma_{l} (W^{l} a^{l-1}(x) + b^{l}) \in \mathbb{R}^{n_l}, 
\end{equation*}

where $W^{l} \in \mathbb{R}^{n_l \times n_{l-1}}, \; b^{l} \in \mathbb{R}^{n_l}$ and $z^{l}(x) = W^{l} a^{l-1}(x) + b^{l} \in \mathbb{R}^{n_l}$ for $l = 1, \ldots, L$.

\begin{equation*}
    f_{\theta}(x) = a^{L}(x) = \sigma_{L} (W^{L} a^{[L-1]}(x) + b^{L}) \in \mathbb{R}^{1} 
\end{equation*}

\begin{align*}
    \partial_x f_{\theta}(x) & =  \partial_x a^{L}(x) = \partial_x \sigma_{L} (W^{L} a^{[L-1]}(x) + b^{L}) = \\
    & =  \frac{\mathrm{d}}{\mathrm{d} z^{L}} \sigma_{L} (z^{L}(x)) \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} a^{L}} z^{L}(x) = \\ 
    & =  \frac{\mathrm{d}}{\mathrm{d} z^{L}} \sigma_{L} (z^{L}(x)) \, \cdot \, W^{L} \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} x} a^{[L-1]}(x) = \\
    & =  \ldots = \\
    & =  \frac{\mathrm{d}}{\mathrm{d} z^{L}} \sigma_{L} (z^{L}(x)) \, \cdot \, W^{L} \, \cdot \, \frac{\mathrm{d}}{\mathrm{d} z^{[L-1]}} \sigma_{[L-1]} (z^{[L-1]}(x)) \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \\
    & \quad \cdot \, \frac{\mathrm{d}}{\mathrm{d} z^{[1]}} \sigma_{[1]} (z^{[1]}(x)) \, \cdot \, W^{[1]} \, \cdot \,\frac{\mathrm{d}}{\mathrm{d} x} a^{[0]}(x) 
\end{align*}
    
\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d} x} a^{[0]}(x) = \frac{\mathrm{d}}{\mathrm{d} x} \begin{pmatrix} x \\ t \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}
\end{equation*}

\begin{equation*}
    \frac{\mathrm{d}}{\mathrm{d} z^{l}} \sigma_{l} (z^{l}(x)) = D^{l} \in \mathbb{R}^{n_l \times n_l}, \quad D_{i, i}^{l} = {\sigma_{l}}^{\prime} (z_{i}^{l})
\end{equation*}

(diagonal matrix, since we apply the activation function component-wise)

\begin{equation*}
    \partial_x f_{\theta}(x) = D^{L} \, \cdot \, W^{L} \, \cdot \, D^{[L-1]} \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \, D^{[1]} \, \cdot \, W^{[1]} \, \cdot \, \begin{pmatrix} 1 \\ 0 \end{pmatrix} \in \mathbb{R}
\end{equation*}

\begin{equation*}
    \partial_t f_{\theta}(x) = D^{L} \, \cdot \, W^{L} \, \cdot \, D^{[L-1]} \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \, D^{[1]} \, \cdot \, W^{[1]} \, \cdot \, \begin{pmatrix} 0 \\ 1 \end{pmatrix} 
\end{equation*}

If we set $s = (x)^{\mathrm{T}}$ we can write the gradient of $f_{\theta}(s)$ as

\begin{equation*}
    \nabla_s f_{\theta}(s) = {W^{[1]}}^{\mathrm{T}} \, \cdot \, D^{[1]} \, \cdot \, {W^{[2]}}^{\mathrm{T}} \, \cdot \, D^{[2]} \, \cdot \, \ldots \, \cdot \, {W^{L}}^{\mathrm{T}} \, \cdot \, D^{L},
\end{equation*}

which we compute in the Backpropagation algorithm and get

\begin{equation*}
    {\nabla_s f_{\theta}(s)}^{\mathrm{T}} \cdot \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \partial_x f_{\theta}(x) \quad \text{and} \quad {\nabla_s f_{\theta}(s)}^{\mathrm{T}} \cdot \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \partial_t f_{\theta}(x)
\end{equation*}


\begin{equation*}
    \partial_x [f(f_{\theta}(x)) \partial_x V_e(x)] = \frac{\mathrm{d}}{\mathrm{d} f_{\theta}} f(f_{\theta}(x)) \partial_x f_{\theta}(x) \partial_x V_e(x) + f(f_{\theta}(x)) \partial_{xx} V_e(x)
\end{equation*}

I suppose we use $f(f_e) = f_e(1-f_e) \Rightarrow f^{\prime}(f_e) = 1 - 2 f_e$. Of course, $\partial_x V_e(x)$ and $\partial_{xx} V_e(x)$ still remain to be computed. 



\begin{equation*}
    \partial_{x x} f_{\theta}(x) = ???
\end{equation*}

Attempt 1: Suppose $\partial_x f_{\theta}(x)$ is computed correctly and the activation function on each layer is $L-1$ times continuously differentiable

\begin{align*}
    \partial_{x x} f_{\theta}(x) & = \partial_{x} \partial_{x} f_{\theta}(x) = \\
    & = \partial_{x} [D^{L} \, \cdot \, W^{L} \, \cdot \, D^{[L-1]} \, \cdot \, W^{[L-1]} \, \cdot \, \ldots \, \cdot \, D^{[1]} \, \cdot \, W^{[1]} \, \cdot \, (1, 0)^{\mathrm{T}}]
\end{align*}

Only $D^{[1]}, \ldots, D^{L}$ depend on $x$. We first take the derivative $D^{[1]}$ with respect to $x$. Since all $D^{l}$ are diagonal matrices, we only consider the diagonal elements $D_{i, i}^{l} = {\sigma_{l}}^{\prime} (z_{i}^{l}) \in \mathbb{R}$.  

\begin{align*}
    \frac{\mathrm{d}}{\mathrm{d} x} {\sigma_{[1]}}^{\prime} (z_{i}^{[1]}) & = \frac{\mathrm{d}}{\mathrm{d} z_{i}^{[1]}} {\sigma_{[1]}}^{\prime} (z_{i}^{[1]}) \frac{\mathrm{d}}{\mathrm{d} x} z_{i}^{[1]} = {\sigma_{[1]}}^{\prime \prime} (z_{i}^{[1]}) \frac{\mathrm{d}}{\mathrm{d} x} (W^{[1]} a^{[0]}(x) + b^{[1]})_i = \\
    & = {\sigma_{[1]}}^{\prime \prime} (z_{i}^{[1]}) \frac{\mathrm{d}}{\mathrm{d} x} \left[ \sum^{n_0}_{j=1} W^{[1]}_{i,j} a^{[0]}_{j}(x) + b^{[1]}_i \right] = \\
    & = {\sigma_{[1]}}^{\prime \prime} (z_{i}^{[1]}) \sum^{n_0}_{j=1} W^{[1]}_{i,j} \frac{\mathrm{d}}{\mathrm{d} x} a^{[0]}_{j}(x) = \\
    & = {\sigma_{[1]}}^{\prime \prime} (z_{i}^{[1]}) \left\langle W^{[1]}_{i, \colon}, \frac{\mathrm{d}}{\mathrm{d} x} a^{[0]}(x)  \right\rangle = \\
    & = {\sigma_{[1]}}^{\prime \prime} (W^{[1]}_{i, 1} + b^{[1]}_i) W^{[1]}_{i, 1}
\end{align*}

Okay, that wasn't bad. You can certainly continue it iteratively. 

But what about this long matrix product? Does the product rule apply here? If so, then we have to apply it $L-1$ times? And then we have to differentiate a selected matrix (probably $D^{L}$) $L-1$ times. Great.

$\Rightarrow$ If we consider the matrix product component-wise, then the product rule must actually be applied $L-1 \times n_{L-1} \times n_{L-2} \times \ldots \times n_{2}$ times for $\frac{\mathrm{d}}{\mathrm{d} z^{l}_i} \sigma_{l}(z^{l}_i)$ in the second derivative.  

\textbf{Ruthotto:}

For $n_L = 1$ we have

\begin{align*}
    \delta^{L} & = {W^{L}}^{\mathrm{T}} \mathrm{diag}({\sigma_{L}}^{\prime}(W^{L} a^{[L-1]}(s) + b^{L})) = {W^{L}}^{\mathrm{T}} D^{L} \in \mathbb{R}^{n_{L-1}} \\
    \delta^{l} & = {W^{l}}^{\mathrm{T}} D^{l} \cdot \, \delta^{[l+1]} \, \cdot \, \ldots \,  \cdot \, \delta^{L} \in \mathbb{R}^{n_{l-1}} \quad \text{for} \quad l = 2, \ldots, L-1
\end{align*}

We note that for $A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^{m}$, $c, d \in \mathbb{R}^{n}$

\begin{align*}
    d \, \odot \, c &= c \, \odot \, d \in \mathbb{R}^{n} \\
    \mathrm{diag}(b) \, \cdot \, A &= b \, \odot \, A \in \mathbb{R}^{m \times n} \\
    A \, \cdot \, \mathrm{diag}(c) &= A \, \odot \, c^{\mathrm{T}} \in \mathbb{R}^{m \times n} \\
    \mathrm{diag}(b) \, \cdot \, A \, \cdot \, \mathrm{diag}(c) &= b \, \odot \, A \, \odot \, c^{\mathrm{T}} \in \mathbb{R}^{m \times n} \\
    \mathrm{diag}(c) \, \cdot \, d &= c \, \odot \, d \in \mathbb{R}^{n} \\
    c^{\mathrm{T}} \, \cdot \, \mathrm{diag}(d) &= c^{\mathrm{T}} \, \odot \, d^{\mathrm{T}} \in \mathbb{R}^{1 \times n} \\
\end{align*}

We set 

\begin{equation*}
    H^{l} = \mathrm{diag}({\sigma_{l}}^{\prime \prime}(W^{l} a^{l-1}(s) + b^{l})) \in \mathbb{R}^{n_l \times n_l}, \quad (H^{l})_{i, i} = {\sigma_{l}}^{\prime \prime} (z_{i}^{l})
\end{equation*}

\begin{align*}
    \nabla^{2} f_{\theta}(s) & = \vartheta^{[1]} \in \mathbb{R}^{2 \times 2} \\
    \vartheta^{[1]} & = {W^{[1]}}^{\mathrm{T}} \left( \mathrm{diag}(H^{[1]} \delta^{[2]}) + D^{[1]} \vartheta^{[2]} D^{[1]} \right) W^{[1]} \\
    & \vdots \\
    \vartheta^{l} & = {W^{l}}^{\mathrm{T}} \left( \mathrm{diag}(H^{l} \delta^{[l+1]}) + D^{l} \vartheta^{[l+1]} D^{l} \right) W^{l} \in \mathbb{R}^{n_{l-1} \times n_{l-1}} \\
    & \vdots \\
    \vartheta^{L} & = {W^{L}}^{\mathrm{T}} H^{L} W^{L} \in \mathbb{R}^{n_{L-1} \times n_{L-1}}
\end{align*}






\subsection{Backpropagation in ResNet}

\begin{equation*}
    a^{[1]}(x) = \sigma_{[1]} (W^{[1]} a^{[0]}(x) + b^{[1]}) \in \mathbb{R}^{m}, 
\end{equation*}

where $W^{[1]} \in \mathbb{R}^{m \times 2}$ and $b^{[1]} \in \mathbb{R}^{m}$.

\begin{equation*}
    a^{l}(x) = a^{l-1}(x) + h \, \sigma_{l} (z^{l}(x)) = a^{l-1}(x) + h \, \sigma_{l} (W^{l} a^{l-1}(x) + b^{l}) \in \mathbb{R}^{m}, 
\end{equation*}

for $l = 2, \ldots, L$, where $W^{[2]}, \ldots, W^{L} \in \mathbb{R}^{m \times m}$ and $b^{[2]}, \ldots, b^{L} \in \mathbb{R}^{m}$.

\begin{equation*}
    f_{\theta}(x) = f_{\theta}(s) = w^{\mathrm{T}} a^{L}(s) + \frac{1}{2} s^{\mathrm{T}} A s + c^{\mathrm{T}} s 
\end{equation*}

where $s = (x)^{\mathrm{T}} \in \mathbb{R}^{2}$, $w \in \mathbb{R}^{m}$, $A \in \mathbb{R}^{2 \times 2}$ and $c \in \mathbb{R}^2$.

\textbf{Gradient:}

\begin{equation*}
    \nabla_s f_{\theta}(s) = \nabla_s a^{L}(s) \, w + A s + c
\end{equation*}

We set

\begin{equation*}
    D^{l} = \mathrm{diag} \left( \frac{\mathrm{d}}{\mathrm{d}z^{l}} \sigma_{l} (z^{l}) \right) \in \mathbb{R}^{m \times m}, \quad D_{i, i}^{l} = {\sigma_{l}}^{\prime} (z_{i}^{l})
\end{equation*}

\begin{align*}
    \nabla_s a^{L}(s) \, w & = \delta^{[1]}  \\
    \delta^{[1]} & = {W^{[1]}}^{\mathrm{T}} D^{[1]} \, \delta^{[2]} \\
    \delta^{[2]} & = \delta^{[3]} + h \, {W^{[2]}}^{\mathrm{T}} D^{[2]} \, \delta^{[3]} \\
    &\vdots\\
    \delta^{l} & = \delta^{[l+1]} + h \, {W^{l}}^{\mathrm{T}} D^{l} \, \delta^{[l+1]} \\
    &\vdots\\
    \delta^{[L-1]} & = \delta^{L} + h \, {W^{[L-1]}}^{\mathrm{T}} D^{[L-1]} \, \delta^{L} \\
    \delta^{L} & = w + h \, {W^{L}}^{\mathrm{T}} D^{L} \, w
\end{align*}


\textbf{Hessian:}

First my derivation:

Ruthotto computes the Laplacian of the potential model with respect to the spatial variable $x$. For this he uses the trace of the Hessian matrix of our model $f_{\theta}(s)$, i.e. 

\begin{align*}
    \Delta_x f_{\theta}(s) & = \mathrm{tr}(E^{\mathrm{T}} \, \nabla^{2}_s f_{\theta}(s) \, E) = \\
    & = \mathrm{tr}(E^{\mathrm{T}} \, (\nabla^{2}_s a^{L}(s) \, w + A) \, E) = \\
    & = \mathrm{tr}(E^{\mathrm{T}} \, \nabla^{2}_s a^{L}(s) \, w \, E) + \mathrm{tr}(E^{\mathrm{T}} \,  A \, E) = \\
    & = \Delta_x (a^{L}(s) \, w) + \mathrm{tr}(E^{\mathrm{T}} \,  A \, E)
\end{align*}

where the columns of $E \in \mathbb{R}^{(d+1) \times d}$ are given by the first $d$ standard basis vectors in $R^{d+1}$ so that only the second derivative of the spatial coordinates $x \in \mathbb{R}^d$ is computed. In our case we use $E = (1, 0)^{\mathrm{T}} \in \mathbb{R}^{2}$. \\
Ruthotto sets

\begin{align*}
    \Delta_x (a^{L}(s) \, w) & = t^{[0]} + h \, \sum^{L}_{l=1} t^{l} = \mathrm{tr}(\vartheta^{[0]}) + h \, \sum^{L}_{l=1} \mathrm{tr}(\vartheta^{l}) = \\
    & = \mathrm{tr} \left( \vartheta^{[0]} + h \, \sum^{L}_{l=1} \vartheta^{l} \right) = \mathrm{tr} \left( \nabla^{2}_x a^{L}(s) \, w \right)
\end{align*}

\begin{equation*}
    \partial_{xx} a^{L}(s) w = \nabla^{2}_x a^{L}(s) w = E^{\mathrm{T}} \, \nabla^{2}_s a^{L}(s) \, w \, E = \vartheta^{[0]} + h \, \sum^{L}_{l=1} \vartheta^{l}
\end{equation*}

where 

\begin{equation*}
    \vartheta^{[0]} = E^T {W^{[1]}}^{\mathrm{T}} \mathrm{diag}({\sigma_{[1]}}^{\prime \prime}(W^{[1]} s + b^{[1]}) \odot \delta^{[2]}) W^{[1]} E
\end{equation*}

\begin{equation*}
    \vartheta^{l} = {J^{l-1}}^{\mathrm{T}} {W^{l}}^{\mathrm{T}} \mathrm{diag}({\sigma_{l}}^{\prime \prime}(W^{l} a^{l-1}(s) + b^{l}) \odot \delta^{[l+1]}) W^{l} J^{l-1}
\end{equation*}

where 

\begin{equation*}
    J^{l-1} = J_x(a^{l-1}(s)) = \left( \partial_x a^{l-1}_1(s), \ldots, \partial_x a^{l-1}_m(s) \right)^{\mathrm{T}} \in \mathbb{R}^{m}
\end{equation*}


Now to the Hessian as it has been implemented: 

\begin{align*}
    \vartheta^{[1]} & = h \, {W^{[1]}}^{\mathrm{T}} \mathrm{diag} \left( H^{[1]} \delta^{[2]} \right) W^{[1]} + \\
    & \quad + \left( I + h \, {W^{[1]}}^{\mathrm{T}} D^{[1]} \right) \, \left( \left( I + h \, {W^{[1]}}^{\mathrm{T}} D^{[1]} \right) \, \vartheta^{[2]} \right)^{\mathrm{T}} \\ 
    &\vdots\\
    \vartheta^{l} & = h \, {W^{l}}^{\mathrm{T}} \mathrm{diag} \left( H^{l} \delta^{[l+1]} \right) W^{l} + \\
    & \quad + \left( I + h \, {W^{l}}^{\mathrm{T}} D^{l} \right) \, \left( \left( I + h \, {W^{l}}^{\mathrm{T}} D^{l} \right) \, \vartheta^{[l+1]} \right)^{\mathrm{T}} \\ 
    \vartheta^{l} & = h \, {W^{l}}^{\mathrm{T}} \mathrm{diag} \left( H^{l} \delta^{[l+1]} \right) W^{l} + \\
    & \quad + \left( I + h \, {W^{l}}^{\mathrm{T}} D^{l} \right) \, \vartheta^{[l+1]}  \left( I + h \,  D^{l} {W^{l}} \right) \\ 
    &\vdots\\
    \vartheta^{[L-1]} & = h \, {W^{[L-1]}}^{\mathrm{T}} \mathrm{diag} \left( H^{[L-1]} \delta^{L} \right) W^{[L-1]} + \\
    & \quad + \left( I + h \, {W^{[L-1]}}^{\mathrm{T}} D^{[L-1]} \right) \, \left( \left( I + h \, {W^{[L-1]}}^{\mathrm{T}} D^{[L-1]} \right) \, \vartheta^{L} \right)^{\mathrm{T}} \\
    \vartheta^{L} &  = h \, {W^{L}}^{\mathrm{T}} \mathrm{diag}(H^{L} w) W^{L} 
\end{align*}

