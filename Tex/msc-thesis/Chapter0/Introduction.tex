\chapter{Introduction}

Dynamic processes on networks, see \cite{newman2018networks}, are crucial for understanding complex phenomena across science and engineering. Networks or rather their mathematical representations, graphs, have become ubiquitous as modelling paradigm. In the case of complex dynamics observed within the network, so-called metric graphs provide a natural representation as both the edge length is incorporated into the modelling as well as a partial differential equation defined on the graph, see \cite{BerkolaikoKuchment:2013}. \\
In this thesis, we introduce an approach to use physics informed neural networks to solve time dependent drift-diffusion equations on metric graphs. A metric graph is a tuple $\Gamma = \left(\mathcal{V},\mathcal{E}\right)$ composed of a set of vertices $\mathcal{V}$ and a set of edges $\mathcal{E}$, where each edge $e \in \mathcal{E}$ is equipped with a length $\ell_e>0$. For the model considered in this thesis, we consider the following differential operator on each edge $e \in \mathcal{E}$ of a metric graph $\Gamma = \left(\mathcal{V},\mathcal{E}\right)$
\begin{equation} 
    \label{eq:H}
    \mathcal{H} [\rho_e]  \left( t,x \right)  \coloneqq \partial_t \rho_e  \left( t,x \right)   - \partial_x  \left( \varepsilon \partial_x \rho_e  \left( t,x \right)  + f \left( \rho_e  \left( t,x \right)   \right)  \partial_x V_e  \left( t,x \right)  \right) ,
\end{equation}
where $\rho_e \colon  \left( 0, T \right)  \times \left[ 0, \ell_e \right] \to \mathbb{R}_{+}$ denotes the density distribution on each edge while $V_e \colon  \left( 0,T \right)  \times \left[ 0, \ell_e \right] \to \mathbb{R}_{+}$ is a given potential, that may vary from edge to edge. The function $f \colon \mathbb{R}_{+} \to \mathbb{R}_{+}$ is called mobility and its simplest choice is $f\left(\rho_e\right) = \rho_e$, i.e. linear transport. The differential operator \cref{eq:H} is then equipped with Kirchhoff-Neumann coupling conditions on the set on interior vertices $\mathcal{V}_\mathcal{K} \subset \mathcal{V}$ while on the remaining exterior vertices $\mathcal{V} \setminus \mathcal{V}_\mathcal{K}$, different boundary conditions are possible. While classical choices are Dirichlet- or Neumann-conditions, we will focus in this thesis on flux boundary conditions that prescribe the in- and out-flux at these vertices, respectively. Drift-diffusion equations play a crucial role in many application areas ranging from semiconductor modelling \cite{natalini1996bipolar} and solar \cite{hwang2009drift} to pedestrian motion \cite{hughes2002continuum} and vehicular transport on networks \cite{burger2018derivation,tordeux2018traffic}. \\
Physics informed neural networks, abbreviated $PINN$, were introduced in \cite{RaissiPerdikarisKarniadakisPart1:2017} utilizing modern machine learning ideas as well as the computational frameworks such as \lstinline!Tensorflow!, see \cite{TensorFlow}, to solve differential equations. Thereby, neural networks are trained by minimising the norm of a so-called residual network, which is derived by the right-hand side of \cref{eq:H}, over a set of collocation points. The derivatives needed for the partial differential equations are in general computed by using automatic differentiation, which is one of the distinguishing features of implemented $PINN$ approaches. Based on the seminal frameworks of \cite{RaissiPerdikarisKarniadakis:2019,lu2021deepxde}, the idea of $PINN$s has spread to many different application areas \cite{zhu2019physics,mao2020physics,jin2021nsfnets,sahli2020physics}including fluid dynamics \cite{raissi2018hidden,MAO2020112789,lye2020deep,magiera2020constraint,wessels2020neural}, continuum mechanics and elastodynamics \cite{haghighat2020deep,nguyen2020deep,rao2020physics}, inverse problems \cite{meng2020composite,jagtap2020conservative}, fractional advection-diffusion equations \cite{pang2019fpinns}, stochastic advection-diffusion-reaction equations \cite{chen2019learning}, stochastic differential equations \cite{yang2020physics} and power systems \cite{misyris2020physics}. \\
The aim of this thesis is to illustrate that the $PINN$s framework can be generalized to the case when the partial differential equation represents a drift-diffusion equation posed on a metric graph as described by \cref{eq:H}. We further show how different types of neural networks can be used for this approach and demonstrate in numerical experiments how these affect the approximation quality of the resulting method. Moreover, we are interested in the use of explicit derivatives in the differential operators defined by \cref{eq:H} instead of using automatic differentiation, which is why we present explicit forms for the first and second derivatives of the neural networks used in the numerical experiments with respect to the input and check in numerical experiments whether their use is worthwhile compared to the use of automatic differentiation. \\
The thesis is organized as follows: we introduce metric graphs and their characteristic in \cref{ch1:sec1}. In \cref{ch1:sec2} we explain the approximation problem which is given by \cref{eq:H} on each edge $e \in \mathcal{E}$ of a metric graph $\Gamma = \left( \mathcal{V}, \mathcal{E}\right)$ with initial and different vertex conditions. In \cref{ch1:sec3} we introduce neural networks in general and in \cref{ch1:sec4} we explain how they can be used in the $PINN$ setup to approximate the solution of a partial differential equation. In \cref{ch2} we present a finite volume method which returns ground truth solutions to the approximation problem for our numerical experiments. In \cref{ch3:sec1}, we construct the cost function used to train the networks for the $PINN$ approach discussed in this thesis and perform numerical experiments in which we train three different types of neural networks using this cost function and compare the resulting errors with respect to the finite volume method of \cref{ch2}. In \cref{ch3:sec3} we present methods to compute the explicit derivatives for the neural networks used in \cref{ch3:sec1} and compare the time needed for these implemented methods with the time needed when computing the same values with automatic differentiation of \lstinline!Tensorflow!. Finally, conclusions are drawn and an outlook for the future is given in \cref{ch5}.